{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Postal Codes\n",
    "\n",
    "__Convolutional Neural Networks__\n",
    "\n",
    "_By Marnick van der Arend & Jeroen Smienk_\n",
    "\n",
    "![Sample Digits](digits-sample.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "__MODEL_PATH = 'conv-models'\n",
    "__DATA_PATH = 'dataset-images'\n",
    "__TENSOR_LOG_DIR = 'conv-logs'\n",
    "\n",
    "__LABELS = 10\n",
    "__IM_SIZE = 32\n",
    "__N_DIGITS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "480 Images consisting of 4 digit postal codes with the label as the file name e.g. `3365.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(image):\n",
    "    \"\"\"\n",
    "    Returns a binarized image where lighter values are set to 255 and the lower values set to 0.\n",
    "    \"\"\"\n",
    "    blur = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "    _, thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    return thresh\n",
    "\n",
    "def crop(image):\n",
    "    \"\"\"\n",
    "    Crop an 128x32 image to 4 32x32 images\n",
    "    \"\"\"\n",
    "    digits = []\n",
    "    x,y,w,h = 0,0,__IM_SIZE,__IM_SIZE\n",
    "    for i in range(__N_DIGITS):\n",
    "        x = i*w\n",
    "        digits.append(image[y:y+h, x:x+w])\n",
    "    return digits\n",
    "\n",
    "def get_images_in_path(path, extension):\n",
    "    \"\"\"\n",
    "    Create a list of all images and their file names (labels) in a certain path\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    for name in os.listdir(path):\n",
    "        if name.endswith(extension):\n",
    "            img = cv2.imread(os.path.join(path, name), cv2.IMREAD_GRAYSCALE)\n",
    "            label = name[:-len(extension)] # remove extension\n",
    "            digits = crop(binarize(img))\n",
    "            for i in range(__N_DIGITS):\n",
    "                images.append(digits[i])\n",
    "                labels.append(int(label[i]))\n",
    "    return np.array(images, dtype=np.int32), np.array(labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dataset\n",
    "\n",
    "We split the 480 images in 1920 individual black-and-white digits and save them with their correct label as a tuple in a list. We also binarize the images so there is less noise in the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = get_images_in_path(__DATA_PATH, '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what kind of digits are in the set by plotting them with their labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 1920 images\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x216 with 60 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Size of dataset: {len} images\".format(len=len(images)))\n",
    "\n",
    "PLOT_SIZE = 60\n",
    "ROW_WIDTH = 20\n",
    "plt.figure(figsize=(15, PLOT_SIZE / ROW_WIDTH))\n",
    "for i in range(PLOT_SIZE):\n",
    "    plt.subplot(PLOT_SIZE / ROW_WIDTH, ROW_WIDTH, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.xlabel(labels[i])\n",
    "    plt.imshow(images[i], cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot Eencoding\n",
    "\n",
    "To compare the labels with the predictions of the neural network we need to 'one-hot' encode the labels:\n",
    "\n",
    "The index of the `1` is the correct label of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One hot encoded labels:\n",
      "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "onehot = np.zeros((len(labels), __LABELS))\n",
    "onehot[range(len(labels)), labels] = 1\n",
    "\n",
    "print('One hot encoded labels:')\n",
    "print(onehot[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine the data and the labels to be able to shuffle them without forgetting which labels belong to which images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([np.array([images[i], onehot[i]]) for i in range(len(images))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (1920, 32, 32, 1)\n",
      "Labels shape: (1920, 10)\n"
     ]
    }
   ],
   "source": [
    "shuffled_data = np.random.permutation(data)\n",
    "\n",
    "X = np.array([t[0] for t in shuffled_data])\n",
    "X = np.reshape(X, (len(X), 32, 32, 1))\n",
    "Y = np.array([t[1] for t in shuffled_data])\n",
    "\n",
    "print('Images shape: {}'.format(X.shape))\n",
    "print('Labels shape: {}'.format(Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into train, test, and validation sets.\n",
    "\n",
    "We use the train set to train; the test set to cross-validate during training; and the validation set to validate the model after the model is done training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split X (train, test, validation): (1387, 32, 32, 1) (245, 32, 32, 1) (288, 32, 32, 1)\n",
      "Split Y (train, test, validation): (1387, 10) (245, 10) (288, 10)\n"
     ]
    }
   ],
   "source": [
    "split = int(len(X) * .85)\n",
    "X_train = X[:split]\n",
    "X_valid = X[split:]\n",
    "Y_train = Y[:split]\n",
    "Y_valid = Y[split:]\n",
    "\n",
    "split = int(len(X_train) * .85)\n",
    "X_test = X_train[split:]\n",
    "X_train = X_train[:split]\n",
    "Y_test = Y_train[split:]\n",
    "Y_train = Y_train[:split]\n",
    "\n",
    "print('Split X (train, test, validation):', X_train.shape, X_test.shape, X_valid.shape)\n",
    "print('Split Y (train, test, validation):', Y_train.shape, Y_test.shape, Y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that returns a random batch of a certain size. This batch is used in training to train faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, y, size):\n",
    "    batch = np.array([(x[i], y[i]) for i in range(len(x))])\n",
    "    random_batch = np.random.permutation(batch)[:size]\n",
    "    return np.array([x[0] for x in random_batch]), np.array([x[1] for x in random_batch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "We designed several models:\n",
    "\n",
    "rank | name | layers | score\n",
    "--- | --- | --- | ---\n",
    "5 | model_8 | (64, Tanh, Drop=.2) (128, Tanh, Drop=.3) (256, Tanh, Drop=.4) (512, Tanh, Drop=.5) (64, Tanh) | 0.93714285\n",
    "4 | model_7 | (64, ReLU) (128, ReLU) (256, ReLU) (512, ReLU, Drop=.3) (64, ReLU) | 0.94057140\n",
    "3 | model_6 | (200, Tanh) (300, Tanh) (600, Tanh) | 0.97828573\n",
    "1 | model_5_2 | (600, Tanh, Drop=.3) (300, Tanh, Drop=.3) (200, Tanh, Drop=.3) | 0.98742855\n",
    "2 | model_5 | (600, Tanh) (300, Tanh) (200, Tanh) | 0.97942860\n",
    "6 | model_4 | (128, Tanh) (64, Tanh) (32, Tanh) | 0.87771430\n",
    "7 | model_3 | (12, ReLU) (24, ReLU) (48, ReLU, Drop=.1) (96, ReLU) | 0.73028570\n",
    "8 | model_2 | (128, ReLU) (64, ReLU) (32, ReLU) | 0.82400000\n",
    "9 | model_1 | (128, Sigmoid) | 0.66628570\n",
    "\n",
    "Dropout has a positive effect on the score as can be seen in the table. We also found that the tanh activation function performed well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model 1: BLUE\n",
    "- Model 2: RED\n",
    "- Model 3: LIGHT BLUE\n",
    "- Model 4: PINK\n",
    "- Model 5: GREEN\n",
    "- Model 6: GRAY\n",
    "- Model 7: ORANGE\n",
    "- Model 8: ORANGE\n",
    "\n",
    "### Batch Accuracy\n",
    "\n",
    "\n",
    "### Batch Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_1(x, output_shape):\n",
    "    \"\"\"\n",
    "    One Conv2d layer with a kernel size of 3 and \n",
    "    \"\"\"\n",
    "    l_1 = tf.layers.conv2d(x, filters=32, kernel_size=3, strides=1, padding=\"same\", \n",
    "                           activation=tf.nn.relu)\n",
    "    return tf.layers.dense(l_1, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2(x, output_shape):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    conv1 = tf.layers.conv2d(x, filters=32, kernel_size=5, \n",
    "                             padding=\"same\", activation=tf.nn.relu)\n",
    "    pool1 = tf.layers.max_pooling2d(conv1, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    flatten = tf.contrib.layers.flatten(pool1)\n",
    "    return tf.layers.dense(flatten, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_3(x, output_shape):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #32 convolution filters used each of size 3x3\n",
    "    conv1 = tf.layers.conv2d(x, filters=32, kernel_size = 3,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "    \n",
    "    #64 convolution filters used each of size 3x3\n",
    "    conv2 = tf.layers.conv2d(conv1, filters=64, kernel_size = 3,\n",
    "                         strides = 1, padding=\"valid\",\n",
    "                         activation = tf.nn.relu)\n",
    "    \n",
    "    #choose the best features via pooling\n",
    "    pool1 = tf.layers.max_pooling2d(conv2, pool_size=2, strides=2)\n",
    "    \n",
    "    #randomly turn neurons on and off to improve convergence\n",
    "    dropout1 = tf.layers.dropout(pool1, rate=.25)\n",
    "    \n",
    "    #flatten since too many dimensions, we only want a classification output\n",
    "    flatten = tf.contrib.layers.flatten(dropout1)\n",
    "\n",
    "    #fully connected to get all relevant data\n",
    "    dense = tf.layers.dense(flatten, units=128, activation=tf.nn.relu)\n",
    "    \n",
    "    #one more dropout for convergence' sake :) \n",
    "    dropout2 = tf.layers.dropout(dense, rate=.5)\n",
    "\n",
    "    return tf.layers.dense(dropout2, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_4(x, output_shape):\n",
    "    \"\"\"\n",
    "    IMPLEMENT BATCH NORMALIZATION AFTER CONV LAYERS\n",
    "    \"\"\"\n",
    "    conv1 = tf.layers.conv2d(x, filters=32, kernel_size = 3,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(conv1, filters=32, kernel_size = 3,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "        \n",
    "    pool1 = tf.layers.max_pooling2d(conv2, pool_size=2, strides=2)\n",
    "\n",
    "    conv3 = tf.layers.conv2d(pool1, filters=64, kernel_size = 3,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "\n",
    "    conv4 = tf.layers.conv2d(conv3, filters=64, kernel_size = 3,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "\n",
    "    pool2 = tf.layers.max_pooling2d(conv4, pool_size=2, strides=2)\n",
    "\n",
    "    flatten = tf.contrib.layers.flatten(pool2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    dense = tf.layers.dense(flatten, units=512, activation=tf.nn.relu)\n",
    "\n",
    "    dropout = tf.layers.dropout(dense, rate=.2)\n",
    "    \n",
    "    return tf.layers.dense(dropout, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_5(x, output_shape):\n",
    "    \"\"\"\n",
    "    High number of neurons in layers, decreasing per layer\n",
    "    \"\"\"\n",
    "    l_1 = tf.layers.dense(x, units=600, activation=tf.nn.tanh)\n",
    "    l_2 = tf.layers.dense(l_1, units=300, activation=tf.nn.tanh)\n",
    "    l_3 = tf.layers.dense(l_2, units=200, activation=tf.nn.tanh)\n",
    "    return tf.layers.dense(l_3, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_5_2(x, output_shape):\n",
    "    \"\"\"\n",
    "    High number of neurons in layers, decreasing per layer\n",
    "    \"\"\"\n",
    "    l_1 = tf.layers.dense(x, units=600, activation=tf.nn.tanh)\n",
    "    d_1 = tf.layers.dropout(l_1, rate=.3)\n",
    "    l_2 = tf.layers.dense(d_1, units=300, activation=tf.nn.tanh)\n",
    "    d_2 = tf.layers.dropout(l_2, rate=.3)\n",
    "    l_3 = tf.layers.dense(d_2, units=200, activation=tf.nn.tanh)\n",
    "    d_3 = tf.layers.dropout(l_3, rate=.3)\n",
    "    return tf.layers.dense(d_3, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_6(x, output_shape):\n",
    "    \"\"\"\n",
    "    High number of neurons in layers, increasing per layer\n",
    "    \"\"\"\n",
    "    l_1 = tf.layers.dense(x, units=200, activation=tf.nn.tanh)\n",
    "    l_2 = tf.layers.dense(l_1, units=300, activation=tf.nn.tanh)\n",
    "    l_3 = tf.layers.dense(l_2, units=600, activation=tf.nn.tanh)\n",
    "    return tf.layers.dense(l_3, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_7(x, output_shape):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    l_1 = tf.layers.dense(x, units=64, activation=tf.nn.relu)\n",
    "    l_2 = tf.layers.dense(l_1, units=128, activation=tf.nn.relu)\n",
    "    l_3 = tf.layers.dense(l_2, units=256, activation=tf.nn.relu)\n",
    "    l_4 = tf.layers.dense(l_3, units=512, activation=tf.nn.relu)\n",
    "    d_4 = tf.layers.dropout(l_4, rate=.3)\n",
    "    l_5 = tf.layers.dense(d_4, units=64, activation=tf.nn.relu)\n",
    "    return tf.layers.dense(l_5, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_8(x, output_shape):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    l_1 = tf.layers.dense(x, units=64, activation=tf.nn.tanh)\n",
    "    d_1 = tf.layers.dropout(l_1, rate=.2)\n",
    "    l_2 = tf.layers.dense(d_1, units=128, activation=tf.nn.tanh)\n",
    "    d_2 = tf.layers.dropout(l_2, rate=.3)\n",
    "    l_3 = tf.layers.dense(d_2, units=256, activation=tf.nn.tanh)\n",
    "    d_3 = tf.layers.dropout(l_3, rate=.4)\n",
    "    l_4 = tf.layers.dense(d_3, units=512, activation=tf.nn.tanh)\n",
    "    d_4 = tf.layers.dropout(l_4, rate=.5)\n",
    "    l_5 = tf.layers.dense(d_4, units=64, activation=tf.nn.tanh)\n",
    "    return tf.layers.dense(l_5, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the placeholder for our 5-dice input and 7-class output and choose a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, X_train.shape[1], X_train.shape[2], 1], name='x')\n",
    "y = tf.placeholder(tf.float32, shape=[None, Y_train.shape[1]], name='y')\n",
    "\n",
    "model_fn = model_3\n",
    "y_pred = model_fn(x, Y_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose an optimizer, a loss functon and metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=y_pred)\n",
    "loss_fn = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# Optimizer minimizes the loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=.0005).minimize(loss_fn)\n",
    "\n",
    "# Accuracy metric\n",
    "#   checks if the indices of the highest values in the real \n",
    "#   and predicted arrays are equal\n",
    "prediction = tf.equal(tf.argmax(y, axis=1), tf.argmax(y_pred, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model using a certain batch size and for a number of iterations while posting scalars to TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iters = 1500\n",
    "train_batch_size = 20\n",
    "\n",
    "session = tf.Session()\n",
    "with session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    sum_loss_train = tf.summary.scalar('loss_train', loss_fn)\n",
    "    sum_loss_test = tf.summary.scalar('loss_test', loss_fn)\n",
    "    sum_acc_train = tf.summary.scalar('acc_train', accuracy)\n",
    "    sum_acc_test = tf.summary.scalar('acc_test', accuracy)\n",
    "    tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(os.path.join(__TENSOR_LOG_DIR, model_fn.__name__), session.graph)\n",
    "    \n",
    "    for i in range(iters):\n",
    "        x_batch, y_batch = get_batch(X_train, Y_train, train_batch_size)\n",
    "                \n",
    "        loss_val, _, acc_val, sum_1, sum_2 = session.run([loss_fn, optimizer, accuracy, \n",
    "                                                          sum_loss_train, sum_acc_train], \n",
    "                                                         feed_dict={x: x_batch, y: y_batch})\n",
    "\n",
    "        writer.add_summary(sum_1, global_step=i)\n",
    "        writer.add_summary(sum_2, global_step=i)\n",
    "    #     print('Training - i:', i+1, 'Loss:', loss_val, 'Accuracy:', acc_val)\n",
    "\n",
    "        # Validate every 50 iterations\n",
    "        if i % 50 == 0:\n",
    "            acc_val, sum_1, sum_2 = session.run([accuracy, sum_loss_test, sum_acc_test], \n",
    "                                                feed_dict={x: X_test, y: Y_test})\n",
    "\n",
    "            writer.add_summary(sum_1, global_step=i)\n",
    "            writer.add_summary(sum_2, global_step=i)\n",
    "            print('Validation - i:', i+1, ' Accuracy:', acc_val)\n",
    "    \n",
    "\n",
    "    # Validate the model with unseen data\n",
    "    acc_val = session.run([accuracy], feed_dict={x: X_valid, y: Y_valid})\n",
    "\n",
    "    # Print test metrics\n",
    "    print('Accuracy:', acc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We validate the model with the data it has not seen yet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with session:\n",
    "#     # Validate the model with unseen data\n",
    "#     acc_val = session.run([accuracy], feed_dict={x: X_valid, y: Y_valid})\n",
    "\n",
    "#     # Print test metrics\n",
    "#     print('Accuracy:', acc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting & Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '{}.ckpt'.format(os.path.join(__MODEL_PATH, model_fn.__name__, model_fn.__name__))\n",
    "\n",
    "model_to_load = model_5_2\n",
    "\n",
    "load_path = '{}.ckpt'.format(os.path.join(__MODEL_PATH, model_to_load.__name__, model_to_load.__name__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the model that worked best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with session:\n",
    "#     tf.train.Saver().save(session, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the model that worked best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as saved_session:\n",
    "    tf.train.Saver().restore(saved_session, load_path)\n",
    "\n",
    "    # Validate the model with unseen data\n",
    "    acc_val = saved_session.run([accuracy], feed_dict={x: X_valid, y: Y_valid})\n",
    "\n",
    "    # Print test metrics\n",
    "    print('Accuracy:', acc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Dataset\n",
    "\n",
    "- The dataset contains 5832 rows which is enough for this deep learning excersice.\n",
    "- \n",
    "\n",
    "### Model\n",
    "\n",
    "- We could clearly see when a model was overfitting when the validation accuracy was more than 1-2 percent lower than the testing accuracy.\n",
    "  - When this occured we increased the amount of dropout or we reduced the number of neurons in one ore more layers.\n",
    "\n",
    "#### Sizes\n",
    "\n",
    "#### Activation Functions\n",
    "\n",
    "#### Kernels & Strides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
