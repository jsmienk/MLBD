{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Postal Codes\n",
    "\n",
    "__Convolutional Neural Networks__\n",
    "\n",
    "_By Marnick van der Arend & Jeroen Smienk_\n",
    "\n",
    "![Sample Digits](digits-sample.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jeroen\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "__MODEL_PATH = 'conv-models'\n",
    "__DATA_PATH = 'dataset-images'\n",
    "__TENSOR_LOG_DIR = 'conv-logs'\n",
    "\n",
    "__LABELS = 10\n",
    "__IM_SIZE = 32\n",
    "__N_DIGITS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "480 Images consisting of 4 digit postal codes with the label as the file name e.g. `3365.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(image):\n",
    "    \"\"\"\n",
    "    Returns a binarized image where lighter values are set to 255 and the lower values set to 0.\n",
    "    \"\"\"\n",
    "    blur = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "    _, thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    return thresh\n",
    "\n",
    "def crop(image):\n",
    "    \"\"\"\n",
    "    Crop an 128x32 image to 4 32x32 images\n",
    "    \"\"\"\n",
    "    digits = []\n",
    "    x,y,w,h = 0,0,__IM_SIZE,__IM_SIZE\n",
    "    for i in range(__N_DIGITS):\n",
    "        x = i*w\n",
    "        digits.append(image[y:y+h, x:x+w])\n",
    "    return digits\n",
    "\n",
    "def get_images_in_path(path, extension):\n",
    "    \"\"\"\n",
    "    Create a list of all images and their file names (labels) in a certain path\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    for name in os.listdir(path):\n",
    "        if name.endswith(extension):\n",
    "            img = cv2.imread(os.path.join(path, name), cv2.IMREAD_GRAYSCALE)\n",
    "            label = name[:-len(extension)] # remove extension\n",
    "            digits = crop(binarize(img))\n",
    "            for i in range(__N_DIGITS):\n",
    "                images.append(digits[i])\n",
    "                labels.append(int(label[i]))\n",
    "    return np.array(images, dtype=np.int32), np.array(labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dataset\n",
    "\n",
    "We split the 480 images in 1920 individual black-and-white digits and save them with their correct label as a tuple in a list. We also binarize the images so there is less noise in the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = get_images_in_path(__DATA_PATH, '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what kind of digits are in the set by plotting them with their labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 1920 images\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x216 with 60 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Size of dataset: {len} images\".format(len=len(images)))\n",
    "\n",
    "PLOT_SIZE = 60\n",
    "ROW_WIDTH = 20\n",
    "plt.figure(figsize=(15, PLOT_SIZE / ROW_WIDTH))\n",
    "for i in range(PLOT_SIZE):\n",
    "    plt.subplot(PLOT_SIZE / ROW_WIDTH, ROW_WIDTH, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.xlabel(labels[i])\n",
    "    plt.imshow(images[i], cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also curious how well the digits are distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9    0.106250\n",
       "7    0.103646\n",
       "3    0.103646\n",
       "1    0.103125\n",
       "6    0.101562\n",
       "2    0.101562\n",
       "5    0.097396\n",
       "4    0.096875\n",
       "8    0.093750\n",
       "0    0.092188\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(labels, columns=['label'])\n",
    "df.label.value_counts() / len(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the Yahtzee dataset labels, the distribution of the digits is almost perfect. There are about 10% of every digit in the dataset. Which is pretty equally distributed. This is very good for our deep learning model, because it will prevent bias for a certain label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot Eencoding\n",
    "\n",
    "To compare the labels with the predictions of the neural network we need to 'one-hot' encode the labels:\n",
    "\n",
    "The index of the `1` is the correct label of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One hot encoded labels:\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(labels):\n",
    "    onehot = np.zeros((len(labels), __LABELS))\n",
    "    onehot[range(len(labels)), labels] = 1\n",
    "    return onehot\n",
    "\n",
    "onehot = one_hot_encode(labels)\n",
    "print('One hot encoded labels:')\n",
    "print(onehot[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine the data and the labels to be able to shuffle them without forgetting which labels belong to which images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (1920, 32, 32, 1)\n",
      "Labels shape: (1920, 10)\n"
     ]
    }
   ],
   "source": [
    "def get_shuffled_xy(images, onehot):\n",
    "    data = np.array([np.array([images[i], onehot[i]]) for i in range(len(images))])\n",
    "    shuffled_data = np.random.permutation(data)\n",
    "    x = np.array([t[0] for t in shuffled_data])\n",
    "    x = np.reshape(x, (len(x), 32, 32, 1))\n",
    "    y = np.array([t[1] for t in shuffled_data])\n",
    "    return x, y\n",
    "\n",
    "X, Y = get_shuffled_xy(images, onehot)\n",
    "\n",
    "print('Images shape: {}'.format(X.shape))\n",
    "print('Labels shape: {}'.format(Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into train, test, and validation sets.\n",
    "\n",
    "We use the train set to train; the test set to cross-validate during training; and the validation set to validate the model after the model is done training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split X (train, test, validation): (1387, 32, 32, 1) (245, 32, 32, 1) (288, 32, 32, 1)\n",
      "Split Y (train, test, validation): (1387, 10) (245, 10) (288, 10)\n"
     ]
    }
   ],
   "source": [
    "def get_validation_split(x, y, frac):\n",
    "    split = int(len(x) * frac)\n",
    "    return x[:split], y[:split], x[split:], y[split:]\n",
    "\n",
    "X_train, Y_train, X_valid, Y_valid = get_validation_split(X, Y, .85)\n",
    "\n",
    "def get_test_split(x_train, y_train, frac):\n",
    "    split = int(len(x_train) * frac)\n",
    "    return x_train[:split], y_train[:split], x_train[split:], y_train[split:]\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = get_test_split(X_train, Y_train, .85)\n",
    "\n",
    "print('Split X (train, test, validation):', X_train.shape, X_test.shape, X_valid.shape)\n",
    "print('Split Y (train, test, validation):', Y_train.shape, Y_test.shape, Y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that returns a random batch of a certain size. This batch is used in training to train faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, y, size):\n",
    "    batch = np.array([(x[i], y[i]) for i in range(len(x))])\n",
    "    random_batch = np.random.permutation(batch)[:size]\n",
    "    return np.array([x[0] for x in random_batch]), np.array([x[1] for x in random_batch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a function that does all the above so we can load an arbitrary dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folder_xy(path, extension):\n",
    "    images, labels = get_images_in_path(path, extension)\n",
    "    return get_shuffled_xy(images, one_hot_encode(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "We designed several models which were run with an iteration count of 1500 and a batch size of 20\n",
    "\n",
    "rank | name | layers | optimizer | score\n",
    "--- | --- | --- | --- | ---\n",
    "? | model_1 | Conv2D(f=64, k=3, s=1, act=relu) | Adam | 0.9444444\n",
    "? | model_2 | Conv2D(f=64, k=5, act=relu), MaxPool2D(size=2, s=2) | Adam | 0.9652778\n",
    "? | model_3 | Conv2D(f=32, k=3, s=1, act=relu), Conv2D(f=64, k=3, s=1, act=relu), MaxPool2D(size=2, s=2), Dropout(0.25), Dense(128, act=relu), Dropout(0.5) | Adam | 0.9722222\n",
    "? | model_4 | Conv2D(f=32, k=3, s=1, act=relu), Conv2D(f=32, k=3, s=1, act=relu), MaxPool2D(size=2, s=2), Conv2D(f=64, k=3, s=1, act=relu), Conv2D(f=64, k=3, s=1, act=relu), Dense(512, act=relu), Dropout(0.2) | Adam | 0.9930556"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model 1: BLUE\n",
    "- Model 2: RED\n",
    "- Model 3: LIGHT BLUE\n",
    "- Model 4: PINK\n",
    "- Model 5: GREEN\n",
    "- Model 6: GRAY\n",
    "- Model 7: ORANGE\n",
    "- Model 8: ORANGE\n",
    "\n",
    "### Batch Accuracy\n",
    "\n",
    "\n",
    "### Batch Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_1(x, output_shape):\n",
    "    \"\"\"\n",
    "    One Conv2d layer with a kernel size of 3 and \n",
    "    \"\"\"\n",
    "    conv1 = tf.layers.conv2d(x, filters=32, kernel_size=3, strides=1, padding=\"same\", \n",
    "                           activation=tf.nn.relu)\n",
    "    \n",
    "    flatten = tf.contrib.layers.flatten(conv1)\n",
    "    return tf.layers.dense(flatten, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2(x, output_shape):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    conv1 = tf.layers.conv2d(x, filters=32, kernel_size=5, \n",
    "                             padding=\"same\", activation=tf.nn.relu)\n",
    "    pool1 = tf.layers.max_pooling2d(conv1, pool_size=2, strides=2)\n",
    "    \n",
    "    flatten = tf.contrib.layers.flatten(pool1)\n",
    "    return tf.layers.dense(flatten, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_3(x, output_shape):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #32 convolution filters used each of size 3x3\n",
    "    conv1 = tf.layers.conv2d(x, filters=32, kernel_size = 3,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "    \n",
    "    #64 convolution filters used each of size 3x3\n",
    "    conv2 = tf.layers.conv2d(conv1, filters=64, kernel_size = 3,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "    \n",
    "    #choose the best features via pooling\n",
    "    pool1 = tf.layers.max_pooling2d(conv2, pool_size=2, strides=2)\n",
    "    \n",
    "    #randomly turn neurons on and off to improve convergence\n",
    "    dropout1 = tf.layers.dropout(pool1, rate=.25)\n",
    "    \n",
    "    #flatten since too many dimensions, we only want a classification output\n",
    "    flatten = tf.contrib.layers.flatten(dropout1)\n",
    "\n",
    "    #fully connected to get all relevant data\n",
    "    dense = tf.layers.dense(flatten, units=128, activation=tf.nn.relu)\n",
    "    \n",
    "    #one more dropout for convergence' sake :) \n",
    "    dropout2 = tf.layers.dropout(dense, rate=.5)\n",
    "\n",
    "    return tf.layers.dense(dropout2, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_4(x, output_shape):\n",
    "    \"\"\"\n",
    "    IMPLEMENT BATCH NORMALIZATION AFTER CONV LAYERS\n",
    "    \"\"\"\n",
    "    conv1 = tf.layers.conv2d(x, filters=32, kernel_size = 3,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(conv1, filters=32, kernel_size = 3,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "        \n",
    "    pool1 = tf.layers.max_pooling2d(conv2, pool_size=2, strides=2)\n",
    "\n",
    "    conv3 = tf.layers.conv2d(pool1, filters=64, kernel_size = 3,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "\n",
    "    conv4 = tf.layers.conv2d(conv3, filters=64, kernel_size = 3,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "\n",
    "    pool2 = tf.layers.max_pooling2d(conv4, pool_size=2, strides=2)\n",
    "\n",
    "    flatten = tf.contrib.layers.flatten(pool2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    dense = tf.layers.dense(flatten, units=512, activation=tf.nn.relu)\n",
    "\n",
    "    dropout = tf.layers.dropout(dense, rate=.2)\n",
    "    \n",
    "    return tf.layers.dense(dropout, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_5(x, output_shape):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    conv1 = tf.layers.conv2d(x, filters=32, kernel_size = 5,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "    conv2 = tf.layers.conv2d(conv1, filters=32, kernel_size = 5,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "    pool1 = tf.layers.max_pooling2d(conv2, pool_size=2, strides=2)\n",
    "    dropout1 = tf.layers.dropout(pool1, rate=.25)\n",
    "    \n",
    "    conv3 = tf.layers.conv2d(x, filters=64, kernel_size = 3,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "    conv4 = tf.layers.conv2d(conv3, filters=64, kernel_size = 3,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "    pool2 = tf.layers.max_pooling2d(conv4, pool_size=2, strides=2)\n",
    "    dropout2 = tf.layers.dropout(pool2, rate=.25)\n",
    "    \n",
    "    flatten = tf.contrib.layers.flatten(dropout2)\n",
    "    dense = tf.layers.dense(flatten, units=256, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(dense, rate=.5)\n",
    "\n",
    "    return tf.layers.dense(dropout, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_6(x, output_shape):\n",
    "    \n",
    "    conv1 = tf.layers.conv2d(x, filters=32, kernel_size = 3,\n",
    "                         strides = 1, padding = \"valid\",\n",
    "                         kernel_initializer = tf.contrib.layers.xavier_initializer(uniform=False),\n",
    "                         activation = tf.nn.relu)\n",
    "    conv2 = tf.layers.conv2d(conv1, filters=32, kernel_size = 3,\n",
    "                         strides = 1, padding = \"valid\",\n",
    "                         kernel_initializer = tf.contrib.layers.xavier_initializer(uniform=False),\n",
    "                         activation = tf.nn.relu)\n",
    "    \n",
    "    pool1 = tf.layers.max_pooling2d(conv2, pool_size=2, strides=1)\n",
    "    dropout1 = tf.layers.dropout(pool1, rate=.2)\n",
    "    \n",
    "    conv3 = tf.layers.conv2d(dropout1, filters=64, kernel_size = 3,\n",
    "                         strides = 1, padding = \"same\",\n",
    "                         kernel_initializer = tf.contrib.layers.xavier_initializer(uniform=False),\n",
    "                         activation = tf.nn.relu)\n",
    "    conv4 = tf.layers.conv2d(conv3, filters=64, kernel_size = 3,\n",
    "                         strides = 1, padding = \"same\",\n",
    "                         kernel_initializer = tf.contrib.layers.xavier_initializer(uniform=False),\n",
    "                         activation = tf.nn.relu)\n",
    "    \n",
    "    pool2 = tf.layers.max_pooling2d(conv4, pool_size=2, strides=1)\n",
    "    dropout2 = tf.layers.dropout(pool2, rate=.25)\n",
    "    \n",
    "    conv4 = tf.layers.conv2d(dropout2, filters=128, kernel_size = 3,\n",
    "                         strides = 1, padding = \"same\",\n",
    "                         kernel_initializer = tf.contrib.layers.xavier_initializer(uniform=False),\n",
    "                         activation = tf.nn.relu)\n",
    "    \n",
    "    dropout3 = tf.layers.dropout(conv4, rate=.25)\n",
    "    flatten = tf.contrib.layers.flatten(dropout3)\n",
    "    dense = tf.layers.dense(flatten, units=128, activation=tf.nn.relu)\n",
    "    batch_norm = tf.layers.batch_normalization(dense, training=is_training)\n",
    "    dropout = tf.layers.dropout(batch_norm, rate=.25)\n",
    "    \n",
    "    return tf.layers.dense(dropout, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the placeholder for our 32x32 pixel input and 10-class output and choose a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, X_train.shape[1], X_train.shape[2], 1], name='x')\n",
    "y = tf.placeholder(tf.float32, shape=[None, Y_train.shape[1]], name='y')\n",
    "is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "\n",
    "model_fn = model_6\n",
    "y_pred = model_fn(x, Y_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose an optimizer and define a loss functon and the accuracy metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=y_pred)\n",
    "loss_fn = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# Optimizer minimizes the loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=.0001)\n",
    "# optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001).minimize(loss_fn)\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_op = optimizer.minimize(loss_fn)\n",
    "\n",
    "# Accuracy metric\n",
    "#   checks if the indices of the highest values in the real \n",
    "#   and predicted arrays are equal\n",
    "prediction = tf.equal(tf.argmax(y, axis=1), tf.argmax(y_pred, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Test, Validate\n",
    "\n",
    "We train the model using a certain batch size and for a number of iterations while posting scalars to TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'x_24' with dtype float and shape [?,32,32,1]\n\t [[node x_24 (defined at <ipython-input-142-56c79a867d76>:1)  = Placeholder[dtype=DT_FLOAT, shape=[?,32,32,1], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'x_24', defined at:\n  File \"/anaconda3/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/anaconda3/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/anaconda3/lib/python3.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/anaconda3/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/anaconda3/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/anaconda3/lib/python3.7/asyncio/base_events.py\", line 528, in run_forever\n    self._run_once()\n  File \"/anaconda3/lib/python3.7/asyncio/base_events.py\", line 1764, in _run_once\n    handle._run()\n  File \"/anaconda3/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/anaconda3/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/anaconda3/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/anaconda3/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/anaconda3/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-142-56c79a867d76>\", line 1, in <module>\n    x = tf.placeholder(tf.float32, shape=[None, X_train.shape[1], X_train.shape[2], 1], name='x')\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1747, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 5206, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'x_24' with dtype float and shape [?,32,32,1]\n\t [[node x_24 (defined at <ipython-input-142-56c79a867d76>:1)  = Placeholder[dtype=DT_FLOAT, shape=[?,32,32,1], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'x_24' with dtype float and shape [?,32,32,1]\n\t [[{{node x_24}} = Placeholder[dtype=DT_FLOAT, shape=[?,32,32,1], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-f70bb491d767>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         loss_val, _, acc_val, sum_1, sum_2 = session.run([train_op, accuracy, \n\u001b[1;32m     19\u001b[0m                                                           sum_loss_train, sum_acc_train], \n\u001b[0;32m---> 20\u001b[0;31m                                                          feed_dict={x: x_batch, y: y_batch, is_training: 1})\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'x_24' with dtype float and shape [?,32,32,1]\n\t [[node x_24 (defined at <ipython-input-142-56c79a867d76>:1)  = Placeholder[dtype=DT_FLOAT, shape=[?,32,32,1], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'x_24', defined at:\n  File \"/anaconda3/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/anaconda3/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/anaconda3/lib/python3.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/anaconda3/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/anaconda3/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/anaconda3/lib/python3.7/asyncio/base_events.py\", line 528, in run_forever\n    self._run_once()\n  File \"/anaconda3/lib/python3.7/asyncio/base_events.py\", line 1764, in _run_once\n    handle._run()\n  File \"/anaconda3/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/anaconda3/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/anaconda3/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/anaconda3/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/anaconda3/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-142-56c79a867d76>\", line 1, in <module>\n    x = tf.placeholder(tf.float32, shape=[None, X_train.shape[1], X_train.shape[2], 1], name='x')\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1747, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 5206, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'x_24' with dtype float and shape [?,32,32,1]\n\t [[node x_24 (defined at <ipython-input-142-56c79a867d76>:1)  = Placeholder[dtype=DT_FLOAT, shape=[?,32,32,1], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "iters = 1200 \n",
    "train_batch_size = 20\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "session = tf.Session()\n",
    "with session:\n",
    "    session.run(init_op)\n",
    "\n",
    "    # Defining the metrics we want to log in TensorBoard\n",
    "    sum_loss_train = tf.summary.scalar('loss_train', loss_fn)\n",
    "    sum_loss_test = tf.summary.scalar('loss_test', loss_fn)\n",
    "    sum_acc_train = tf.summary.scalar('acc_train', accuracy)\n",
    "    sum_acc_test = tf.summary.scalar('acc_test', accuracy)\n",
    "    tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(os.path.join(__TENSOR_LOG_DIR, model_fn.__name__), session.graph)\n",
    "    \n",
    "    # Start training for a certain number of iterations\n",
    "    for i in range(iters):\n",
    "        # Every iteration we get a random batch of the training data\n",
    "        x_batch, y_batch = get_batch(X_train, Y_train, train_batch_size)\n",
    "        # We train the model by providing the 'optimizer' variable to the run function.\n",
    "        # We also want to calculate the accuracy and loss TensorBoard metrics\n",
    "        train_op_val, acc_val, sum_1, sum_2 = session.run([train_op, accuracy, \n",
    "                                                       sum_loss_train, sum_acc_train], \n",
    "                                                       feed_dict={x: x_batch, y: y_batch, is_training: 1})\n",
    "        # Write the metrics to TensorBoard\n",
    "        writer.add_summary(sum_1, global_step=i)\n",
    "        writer.add_summary(sum_2, global_step=i)\n",
    "\n",
    "        # Validate every 50 iterations\n",
    "        if i % 50 == 0:\n",
    "            # DO NOT PROVIDE THE 'optimzer' VARIABLE HERE\n",
    "            # ELSE THE MODEL WILL TRAIN ON THE TEST DATA\n",
    "            acc_val, sum_1, sum_2 = session.run([accuracy, sum_loss_test, sum_acc_test], \n",
    "                                                feed_dict={x: X_test, y: Y_test, is_training: 0})\n",
    "            # Write the metrics to TensorBoard\n",
    "            writer.add_summary(sum_1, global_step=i)\n",
    "            writer.add_summary(sum_2, global_step=i)\n",
    "            print('Validation - i:', i+1, ' Accuracy:', acc_val)\n",
    "    \n",
    "\n",
    "    # Validate the model with unseen data\n",
    "    acc_val = session.run([accuracy], feed_dict={x: X_valid, y: Y_valid, is_training: 0})\n",
    "    print('Validation accuracy:', acc_val)\n",
    "    \n",
    "    # Save the model\n",
    "    saver.save(session, '{}.ckpt'.format(os.path.join(__MODEL_PATH, model_fn.__name__, model_fn.__name__)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Saved Model\n",
    "\n",
    "_Note: This requires all cells up to 'Train, Test, Validate' to be executed._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EDIT THIS TO RUN BEST MODEL\n",
    "\"\"\"\n",
    "__TEACHER_VALIDATION_PATH = 'dataset-images'\n",
    "__TEACHER_VALIDATION_EXTENSTION = '.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the model and the dataset and run just the accuracy metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from conv-models\\model_6\\model_6.ckpt\n",
      "Validation accuracy imported model_6: [0.9953125]\n"
     ]
    }
   ],
   "source": [
    "model_to_load = model_6\n",
    "\n",
    "load_path = '{}.ckpt'.format(os.path.join(__MODEL_PATH, \n",
    "                                          model_to_load.__name__, \n",
    "                                          model_to_load.__name__))\n",
    "\n",
    "X_teacher, Y_teacher = get_folder_xy(__TEACHER_VALIDATION_PATH, __TEACHER_VALIDATION_EXTENSTION)\n",
    "\n",
    "with tf.Session() as saved_session:\n",
    "    tf.train.Saver().restore(saved_session, load_path)\n",
    "\n",
    "    # Validate the model with unseen data\n",
    "    acc_val = saved_session.run([accuracy], feed_dict={x: X_teacher, y: Y_teacher, is_training: 0})\n",
    "    print('Validation accuracy imported {}: {}'.format(model_to_load.__name__, acc_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Dataset\n",
    "\n",
    "- The dataset only contains 1920 individual digits. This should be way too less to properly train a neural network.\n",
    "- The digits in the dataset are almost perfectly distributed; unlike Yahtzee. This prevents the model from learning a bias for more ocurring labels.\n",
    "\n",
    "### Model\n",
    "\n",
    "- We could clearly see when a model was overfitting when the validation accuracy was more than 1-2 percent lower than the testing accuracy.\n",
    "  - When this occured we increased the amount of dropout or we reduced the number of neurons in one ore more layers.\n",
    "\n",
    "#### Sizes\n",
    "\n",
    "#### Activation Functions\n",
    "\n",
    "#### Kernels & Strides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
