{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Postal Codes\n",
    "\n",
    "__Convolutional Neural Networks__\n",
    "\n",
    "_By Marnick van der Arend & Jeroen Smienk_\n",
    "\n",
    "![Sample Digits](digits_sample.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "__MODEL_PATH = 'conv_models'\n",
    "__DATA_PATH = 'dataset-images'\n",
    "__TENSOR_LOG_DIR = 'conv_logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "480 Images consisting of 4 digit postal codes with the label as the file name e.g. `3365.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(image):\n",
    "    \"\"\"\n",
    "    Crop an 128x32 image to 4 32x32 images\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : string\n",
    "    Path to images\n",
    "    \"\"\"\n",
    "    digits = []\n",
    "    x,y,w,h = 0,0,32,32\n",
    "    for i in range(4):\n",
    "        x = i*w\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        digits.append(gray[y:y+h, x:x+w])\n",
    "    return digits\n",
    "\n",
    "def get_images_in_path(path, extension):\n",
    "    \"\"\"\n",
    "    Create a list of all images and their file names (labels) in a certain path\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : string\n",
    "    Path to images\n",
    "    \"\"\"\n",
    "    img_list = []\n",
    "    for name in os.listdir(path):\n",
    "        if name.endswith(extension):\n",
    "            img = cv2.imread(os.path.join(path, name))\n",
    "            label = name[:-len(extension)] # remove extension\n",
    "            images = crop(img)\n",
    "            for i in range(4):\n",
    "                img_list.append((images[i], int(label[i])))\n",
    "    return img_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dataset\n",
    "\n",
    "We split the 480 images in 1920 individual black-and-white digits and save them with their correct label as a tuple in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = get_images_in_path(__DATA_PATH, '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what kind of digits are in the set by plotting them with their labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 1920 images\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of dataset: {len} images\".format(len=len(images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One hot encode the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One hot encoded labels:\n",
      "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([x[1] for x in images])\n",
    "labels = np.zeros((len(images),10))\n",
    "labels[np.arange(len(images)), a] = 1\n",
    "\n",
    "print(\"One hot encoded labels:\")\n",
    "print(labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shapes of training set\n",
    "data = np.array([x[0] for x in images])\n",
    "\n",
    "# Combine images and labels\n",
    "data_and_labels = np.array([(data[i], labels[i]) for i in np.arange(len(data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (1920, 32, 32, 1)\n",
      "Labels shape: (1920, 10)\n"
     ]
    }
   ],
   "source": [
    "shuffled = np.random.permutation(data_and_labels)\n",
    "\n",
    "X = np.array([x[0] for x in shuffled])\n",
    "Y = np.array([x[1] for x in shuffled])\n",
    "\n",
    "X = np.reshape(X, (len(X), 32, 32, 1))\n",
    "\n",
    "print(\"Images shape: {shape}\".format(shape=X.shape))\n",
    "print(\"Labels shape: {shape}\".format(shape=Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to classify these categorical labels, we have to 'one-hot encode' them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split X (train, test, validation): (1387, 32, 32, 1) (245, 32, 32, 1) (288, 32, 32, 1)\n",
      "Split Y (train, test, validation): (1387, 10) (245, 10) (288, 10)\n"
     ]
    }
   ],
   "source": [
    "split = int(len(X) * .85)\n",
    "X_train = X[:split]\n",
    "X_valid = X[split:]\n",
    "Y_train = Y[:split]\n",
    "Y_valid = Y[split:]\n",
    "\n",
    "split = int(len(X_train) * .85)\n",
    "X_test = X_train[split:]\n",
    "X_train = X_train[:split]\n",
    "Y_test = Y_train[split:]\n",
    "Y_train = Y_train[:split]\n",
    "\n",
    "print('Split X (train, test, validation):', X_train.shape, X_test.shape, X_valid.shape)\n",
    "print('Split Y (train, test, validation):', Y_train.shape, Y_test.shape, Y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split on the training data to create the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, labels, batch_size):\n",
    "    batch = np.array([(data[i], labels[i]) for i in np.arange(len(data))])\n",
    "    random_batch = np.random.permutation(batch)[:batch_size]\n",
    "    return np.array([x[0] for x in random_batch]), np.array([x[1] for x in random_batch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "We designed several models:\n",
    "\n",
    "rank | name | layers | score\n",
    "--- | --- | --- | ---\n",
    "5 | model_8 | (64, Tanh, Drop=.2) (128, Tanh, Drop=.3) (256, Tanh, Drop=.4) (512, Tanh, Drop=.5) (64, Tanh) | 0.93714285\n",
    "4 | model_7 | (64, ReLU) (128, ReLU) (256, ReLU) (512, ReLU, Drop=.3) (64, ReLU) | 0.94057140\n",
    "3 | model_6 | (200, Tanh) (300, Tanh) (600, Tanh) | 0.97828573\n",
    "1 | model_5_2 | (600, Tanh, Drop=.3) (300, Tanh, Drop=.3) (200, Tanh, Drop=.3) | 0.98742855\n",
    "2 | model_5 | (600, Tanh) (300, Tanh) (200, Tanh) | 0.97942860\n",
    "6 | model_4 | (128, Tanh) (64, Tanh) (32, Tanh) | 0.87771430\n",
    "7 | model_3 | (12, ReLU) (24, ReLU) (48, ReLU, Drop=.1) (96, ReLU) | 0.73028570\n",
    "8 | model_2 | (128, ReLU) (64, ReLU) (32, ReLU) | 0.82400000\n",
    "9 | model_1 | (128, Sigmoid) | 0.66628570\n",
    "\n",
    "Dropout has a positive effect on the score as can be seen in the table. We also found that the tanh activation function performed well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model 1: BLUE\n",
    "- Model 2: RED\n",
    "- Model 3: LIGHT BLUE\n",
    "- Model 4: PINK\n",
    "- Model 5: GREEN\n",
    "- Model 6: GRAY\n",
    "- Model 7: ORANGE\n",
    "- Model 8: ORANGE\n",
    "\n",
    "### Batch Accuracy\n",
    "\n",
    "\n",
    "### Batch Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_1(x, output_shape):\n",
    "    \"\"\"\n",
    "    One Conv2d layer with a kernel size of 3 and \n",
    "    \"\"\"\n",
    "    conv1 = tf.layers.conv2d(x, filters=32, kernel_size = 3,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "    \n",
    "    flatten = tf.contrib.layers.flatten(l_1)\n",
    "    return tf.layers.dense(flatten, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2(x, output_shape):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    conv1 = tf.layers.conv2d(x, filters=32, kernel_size=5, \n",
    "                             padding=\"same\", activation=tf.nn.relu)\n",
    "    pool1 = tf.layers.max_pooling2d(conv1, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    flatten = tf.contrib.layers.flatten(pool1)\n",
    "    return tf.layers.dense(flatten, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_3(x, output_shape):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #32 convolution filters used each of size 3x3\n",
    "    conv1 = tf.layers.conv2d(x, filters=32, kernel_size = 3,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "    \n",
    "    #64 convolution filters used each of size 3x3\n",
    "    conv2 = tf.layers.conv2d(conv1, filters=64, kernel_size = 3,\n",
    "                         strides = 1, padding=\"valid\",\n",
    "                         activation = tf.nn.relu)\n",
    "    \n",
    "    #choose the best features via pooling\n",
    "    pool1 = tf.layers.max_pooling2d(conv2, pool_size=2, strides=2)\n",
    "    \n",
    "    #randomly turn neurons on and off to improve convergence\n",
    "    dropout1 = tf.layers.dropout(pool1, rate=.25)\n",
    "    \n",
    "    #flatten since too many dimensions, we only want a classification output\n",
    "    flatten = tf.contrib.layers.flatten(dropout1)\n",
    "\n",
    "    #fully connected to get all relevant data\n",
    "    dense = tf.layers.dense(flatten, units=128, activation=tf.nn.relu)\n",
    "    \n",
    "    #one more dropout for convergence' sake :) \n",
    "    dropout2 = tf.layers.dropout(dense, rate=.5)\n",
    "\n",
    "    return tf.layers.dense(dropout2, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_4(x, output_shape):\n",
    "    \"\"\"\n",
    "    IMPLEMENT BATCH NORMALIZATION AFTER CONV LAYERS\n",
    "    \"\"\"\n",
    "    conv1 = tf.layers.conv2d(x, filters=32, kernel_size = 3,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(conv1, filters=32, kernel_size = 3,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "        \n",
    "    pool1 = tf.layers.max_pooling2d(conv2, pool_size=2, strides=2)\n",
    "\n",
    "    conv3 = tf.layers.conv2d(pool1, filters=64, kernel_size = 3,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "\n",
    "    conv4 = tf.layers.conv2d(conv3, filters=64, kernel_size = 3,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "\n",
    "    pool2 = tf.layers.max_pooling2d(conv4, pool_size=2, strides=2)\n",
    "\n",
    "    flatten = tf.contrib.layers.flatten(pool2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    dense = tf.layers.dense(flatten, units=512, activation=tf.nn.relu)\n",
    "\n",
    "    dropout = tf.layers.dropout(dense, rate=.2)\n",
    "    \n",
    "    return tf.layers.dense(dropout, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_5(x, output_shape):\n",
    "    \"\"\"\n",
    "    High number of neurons in layers, decreasing per layer\n",
    "    \"\"\"\n",
    "    l_1 = tf.layers.dense(x, units=600, activation=tf.nn.tanh)\n",
    "    l_2 = tf.layers.dense(l_1, units=300, activation=tf.nn.tanh)\n",
    "    l_3 = tf.layers.dense(l_2, units=200, activation=tf.nn.tanh)\n",
    "    return tf.layers.dense(l_3, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_5_2(x, output_shape):\n",
    "    \"\"\"\n",
    "    High number of neurons in layers, decreasing per layer\n",
    "    \"\"\"\n",
    "    l_1 = tf.layers.dense(x, units=600, activation=tf.nn.tanh)\n",
    "    d_1 = tf.layers.dropout(l_1, rate=.3)\n",
    "    l_2 = tf.layers.dense(d_1, units=300, activation=tf.nn.tanh)\n",
    "    d_2 = tf.layers.dropout(l_2, rate=.3)\n",
    "    l_3 = tf.layers.dense(d_2, units=200, activation=tf.nn.tanh)\n",
    "    d_3 = tf.layers.dropout(l_3, rate=.3)\n",
    "    return tf.layers.dense(d_3, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_6(x, output_shape):\n",
    "    \"\"\"\n",
    "    High number of neurons in layers, increasing per layer\n",
    "    \"\"\"\n",
    "    l_1 = tf.layers.dense(x, units=200, activation=tf.nn.tanh)\n",
    "    l_2 = tf.layers.dense(l_1, units=300, activation=tf.nn.tanh)\n",
    "    l_3 = tf.layers.dense(l_2, units=600, activation=tf.nn.tanh)\n",
    "    return tf.layers.dense(l_3, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_7(x, output_shape):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    l_1 = tf.layers.dense(x, units=64, activation=tf.nn.relu)\n",
    "    l_2 = tf.layers.dense(l_1, units=128, activation=tf.nn.relu)\n",
    "    l_3 = tf.layers.dense(l_2, units=256, activation=tf.nn.relu)\n",
    "    l_4 = tf.layers.dense(l_3, units=512, activation=tf.nn.relu)\n",
    "    d_4 = tf.layers.dropout(l_4, rate=.3)\n",
    "    l_5 = tf.layers.dense(d_4, units=64, activation=tf.nn.relu)\n",
    "    return tf.layers.dense(l_5, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_8(x, output_shape):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    l_1 = tf.layers.dense(x, units=64, activation=tf.nn.tanh)\n",
    "    d_1 = tf.layers.dropout(l_1, rate=.2)\n",
    "    l_2 = tf.layers.dense(d_1, units=128, activation=tf.nn.tanh)\n",
    "    d_2 = tf.layers.dropout(l_2, rate=.3)\n",
    "    l_3 = tf.layers.dense(d_2, units=256, activation=tf.nn.tanh)\n",
    "    d_3 = tf.layers.dropout(l_3, rate=.4)\n",
    "    l_4 = tf.layers.dense(d_3, units=512, activation=tf.nn.tanh)\n",
    "    d_4 = tf.layers.dropout(l_4, rate=.5)\n",
    "    l_5 = tf.layers.dense(d_4, units=64, activation=tf.nn.tanh)\n",
    "    return tf.layers.dense(l_5, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the placeholder for our 5-dice input and 7-class output and choose a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, X_train.shape[1], X_train.shape[2], 1], name='x')\n",
    "y = tf.placeholder(tf.float32, shape=[None, Y_train.shape[1]], name='y')\n",
    "\n",
    "model_fn = model_3\n",
    "y_pred = model_fn(x, Y_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose an optimizer, a loss functon and metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=y_pred)\n",
    "loss_fn = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# Optimizer minimizes the loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=.0005).minimize(loss_fn)\n",
    "\n",
    "# Accuracy metric\n",
    "#   checks if the indices of the highest values in the real \n",
    "#   and predicted arrays are equal\n",
    "prediction = tf.equal(tf.argmax(y, axis=1), tf.argmax(y_pred, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model using a certain batch size and for a number of iterations while posting scalars to TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - i: 1  Accuracy: 0.08163265\n",
      "Validation - i: 51  Accuracy: 0.8857143\n",
      "Validation - i: 101  Accuracy: 0.94285715\n",
      "Validation - i: 151  Accuracy: 0.94285715\n",
      "Validation - i: 201  Accuracy: 0.9265306\n",
      "Validation - i: 251  Accuracy: 0.89387757\n",
      "Validation - i: 301  Accuracy: 0.9265306\n",
      "Validation - i: 351  Accuracy: 0.93877554\n",
      "Validation - i: 401  Accuracy: 0.93061227\n",
      "Validation - i: 451  Accuracy: 0.955102\n",
      "Validation - i: 501  Accuracy: 0.955102\n",
      "Validation - i: 551  Accuracy: 0.9591837\n",
      "Validation - i: 601  Accuracy: 0.9510204\n",
      "Validation - i: 651  Accuracy: 0.9591837\n",
      "Validation - i: 701  Accuracy: 0.9591837\n",
      "Validation - i: 751  Accuracy: 0.955102\n",
      "Validation - i: 801  Accuracy: 0.955102\n",
      "Validation - i: 851  Accuracy: 0.9510204\n",
      "Validation - i: 901  Accuracy: 0.9591837\n",
      "Validation - i: 951  Accuracy: 0.9510204\n",
      "Validation - i: 1001  Accuracy: 0.955102\n",
      "Validation - i: 1051  Accuracy: 0.9632653\n",
      "Validation - i: 1101  Accuracy: 0.955102\n",
      "Validation - i: 1151  Accuracy: 0.9591837\n",
      "Validation - i: 1201  Accuracy: 0.9591837\n",
      "Validation - i: 1251  Accuracy: 0.9591837\n",
      "Validation - i: 1301  Accuracy: 0.9591837\n",
      "Validation - i: 1351  Accuracy: 0.9591837\n",
      "Validation - i: 1401  Accuracy: 0.9591837\n",
      "Validation - i: 1451  Accuracy: 0.9591837\n",
      "Accuracy: [0.9444444]\n"
     ]
    }
   ],
   "source": [
    "iters = 1500\n",
    "train_batch_size = 20\n",
    "\n",
    "session = tf.Session()\n",
    "with session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    sum_loss_train = tf.summary.scalar('loss_train', loss_fn)\n",
    "    sum_loss_test = tf.summary.scalar('loss_test', loss_fn)\n",
    "    sum_acc_train = tf.summary.scalar('acc_train', accuracy)\n",
    "    sum_acc_test = tf.summary.scalar('acc_test', accuracy)\n",
    "    tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(os.path.join(__TENSOR_LOG_DIR, model_fn.__name__), session.graph)\n",
    "    \n",
    "    for i in range(iters):\n",
    "        x_batch, y_batch = get_batch(X_train, Y_train, train_batch_size)\n",
    "                \n",
    "        loss_val, _, acc_val, sum_1, sum_2 = session.run([loss_fn, optimizer, accuracy, \n",
    "                                                          sum_loss_train, sum_acc_train], \n",
    "                                                         feed_dict={x: x_batch, y: y_batch})\n",
    "\n",
    "        writer.add_summary(sum_1, global_step=i)\n",
    "        writer.add_summary(sum_2, global_step=i)\n",
    "    #     print('Training - i:', i+1, 'Loss:', loss_val, 'Accuracy:', acc_val)\n",
    "\n",
    "        # Validate every 50 iterations\n",
    "        if i % 50 == 0:\n",
    "            acc_val, sum_1, sum_2 = session.run([accuracy, sum_loss_test, sum_acc_test], \n",
    "                                                feed_dict={x: X_test, y: Y_test})\n",
    "\n",
    "            writer.add_summary(sum_1, global_step=i)\n",
    "            writer.add_summary(sum_2, global_step=i)\n",
    "            print('Validation - i:', i+1, ' Accuracy:', acc_val)\n",
    "    \n",
    "\n",
    "    # Validate the model with unseen data\n",
    "    acc_val = session.run([accuracy], feed_dict={x: X_valid, y: Y_valid})\n",
    "\n",
    "    # Print test metrics\n",
    "    print('Accuracy:', acc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We validate the model with the data it has not seen yet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with session:\n",
    "#     # Validate the model with unseen data\n",
    "#     acc_val = session.run([accuracy], feed_dict={x: X_valid, y: Y_valid})\n",
    "\n",
    "#     # Print test metrics\n",
    "#     print('Accuracy:', acc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting & Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '{}.ckpt'.format(os.path.join(__MODEL_PATH, model_fn.__name__, model_fn.__name__))\n",
    "\n",
    "model_to_load = model_5_2\n",
    "\n",
    "load_path = '{}.ckpt'.format(os.path.join(__MODEL_PATH, model_to_load.__name__, model_to_load.__name__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the model that worked best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with session:\n",
    "#     tf.train.Saver().save(session, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the model that worked best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as saved_session:\n",
    "    tf.train.Saver().restore(saved_session, load_path)\n",
    "\n",
    "    # Validate the model with unseen data\n",
    "    acc_val = saved_session.run([accuracy], feed_dict={x: X_valid, y: Y_valid})\n",
    "\n",
    "    # Print test metrics\n",
    "    print('Accuracy:', acc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
