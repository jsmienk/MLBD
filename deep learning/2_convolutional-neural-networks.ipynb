{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Postal Codes\n",
    "\n",
    "__Convolutional Neural Networks__\n",
    "\n",
    "_By Marnick van der Arend & Jeroen Smienk_\n",
    "\n",
    "![Sample Digits](digits-sample.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "__MODEL_PATH = 'conv-models'\n",
    "__DATA_PATH = 'dataset-images'\n",
    "__TENSOR_LOG_DIR = 'conv-logs'\n",
    "\n",
    "__LABELS = 10\n",
    "__IM_SIZE = 32\n",
    "__N_DIGITS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "480 Images consisting of 4 digit postal codes with the label as the file name e.g. `3365.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(image):\n",
    "    \"\"\"\n",
    "    Returns a binarized image where lighter values are set to 255 and the lower values set to 0.\n",
    "    \"\"\"\n",
    "    blur = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "    _, thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    return thresh\n",
    "\n",
    "def crop(image):\n",
    "    \"\"\"\n",
    "    Crop an 128x32 image to 4 32x32 images\n",
    "    \"\"\"\n",
    "    digits = []\n",
    "    x,y,w,h = 0,0,__IM_SIZE,__IM_SIZE\n",
    "    for i in range(__N_DIGITS):\n",
    "        x = i*w\n",
    "        digits.append(image[y:y+h, x:x+w])\n",
    "    return digits\n",
    "\n",
    "def get_images_in_path(path, extension):\n",
    "    \"\"\"\n",
    "    Create a list of all images and their file names (labels) in a certain path\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    for name in os.listdir(path):\n",
    "        if name.endswith(extension):\n",
    "            img = cv2.imread(os.path.join(path, name), cv2.IMREAD_GRAYSCALE)\n",
    "            label = name[:-len(extension)] # remove extension\n",
    "            digits = crop(binarize(img))\n",
    "            for i in range(__N_DIGITS):\n",
    "                images.append(digits[i])\n",
    "                labels.append(int(label[i]))\n",
    "    return np.array(images, dtype=np.int32), np.array(labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dataset\n",
    "\n",
    "We split the 480 images in 1920 individual black-and-white digits and save them with their correct label as a tuple in a list. We also binarize the images so there is less noise in the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = get_images_in_path(__DATA_PATH, '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what kind of digits are in the set by plotting them with their labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 1920 images\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x216 with 60 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Size of dataset: {len} images\".format(len=len(images)))\n",
    "\n",
    "PLOT_SIZE = 60\n",
    "ROW_WIDTH = 20\n",
    "plt.figure(figsize=(15, PLOT_SIZE / ROW_WIDTH))\n",
    "for i in range(PLOT_SIZE):\n",
    "    plt.subplot(PLOT_SIZE / ROW_WIDTH, ROW_WIDTH, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.xlabel(labels[i])\n",
    "    plt.imshow(images[i], cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also curious how well the digits are distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9    0.106250\n",
       "7    0.103646\n",
       "3    0.103646\n",
       "1    0.103125\n",
       "6    0.101562\n",
       "2    0.101562\n",
       "5    0.097396\n",
       "4    0.096875\n",
       "8    0.093750\n",
       "0    0.092188\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(labels, columns=['label'])\n",
    "df.label.value_counts() / len(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the Yahtzee dataset labels, the distribution of the digits is almost perfect. There are about 10% of every digit in the dataset. Which is pretty equally distributed. This is very good for our deep learning model, because it will prevent bias for a certain label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot Eencoding\n",
    "\n",
    "To compare the labels with the predictions of the neural network we need to 'one-hot' encode the labels:\n",
    "\n",
    "The index of the `1` is the correct label of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One hot encoded labels:\n",
      "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(labels):\n",
    "    onehot = np.zeros((len(labels), __LABELS))\n",
    "    onehot[range(len(labels)), labels] = 1\n",
    "    return onehot\n",
    "\n",
    "onehot = one_hot_encode(labels)\n",
    "print('One hot encoded labels:')\n",
    "print(onehot[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine the data and the labels to be able to shuffle them without forgetting which labels belong to which images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (1920, 32, 32, 1)\n",
      "Labels shape: (1920, 10)\n"
     ]
    }
   ],
   "source": [
    "def get_shuffled_xy(images, onehot):\n",
    "    data = np.array([np.array([images[i], onehot[i]]) for i in range(len(images))])\n",
    "    shuffled_data = np.random.permutation(data)\n",
    "    x = np.array([t[0] for t in shuffled_data])\n",
    "    x = np.reshape(x, (len(x), 32, 32, 1))\n",
    "    y = np.array([t[1] for t in shuffled_data])\n",
    "    return x, y\n",
    "\n",
    "X, Y = get_shuffled_xy(images, onehot)\n",
    "\n",
    "print('Images shape: {}'.format(X.shape))\n",
    "print('Labels shape: {}'.format(Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into train, test, and validation sets.\n",
    "\n",
    "We use the train set to train; the test set to cross-validate during training; and the validation set to validate the model after the model is done training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split X (train, test, validation): (1387, 32, 32, 1) (245, 32, 32, 1) (288, 32, 32, 1)\n",
      "Split Y (train, test, validation): (1387, 10) (245, 10) (288, 10)\n"
     ]
    }
   ],
   "source": [
    "def get_split(x, y, frac):\n",
    "    split = int(len(x) * frac)\n",
    "    return x[:split], y[:split], x[split:], y[split:]\n",
    "\n",
    "X_train, Y_train, X_valid, Y_valid = get_split(X, Y, .85)\n",
    "X_train, Y_train, X_test, Y_test = get_split(X_train, Y_train, .85)\n",
    "\n",
    "print('Split X (train, test, validation):', X_train.shape, X_test.shape, X_valid.shape)\n",
    "print('Split Y (train, test, validation):', Y_train.shape, Y_test.shape, Y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that returns a random batch of a certain size. This batch is used in training to train faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, y, size):\n",
    "    batch = np.array([(x[i], y[i]) for i in range(len(x))])\n",
    "    random_batch = np.random.permutation(batch)[:size]\n",
    "    return np.array([x[0] for x in random_batch]), np.array([x[1] for x in random_batch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a function that does all the above so we can load an arbitrary dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folder_xy(path, extension):\n",
    "    images, labels = get_images_in_path(path, extension)\n",
    "    return get_shuffled_xy(images, one_hot_encode(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "We designed several models which were run with an iteration count of 1500 and a batch size of 20\n",
    "\n",
    "rank | name | layers | optimizer | score\n",
    "--- | --- | --- | --- | ---\n",
    "5 | model_1 | Conv2D(f=64, k=3, s=1, act=relu) | Adam | 0.9444444\n",
    "6 | model_2 | Conv2D(f=64, k=5, act=relu), MaxPool2D(size=2, s=2) | Adam | 0.9166667\n",
    "4 | model_3 | Conv2D(f=32, k=3, s=1, act=relu), Conv2D(f=64, k=3, s=1, act=relu), MaxPool2D(size=2, s=2), Dropout(0.25), Dense(128, act=relu), Dropout(0.5) | Adam | 0.9444444\n",
    "2 | model_4 | Conv2D(f=32, k=3, s=1, act=relu), BatchNorm, Conv2D(f=32, k=3, s=1, act=relu), BatchNorm, MaxPool2D(size=2, s=2), Conv2D(f=64, k=3, s=1, act=relu), BatchNorm, Conv2D(f=64, k=3, s=1, act=relu), BatchNorm, Dense(512, act=relu), Dropout(0.2) | Adam | 0.9652778\n",
    "3 | model_5 | Conv2D(f=32, k=5, s=1, act=relu), Conv2D(f=32, k=5, s=1, act=relu), MaxPool2D(size=2, s=2), Dropout(.25), Conv2D(f=64, k=3, s=1, act=relu), Conv2D(f=64, k=3, s=1, act=relu), MaxPool2D(size=2, s=2), Dropout(.25), Dense(256, act=relu), Dropout(.5) | RMSProp | 0.9513889\n",
    "1 | model_6 | Conv2D(f=32, k=5, s=1, act=relu, pad=valid), Conv2D(f=32, k=5, s=1, act=relu, pad=valid), BatchNorm, MaxPool2D(size=2, s=1), Dropout(.2), Conv2D(f=64, k=3, s=1, act=relu, pad=same), Conv2D(f=64, k=3, s=1, act=relu, pad=same), BatchNorm, MaxPool2D(size=2, s=1), Dropout(.25), Conv2D(f=128, k=3, s=1, act=relu, pad=same), Dropout(.25), Dense(128), BatchNorm, Dropout(.25) | RMSProp | 0.9826389"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model 1: BLUE\n",
    "- Model 2: RED\n",
    "- Model 3: LIGHT BLUE\n",
    "- Model 4: PINK\n",
    "- Model 5: GREEN\n",
    "- Model 6: GRAY\n",
    "- Model 7: ORANGE\n",
    "- Model 8: ORANGE\n",
    "\n",
    "### Batch Accuracy\n",
    "\n",
    "\n",
    "### Batch Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_1(x, output_shape, is_training):\n",
    "    \"\"\"\n",
    "    One Conv2d layer with a kernel size of 3 and \n",
    "    \"\"\"\n",
    "    conv1 = tf.layers.conv2d(x, filters=32, kernel_size=3, strides=1, padding=\"same\", \n",
    "                           activation=tf.nn.relu)\n",
    "    \n",
    "    flatten = tf.contrib.layers.flatten(conv1)\n",
    "    return tf.layers.dense(flatten, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2(x, output_shape, is_training):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    conv1 = tf.layers.conv2d(x, filters=32, kernel_size=5, \n",
    "                             padding=\"same\", activation=tf.nn.relu)\n",
    "    pool1 = tf.layers.max_pooling2d(conv1, pool_size=2, strides=2)\n",
    "    \n",
    "    flatten = tf.contrib.layers.flatten(pool1)\n",
    "    return tf.layers.dense(flatten, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_3(x, output_shape, is_training):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #32 convolution filters used each of size 3x3\n",
    "    conv1 = tf.layers.conv2d(x, filters=32, kernel_size = 3,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "    \n",
    "    #64 convolution filters used each of size 3x3\n",
    "    conv2 = tf.layers.conv2d(conv1, filters=64, kernel_size = 3,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "    \n",
    "    #choose the best features via pooling\n",
    "    pool1 = tf.layers.max_pooling2d(conv2, pool_size=2, strides=2)\n",
    "    \n",
    "    #randomly turn neurons on and off to improve convergence\n",
    "    dropout1 = tf.layers.dropout(pool1, rate=.25)\n",
    "    \n",
    "    #flatten since too many dimensions, we only want a classification output\n",
    "    flatten = tf.contrib.layers.flatten(dropout1)\n",
    "\n",
    "    #fully connected to get all relevant data\n",
    "    dense = tf.layers.dense(flatten, units=128, activation=tf.nn.relu)\n",
    "    \n",
    "    #one more dropout for convergence' sake :) \n",
    "    dropout2 = tf.layers.dropout(dense, rate=.5)\n",
    "\n",
    "    return tf.layers.dense(dropout2, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_4(x, output_shape, is_training):\n",
    "    conv1 = tf.layers.conv2d(x, filters=32, kernel_size = 3,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "    batch_norm1 = tf.layers.batch_normalization(conv1, training=is_training)\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(batch_norm1, filters=32, kernel_size = 3,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "    batch_norm2 = tf.layers.batch_normalization(conv2, training=is_training)\n",
    "        \n",
    "    pool1 = tf.layers.max_pooling2d(batch_norm2, pool_size=2, strides=2)\n",
    "\n",
    "    conv3 = tf.layers.conv2d(pool1, filters=64, kernel_size = 3,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "    batch_norm3 = tf.layers.batch_normalization(conv3, training=is_training)\n",
    "\n",
    "    \n",
    "    conv4 = tf.layers.conv2d(batch_norm3, filters=64, kernel_size = 3,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "    batch_norm4 = tf.layers.batch_normalization(conv4, training=is_training)\n",
    "\n",
    "    pool2 = tf.layers.max_pooling2d(batch_norm4, pool_size=2, strides=2)\n",
    "\n",
    "    flatten = tf.contrib.layers.flatten(pool2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    dense = tf.layers.dense(flatten, units=512, activation=tf.nn.relu)\n",
    "\n",
    "    dropout = tf.layers.dropout(dense, rate=.2)\n",
    "    \n",
    "    return tf.layers.dense(dropout, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_5(x, output_shape, is_training):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    conv1 = tf.layers.conv2d(x, filters=32, kernel_size = 5,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "    conv2 = tf.layers.conv2d(conv1, filters=32, kernel_size = 5,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "    pool1 = tf.layers.max_pooling2d(conv2, pool_size=2, strides=2)\n",
    "    dropout1 = tf.layers.dropout(pool1, rate=.25)\n",
    "    \n",
    "    conv3 = tf.layers.conv2d(x, filters=64, kernel_size = 3,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "    conv4 = tf.layers.conv2d(conv3, filters=64, kernel_size = 3,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "    pool2 = tf.layers.max_pooling2d(conv4, pool_size=2, strides=2)\n",
    "    dropout2 = tf.layers.dropout(pool2, rate=.25)\n",
    "    \n",
    "    flatten = tf.contrib.layers.flatten(dropout2)\n",
    "    dense = tf.layers.dense(flatten, units=256, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(dense, rate=.5)\n",
    "\n",
    "    return tf.layers.dense(dropout, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_6(x, output_shape, is_training):\n",
    "    \n",
    "    conv1 = tf.layers.conv2d(x, filters=32, kernel_size=5,\n",
    "                         strides=1, padding=\"valid\",\n",
    "                         kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),\n",
    "                         activation=tf.nn.relu)\n",
    "    conv2 = tf.layers.conv2d(conv1, filters=32, kernel_size=5,\n",
    "                         strides=1, padding=\"valid\",\n",
    "                         kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),\n",
    "                         activation=tf.nn.relu)\n",
    "    batch_norm1 = tf.layers.batch_normalization(conv2, training=is_training)\n",
    "    \n",
    "    pool1 = tf.layers.max_pooling2d(batch_norm1, pool_size=2, strides=1)\n",
    "    dropout1 = tf.layers.dropout(pool1, rate=.2)\n",
    "    \n",
    "    conv3 = tf.layers.conv2d(dropout1, filters=64, kernel_size=3,\n",
    "                         strides=1, padding=\"same\",\n",
    "                         kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),\n",
    "                         activation=tf.nn.relu)\n",
    "    conv4 = tf.layers.conv2d(conv3, filters=64, kernel_size=3,\n",
    "                         strides=1, padding=\"same\",\n",
    "                         kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),\n",
    "                         activation=tf.nn.relu)\n",
    "    batch_norm2 = tf.layers.batch_normalization(conv4, training=is_training)\n",
    "    \n",
    "    pool2 = tf.layers.max_pooling2d(batch_norm2, pool_size=2, strides=1)\n",
    "    dropout2 = tf.layers.dropout(pool2, rate=.25)\n",
    "    \n",
    "    conv4 = tf.layers.conv2d(dropout2, filters=128, kernel_size=3,\n",
    "                         strides=1, padding=\"same\",\n",
    "                         kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),\n",
    "                         activation=tf.nn.relu)\n",
    "    \n",
    "    dropout3 = tf.layers.dropout(conv4, rate=.25)\n",
    "    flatten = tf.contrib.layers.flatten(dropout3)\n",
    "    dense = tf.layers.dense(flatten, units=128, activation=tf.nn.relu)\n",
    "    \n",
    "    batch_norm3 = tf.layers.batch_normalization(dense, training=is_training)\n",
    "    dropout = tf.layers.dropout(batch_norm3, rate=.25)\n",
    "    \n",
    "    return tf.layers.dense(dropout, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the placeholder for our 32x32 pixel input and 10-class output and choose a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(model):\n",
    "    tf.reset_default_graph()\n",
    "    x = tf.placeholder(tf.float32, shape=[None, X_train.shape[1], X_train.shape[2], 1], name='x')\n",
    "    y = tf.placeholder(tf.float32, shape=[None, Y_train.shape[1]], name='y')\n",
    "    is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "    return x, y, is_training, model(x, Y_train.shape[1], is_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose an optimizer and define a loss functon and the accuracy metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_ops(y, y_pred):\n",
    "    # Loss function\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=y_pred)\n",
    "    loss_fn = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    # Optimizer minimizes the loss\n",
    "#     optimizer = tf.train.AdamOptimizer(learning_rate=.0003)\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001)\n",
    "\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = optimizer.minimize(loss_fn)\n",
    "\n",
    "    # Accuracy metric\n",
    "    #   checks if the indices of the highest values in the real \n",
    "    #   and predicted arrays are equal\n",
    "    pred_op = tf.argmax(y_pred, axis=1)\n",
    "    label_op = tf.argmax(y, axis=1)\n",
    "    correct_op = tf.equal(label_op, pred_op)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_op, tf.float32))\n",
    "    return loss_fn, train_op, pred_op, label_op, correct_op, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Test, Validate\n",
    "\n",
    "We train the model using a certain batch size and for a number of iterations while posting scalars to TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, data, split_frac=.85, verbose=2, save=False):\n",
    "    X, Y = data\n",
    "    x, y, is_training, y_pred = setup_model(model)\n",
    "    loss_fn, train_op, _, _, _, accuracy = setup_ops(y, y_pred)\n",
    "    \n",
    "    iters = (32*50) \n",
    "    train_batch_size = 20\n",
    "\n",
    "    X_train, Y_train, X_valid, Y_valid = get_split(X, Y, split_frac)    \n",
    "    X_train, Y_train, X_test, Y_test = get_split(X_train, Y_train, split_frac)\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    session = tf.Session()\n",
    "    with session:\n",
    "        session.run(init_op)\n",
    "\n",
    "        # Defining the metrics we want to log in TensorBoard\n",
    "        sum_loss_train = tf.summary.scalar('loss_train', loss_fn)\n",
    "        sum_loss_test = tf.summary.scalar('loss_test', loss_fn)\n",
    "        sum_acc_train = tf.summary.scalar('acc_train', accuracy)\n",
    "        sum_acc_test = tf.summary.scalar('acc_test', accuracy)\n",
    "        tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter(os.path.join(__TENSOR_LOG_DIR, model.__name__), session.graph)\n",
    "\n",
    "        # Start training for a certain number of iterations\n",
    "        for i in range(iters):\n",
    "            # Every iteration we get a random batch of the training data\n",
    "            x_batch, y_batch = get_batch(X_train, Y_train, train_batch_size)\n",
    "            # We train the model by providing the 'optimizer' variable to the run function.\n",
    "            # We also want to calculate the accuracy and loss TensorBoard metrics\n",
    "            train_op_val, acc_val, sum_1, sum_2 = session.run([train_op, accuracy, \n",
    "                                                           sum_loss_train, sum_acc_train], \n",
    "                                                           feed_dict={x: x_batch, y: y_batch, is_training: 1})\n",
    "            # Write the metrics to TensorBoard\n",
    "            writer.add_summary(sum_1, global_step=i)\n",
    "            writer.add_summary(sum_2, global_step=i)\n",
    "\n",
    "            # Validate every 50 iterations\n",
    "            if i % 50 == 0:\n",
    "                # DO NOT PROVIDE THE 'optimzer' VARIABLE HERE\n",
    "                # ELSE THE MODEL WILL TRAIN ON THE TEST DATA\n",
    "                acc_val, sum_1, sum_2 = session.run([accuracy, sum_loss_test, sum_acc_test], \n",
    "                                                    feed_dict={x: X_test, y: Y_test, is_training: 0})\n",
    "                # Write the metrics to TensorBoard\n",
    "                writer.add_summary(sum_1, global_step=i)\n",
    "                writer.add_summary(sum_2, global_step=i)\n",
    "                if verbose >= 2:\n",
    "                    print('Validation - i:', i+1, ' Accuracy:', acc_val)\n",
    "\n",
    "\n",
    "        # Validate the model with unseen data\n",
    "        acc_ = session.run([accuracy], feed_dict={x: X_valid, y: Y_valid, is_training: 0})\n",
    "        if verbose >= 1:\n",
    "            print('Validation accuracy:', acc_)\n",
    "\n",
    "        # Save the model\n",
    "        if save:\n",
    "            path = saver.save(session, '{}.ckpt'.format(os.path.join(__MODEL_PATH, model.__name__, model.__name__)))\n",
    "            if verbose >= 1:\n",
    "                print('Model saved at: {}'.format(path))\n",
    "    \n",
    "        return acc_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - i: 1  Accuracy: 0.19591837\n",
      "Validation - i: 51  Accuracy: 0.62857145\n",
      "Validation - i: 101  Accuracy: 0.8530612\n",
      "Validation - i: 151  Accuracy: 0.7020408\n",
      "Validation - i: 201  Accuracy: 0.877551\n",
      "Validation - i: 251  Accuracy: 0.9142857\n",
      "Validation - i: 301  Accuracy: 0.955102\n",
      "Validation - i: 351  Accuracy: 0.9918367\n",
      "Validation - i: 401  Accuracy: 0.97959185\n",
      "Validation - i: 451  Accuracy: 0.9755102\n",
      "Validation - i: 501  Accuracy: 0.9755102\n",
      "Validation - i: 551  Accuracy: 0.98367345\n",
      "Validation - i: 601  Accuracy: 0.9918367\n",
      "Validation - i: 651  Accuracy: 0.9918367\n",
      "Validation - i: 701  Accuracy: 0.97959185\n",
      "Validation - i: 751  Accuracy: 0.9877551\n",
      "Validation - i: 801  Accuracy: 0.98367345\n",
      "Validation - i: 851  Accuracy: 0.97959185\n",
      "Validation - i: 901  Accuracy: 0.9877551\n",
      "Validation - i: 951  Accuracy: 0.9755102\n",
      "Validation - i: 1001  Accuracy: 0.97959185\n",
      "Validation - i: 1051  Accuracy: 0.9918367\n",
      "Validation - i: 1101  Accuracy: 0.9918367\n",
      "Validation - i: 1151  Accuracy: 0.9918367\n",
      "Validation - i: 1201  Accuracy: 0.9877551\n",
      "Validation - i: 1251  Accuracy: 0.9959184\n",
      "Validation - i: 1301  Accuracy: 0.9755102\n",
      "Validation - i: 1351  Accuracy: 0.97959185\n",
      "Validation - i: 1401  Accuracy: 0.97959185\n",
      "Validation - i: 1451  Accuracy: 0.98367345\n",
      "Validation - i: 1501  Accuracy: 0.9918367\n",
      "Validation - i: 1551  Accuracy: 0.9877551\n",
      "Validation accuracy: [0.9722222]\n",
      "Model saved at: conv-models/model_6/model_6.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9722222]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(model_6, (X, Y), verbose=2, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "\n",
    "We shuffle and train the model multiple times to rule out luck:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_train(model, dir_path, cv=3, split_frac=.85):\n",
    "    accuracies = []\n",
    "    for i in range(cv):\n",
    "        print('{}/{}'.format(i, cv))\n",
    "        accuracies.append(train_model(model, get_folder_xy(dir_path, \".png\"), verbose=0, save=False))\n",
    "    return cv, np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/3\n",
      "1/3\n",
      "2/3\n",
      "Cross validation (3x): 0.9872685074806213\n"
     ]
    }
   ],
   "source": [
    "cv, mean_acc = cross_validate_train(model_6, __DATA_PATH)\n",
    "print('Cross validation ({}x): {}'.format(cv, mean_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Saved Model\n",
    "\n",
    "_Note: This requires all cells up to 'Train, Test, Validate' to be executed._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EDIT THIS TO RUN BEST MODEL\n",
    "\"\"\"\n",
    "__TEACHER_VALIDATION_PATH = 'dataset-images'\n",
    "__TEACHER_VALIDATION_EXTENSTION = '.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the model and the dataset and run just the accuracy metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_load = model_6\n",
    "\n",
    "load_path = '{}.ckpt'.format(os.path.join(__MODEL_PATH, \n",
    "                                          model_to_load.__name__, \n",
    "                                          model_to_load.__name__))\n",
    "\n",
    "X_teacher, Y_teacher = get_folder_xy(__TEACHER_VALIDATION_PATH, __TEACHER_VALIDATION_EXTENSTION)\n",
    "\n",
    "with tf.Session() as saved_session:\n",
    "    tf.train.Saver().restore(saved_session, load_path)\n",
    "\n",
    "    # Validate the model with unseen data\n",
    "    acc_, correct_, prediction_, label_ = saved_session.run([accuracy, correct, prediction, real_label], \n",
    "                                                             feed_dict={x: X_teacher, y: Y_teacher, is_training: 0})\n",
    "    print('Validation accuracy imported {}: {}'.format(model_to_load.__name__, acc_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Dataset\n",
    "\n",
    "- The dataset only contains 1920 individual digits. This should be way too less to properly train a neural network.\n",
    "- The digits in the dataset are almost perfectly distributed; unlike Yahtzee. This prevents the model from learning a bias for more ocurring labels.\n",
    "\n",
    "### Model\n",
    "\n",
    "- We could clearly see when a model was overfitting when the validation accuracy was more than 1-2 percent lower than the testing accuracy.\n",
    "  - When this occured we increased the amount of dropout or we reduced the number of neurons in one ore more layers.\n",
    "\n",
    "#### Filter sizes\n",
    "- We created different scenarios for the convolutional layers. For Conv2D we chose to use 32, 64 or 128 filters. We saw that the accuracy increased when using two convolutional layers of the same size before pooling. \n",
    "\n",
    "#### Activation Functions\n",
    "- We tried to change the activation function of our best model (6). We changed it from ReLu to Tanh, but this didn't seem to help. Our accuracy dropped from 97,6% to 93,4%.\n",
    "\n",
    "#### Optimizers\n",
    "- At first, we used the Adam optimizers for optimizing our neural networks, but after searching for other neural networks on the internet we saw that they were using the RMSProp optimizer. We tried it out on our models and model 5 and 6 saw an increase in accuracy of 1%. \n",
    "\n",
    "#### Kernels & Strides\n",
    "- We used two different kernel sizes for our convolutional layers. We used 3x3 and 5x5. We first created model 6 with the first 2 convolutional layers with a kernel size of 3x3. The score was 97,6%, but when we used a 5x5 kernel size on the first 2 conv layers the accuracy hit 98,3%! We think that there is a relation between the convolutional filters and the kernel size. A bigger kernel size on less filters and a smaller kernel size on a bigger filter should retrieve higher percentages. \n",
    "\n",
    "#### Batch Normalization\n",
    "- With Batch Normalization we saw a decrease in accuracy of 1,5% when we removed it from model 6. It makes a big difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
