{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Postal Codes\n",
    "\n",
    "__Convolutional Neural Networks__\n",
    "\n",
    "_By Marnick van der Arend & Jeroen Smienk_\n",
    "\n",
    "![Sample Digits](digits-sample.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "__MODEL_PATH = 'conv_models'\n",
    "__DATA_PATH = 'dataset-images'\n",
    "__TENSOR_LOG_DIR = 'conv-logs'\n",
    "\n",
    "__LABELS = 10\n",
    "__IM_SIZE = 32\n",
    "__N_DIGITS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "480 Images consisting of 4 digit postal codes with the label as the file name e.g. `3365.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(image):\n",
    "    \"\"\"\n",
    "    Returns a binarized image where lighter values are set to 255 and the lower values set to 0.\n",
    "    \"\"\"\n",
    "    blur = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "    _, thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    return thresh\n",
    "\n",
    "def crop(image):\n",
    "    \"\"\"\n",
    "    Crop an 128x32 image to 4 32x32 images\n",
    "    \"\"\"\n",
    "    digits = []\n",
    "    x,y,w,h = 0,0,__IM_SIZE,__IM_SIZE\n",
    "    for i in range(__N_DIGITS):\n",
    "        x = i*w\n",
    "        digits.append(image[y:y+h, x:x+w])\n",
    "    return digits\n",
    "\n",
    "def get_images_in_path(path, extension):\n",
    "    \"\"\"\n",
    "    Create a list of all images and their file names (labels) in a certain path\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    for name in os.listdir(path):\n",
    "        if name.endswith(extension):\n",
    "            img = cv2.imread(os.path.join(path, name), cv2.IMREAD_GRAYSCALE)\n",
    "            label = name[:-len(extension)] # remove extension\n",
    "            digits = crop(binarize(img))\n",
    "            for i in range(__N_DIGITS):\n",
    "                images.append(digits[i])\n",
    "                labels.append(int(label[i]))\n",
    "    return np.array(images, dtype=np.int32), np.array(labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dataset\n",
    "\n",
    "We split the 480 images in 1920 individual black-and-white digits and save them with their correct label as a tuple in a list. We also binarize the images so there is less noise in the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = get_images_in_path(__DATA_PATH, '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what kind of digits are in the set by plotting them with their labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 1920 images\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1oAAAC2CAYAAADAzPz6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3V/sJVWVL/DvksZAtxIMowZpFEwM0fAgYLgqSWeuiME/0dHcB0z0wYfrZIJecG5i1JeJjybGeB9uTAygTEQIw5/EeL0IiX9mfACxGwhg48ggQgtOQ/yLmgvoug+njlSfrqqzq2rvvdaq8/0knf53fqdWrf2nalft2iWqCiIiIiIiIsrnRdYBEBERERERLQ0HWkRERERERJlxoEVERERERJQZB1pERERERESZcaBFRERERESUGQdaREREREREmXGgRURERERElBkHWkRERERERJlxoEVERERERJQZB1pERERERESZ7RnzYRHRMZ+/4IILjvu3gwcPjvmKQaoqXf8+Ns62rpj7pO5LrjhTY5ua4xL5BMbltK1vP/riBMbFmhLX3PpaKqdtOfajZJw5+4GabWlO2c+Nc2qbGdK1PzXaUq4+P2cd7Ys3R6w54pxS/mNjr31cWqsdZ8mybsvVlta25dWyf1orGSNgV0f7jD0nyXWc34zf4vhZup231W5Lbbn6pzZRTY957A52fbfI1pjGfH+Wyj0mBx3bSvn+2XHOibG1vW3byN5ZzI27K+YcjXBkvU/+bMd2inXAOetEqTj7Ypya09ptySLOHOXas+2ubVVpSzn6/Zx1dCjmubHOjbP08ai1HbPjUq04c7alqcfP5mcX0z+1viPpc5bHzxJ96Zh+tNRxPspxKXeczXdWKfvU2FMGWsWmDpY6WcgtSpw51NxXVXWbW69xjbGEfYigZj0uua2cF7jWPLfxIdtittyviPmcIuJ+1oh57DbWddVzPj3HRmVZ1c0c28wZd5GBVu6r2KVE6gBy5c7DwUJEjvm17btyxjzlu7zUkwgHVe+m5o45P9aUE0IPvMSxBHNzWbosWNblDJ3jpRzXo6q9X6zD43nMGRfDICIiIiIiymzUYhi7pH3lwssI2WNMm3I/+5D5mb6t2/Ca1xTe9sPbnW0PObGw1KvLqebexdz1/NFK6XoQvX/azI+ImO5TyecwLUWOPRpVzZLv7He0vJ1cTdHVYfSx6ki85XPKlDYP0+C6yrrnYVeXB42+qRop9cNbHaL8Sj2bFUWuufoR9rl2e96V/iPKVLiaz5J6t4RBVoQ8p/K2LyMX45m9vZ26o1VrdSzqNrbCli6LKd+f6wpHDl7iiGLuFdaSZV9rdaySotxRnfN9NfqkSCeJffGk5p79abqU9mWRT+95W4sSp9f2v7ltD338FOv9aK24XHybWQdaURO/jcf9qrGsriXrQZb1tIc2L3GQLS8Huq624W3a0DZzBwgWovTdQHr5ezpx9Cxa+1pbx+lhGq6HGJbE24W1FFZlv1N3tKbwdHWh1nsBliri/peYihsxD7vMe3l5j2/TULxRTmBpWaa+o4h1dTzPFwCilqeXuui1He3UQCsloR4qSxfvgywvDW0XpJSlx+fJAPuTcs/TB7tM3Zb1HeHIfYHXvszTNLtN3vLlLZ5dFbUcPLa11D7XW9zRuVwMw7tclZCV+Xib78eak6OoHXROQ/Pwh36GuYunq62wj9lNHhdf6hO5v/H0XsI5F1S8TClemtp1Y+4FQPJ7zCx+R8vjjs99CM7jPnkV8VkIYLnxWS5R7KXdeL1jkWslJC95brOIqa+c2//WF5dlHRkz68JjWUdkfTfAY3801xL3ycqYiy8e78YBPuLqiqFGPd25O1pERERERESlZbujFfHqxdirlrzalY/XuwrAMvMN8Oo37Z5t/UzfnQxPy2d7lesdZYCfvol3C8fzXIZ9xtzVqNHmp85Q6Po5i3LwdM6U2uenflcOO39Hy0vnUFLEfbTqLFIaoadOxZNI09kiPz/oOc9ennlp8754zBxR497kcT+8xGTdpjd5iyfFtmfHve6T17iAac84emlTtRV/Rsvb1Y5dEzHvEWKOdmXb8opc1+c85C7q84M0XspzuV7Kfezdft6FOV6uGRNR+vnN/S0dd1d+vVwAmrOwiEUfMGWbJV77kmP7Y362dv20VG1599SdjtCp1eSpsuRk3VHkYHEw6/pMOx6Puk5yPQ24NnnrpNu8xjXEUxnPfRDa0750qTkw8NxOgOPjm1r2UQZbtXkv/5xK1QEvg1MLNc6f1tux5m7qYO2keCiELh6n4FB9216uGh3reLptB+Ul1AfPIlyVB+q2Ke91btt0sdR2w36qW0ruvNeRtgj96BIuUteU43VDc7l8YbHXK0ier8K3eY+PB61x5pan1wd5KQ8vOWa7zmtbuVo/wL+W486GdR1e4t2ZWnXA052DSFJe42CxFHktVq/8WKuZS5cDLcB2sBVhOlYf6wPWHNaxj13dZ/3v1nEP8fZsVgRjpmvWECl3VI+nE9w5sXjpPy0HWyW27SWv3o+RKbjidF4e6kPN/tPtQMuLvoc+PVSUiKLe9vZ0UtPF60O8Yz7joQ54Ld8+HnI2xGt8Ux9A97o/mywvUk5ZXpuOt6QcbdYJr/tWq/+PdpzZNOXCgNcyr3GBxd0zWkRERERERNFlG2ilPhTpaVSbY4lK2i2e6u+axV3CEvU/SpuqGSdz4suu7GdJHvvQ6Czr5ZSFu3a9HdVewdDCtneX7ZJqd7TaiU5NvkXF6VudiOaLOm0wolI5XepqmBH3y0u7iZa3uas3RthfjzFGPuGyiDtCnzR0TN9W3t72zVs8nnl7jtm7agOtKSfZLKxxPOerq/wjH3h30VIPRF73y2tcbSkrZ0XYD+D4i4E0XpSy7hI59tqG2vVm2xkadEXIOfuCcZiv43ExjAYrRzkROtNU3vYlykPG3swpR+Z4ZUwOvdzN9tZ+54qyP57bjPer80OLsHhpV6nbrbW42NjjovXsqSjtGPAfq8fXzWQdaG1bvSPSqk1tEV6i7DmvXfXCc7xjLGU/UpRenWeXcrmN95O/XGr2C1NXyer6udrHMp7c5OdtdcSh/jXCO5VSc2WxkvOUwWpNc6dYem1j3liNQarf0drcUS8Ds74lpj00whSeB7Gec9g3pTHlc7uo1DL3Xusu9Ssx8Pbcj9XGu67lROzPI8bsSTt/lu9My8lbO/cWT5fNc/2Q79FKCT51x0oXWt+VFSrLQ2PMUT+tWcaZUoZer8RNzZuHekvTpcy4GKPkoDBKHwT4jjXaYHUpAwDvvJyDTsUXKM9Xc1/cPqPltYLX4P3luGMsYR88Sr0TR3lwmtgy8ES2rmh9kod459ZRy5PwqdPzcsccNYcpvA6yPOfMGl9YTERERERElFmxgdac0W3tB1Br/lzJbUS4Urukqx5L2pcStr26wevKWJufZTn3K3ElurSll6eX/fMSR6po8XaptQ9Dd666fg19T6mYp3wv+/v4PJafu6mDHpO0yctKRF28r+7n5WH3pU0h8pBTms/rc21D5q6YZSHHokcly2Fq/+StbkTgMWdT6mft/Zj7iEOtiypLeB7LQz/qJY4U3mJ1NXXQa0W3xrz4wzKJjeXnT6QyiRTrLmM5Ee2muW0/Z98x9o7W0wB+nvrhwp3cawb+b3FxVjhgZMtnW4G4h+IEdiCnQPbYdzLOguU/Oc7KJ4bZ2lKfjPtTrI4CPuOMcFwC/MdZqU0VbUvsR4+zE8f5tUz7sHNtybjsX4jD0+01IiIiIiKiJXA1dZCIiIiIiGgJONAiIiIiIiLKrOpAS0ROFZGbROQhETksIm+puf1UIvIJEXlQRB4QketF5CTrmLqIyDUiclREHrCOZYiIXNHk8kERudI6nj4icpKI/FBE7mti/ax1TH2i5BQAROQEEblHRL5pHUufKPkUkXNE5N7Wr995jFdEzhSR7zb9/IMicoV1TH0C9fePisj9Tbn/yDqeLlHq51qEnAKAiFwqIj8RkYdF5FPW8fRhW8ovyPEzTLu3ONZXfUZLRK4F8G+qepWIvBjAXlX9TbUAEojIGQB+AOANqvonEbkRwLdU9au2kR1PRA4AeAbAP6vqudbxdBGRcwHcAOBCAM8CuA3AP6jqT00D6yCrJyf3qeozInIiVvXgClW90zi0Y0TKKQCIyD8CeBOAU1T1PdbxbIqWzzUROQHALwD8F1Wd/cB1TiJyOoDTVfWQiLwUwEEAf6eqPzYO7RjB+vtHAbxJVZ+2jiWF5/q5FiGnTR7/HcAlAI4AuBvAB9mWpotQ7mvej5+bPLd7q2N9tTtaInIKgAMArgYAVX3W2yCrZQ+Ak0VkD4C9AJ4wjqeTqv4rgF9Zx7HF6wHcqap/VNXnAXwfwPuNY+qkK880fz2x+eVxtZgwORWR/QDeDeAq61gGhMnnhosB/Ie3gxkAqOqTqnqo+fPvARwGcIZtVL1C9PcBua2fwVwI4GFVfURVn8XqRPF9xjH1YVvKKMjxc5Pndm9yrK85dfC1AJ4C8JXmNuhVIrKv4vaTqOovAHwewGMAngTwW1W93Taq0B4AcEBEThORvQDeBeBM45h6Nbfp7wVwFMAdqnqXdUwdIuX0iwA+CeAv1oEMiJTPtssAXG8dxDYichaA8wC4a0vB+nsFcLuIHBSRj1oHkyBC/YyQ0zMAPN76+xE4vGjBtlREhOPnJs/t3uRYX3OgtQfA+QC+pKrnAfgDAHdzjUXkZVhdLTobwKsA7BORD9lGFZeqHgbwOQB3YHWb9j4Az5sGNUBV/6yqbwSwH8CFza1mV6LkVETeA+Coqh60jmVIlHy2NVOv3wvgX6xjGSIiLwFwM4ArVfV31vFsCtbfX6Sq5wN4J4DLm6njLkWpn4iR066XAbmbacG2lFeU42eb93ZvdayvOdA6AuBI6w7BTVgNvLx5O4CfqepTqvocgFsAvNU4ptBU9WpVPV9VD2A11dH1sy8A0Exr/R6AS41D6RQkpxcBeG8zH/4GAG8Tka/ZhtQtSD7b3gngkKr+p3UgfZrnHG8GcJ2q3mIdT48w/b2qPtH8fhTArVhNKfPKff0EwuT0CI696r4fPqfksS3lFeb42eK+3Vsc66sNtFT1lwAeF5Fzmn+6GICrhzkbjwF4s4jsbRZHuBir5wtoIhF5RfP7qwF8AE5vK4vIy0Xk1ObPJ2N14HjINqpuEXKqqp9W1f2qehZW0wm+o6our3BGyOeGD8JxjE3feTWAw6r6Bet4BoTo70VkX7OoCJop9+/AahqMV67rJxAqp3cDeJ2InN3cMbgMwDeMY+rCtpRRpONnS4R2X/1Yv6f0BjZ8HMB1TWfxCICPVN7+Vqp6l4jcBOAQVrcU7wHwZduouonI9QD+FsDfiMgRAP+kqlfbRtXpZhE5DcBzAC5X1V9bB9TjdADXNqvmvAjAjarqdUnVKDmNIkw+m7nllwD4e+tYBlwE4MMA7m+eeQSAz6jqtwxjOk6g/v6VAG5dnb9iD4Cvq+pttiF1C1I/gSA5VdXnReRjAL4N4AQA16jqg8ZhHYdtabcFavfVj/VVl3cnIiIiIiLaBVVfWExERERERLQLONAiIiIiIiLKjAMtIiIiIiKizDjQIiIiIiIiyowDLSIiIiIiosw40CIiIiIiIsqMAy0iIiIiIqLMONAiIiIiIiLKjAMtIiIiIiKizPaM+bCI6NwNXnDBBYP/f/DgweTvUlXp+vfUOLfFMqRmnGt98Y6JZUiOOFNzOifmvjiBtFjHlnuJWEu1pamx7nqcudtW6Ta/TWrcc9vSGGP2pSv+knW0bW59jV72Jfp7wK4tzTnO9xlTP4Hd6EdT8jw23pxxljzu79I5HlD+PC9XW7I+H10T1fT8zmmEqdsR2Rpz+zsnV5ox+90nNdYclXtbvGPyNrCNWXGOzenUmOc0winlPie3pU4O+/Yjd05Lxdn6/rHftzP5nNNHze2bmu/INoCZui/t/agx0MpRD6KXfenjZ+02n+NY3yelfjafW1z/1Pqu5M9a9felj/s14vRwjtf6nlHbzVXuzXcV6UdLnI+ujbqjNTGI0psgrPKcoyHO2f6cn7GMfRvr3G7KfdAtxXvbr3HxYirvuUuxhH2w4D1vc+Or2Z96z2V0EfIbIUZge5zr//d2nE9hEbunci860Jp68h2lItWKNTWPXZ9jLvPw0MmVvtpVk/d4vcfnnaeDXCrrCxgRczaV9/6e8vF6zEz5WY911GtcKSLHPgcXwyAiIiIiIsqs2B0t71cS1t+fchdoaF9Kx5pjmgbAK/TRRbqb5XlK3prnfC79zsbGcy2GkbzASxyUl4iwbAtJyWvUu1nt77CetdRVhz2e1405b9412QdaNReZyCFlW0vorEt2GGNOrKPn0YLnQcGmCOUbKZ9TeD658Zhf6ymDc0WJc5O3aURDF1/7PrurvD9fnfM4ZDmoWW+z7xy0dmxjHk9ZwnlzLsUXw2jjCffu2WyEQ42v1p3MvoME62R51gflpZWxdT7XIg6wovOYU49XtXPnqXbePV8I8BBDLdYXBiIOXLouXtTKo6d8VRtopSZ2lxpubTU7iqHtWDeApVyBidZWvMfrJb6UeuglVqDMIMt6AQpP+d3kJbZdmQ0SgdVdF+vBR23W51DW7cl6+2ONuUtdUvGB1hIaoVUhWVeOkqI1WCuRTgS9T8ljfcsvak49xh2lT/TQlq14Wp66bypZ6Rgj1NFdYzngHdq2l7piPeDKuurg5k6MLfhd7sA3eamgu2TbQIH1c7m8le22O8IezHntxJifr81LfneBtzrg5WKRlziovLFtIEr5e2vblord0YpSGbaJNIUnEjbC7ZZyN8uDpTwc7cXYfI75/C7nNeWuVpTpWt77hIg8lvsu9Yee99Hy2afNbUc6dwHKx5V1oJU69SFKB+xxkDVmm1HyTMdbUtlZd6652vG2g0tuUaaS5Wb5ygzrurqOIXK5Rx1Ue6kXkcvei+htaBvrqXBdPJ4ve8EXFhMREREREWWWfergtlHrmHX4rXi6StA25Zm3Gvuy9KtHteW87e79fSeWcuTDYhpXlKljkYw9btV+DYVnU+P0VId5NyuN9/ja+u76dJWnp3eneWoXlEfV92h1YaVahsjluNRb3rWmunk5SSkp0glGDZEGAm0RLvRFErEOjOGpbniKpUtqXai9Hzm25z33tY3t/3c9f1WnDta+KjjFmMpT8iDjMTe7KGo5LP0EaK4xz2VZDiS9L5tbQqmcRsvZUB6i7YtnXnLpJY7S1n1qlP2NdA7gLadeVmseqm814qt2R8tbBciF03hi24W7Mbtsqf2OByUeyI54tZvi8dYvjJlV4S12mi53WVq/6L2NfesLig+0oi3zOAUHW3kwh/PxIJyPlymlfQMaL/0OV0K14aX8p4rwct0aS5enrtQcdbqud5Y5jVim0eIdUqv/5KqDGyIfuKLw0lC9xNE2NaaUqRgWdbs9TcTTlJGUdxUN8dJPeMhlLjVPZr1MadkmSpxTlay/Jd6dl1uJRxU89Qnr+ttXjz3F2iVa2yudT+/l1cVDzEXvaEW9mzXluYjoVxdringVx4PNq6secjhnYLjJYvW+KazaOdvNNLnuaqyxn3+B1zrpMaaaPJyPdG3fW32J+uhAhHNrD3VwSM3Yig20IlSEsaxPcL1X3FSeOto+nvM8JX+e92dtKfW7lAjtxps5A3oPFwMimLtoR+52X6KdlCr3kgOPWv1p1OfHog6yvPI2iPaEUweJiIiIiIgyKzLQWuLdrBQczZNHJdtd7u/23oaW3octHcuvriU8YxY9/tLaz90OPY819POUznJGVZd2eW+Wu/Uz2V7qVvaBlpcdKylKx+utLKzfZbAZS58o5ZtiSfsyxRL2f9vByls77xOpzfXF6i3X3uLp4618+2wu3hAl7i4lYo+cj7Yo7cazvvaxlDqSU7ZntHat4vbNRy25HOzc77RsAF5OsrzX04jznHPHnPvZAutnK6eIFGskQ3XLY849xuRduz9KfX5oSSeHS9qX2rznzqoO51hQyFtuN2Nq72PuWKu9sBjwmeypLA6AqfnzdnD2Ek/OFfLWPNdnT7HNfWDeQwyeXgZJ0y0pvx4unnk8Jnnq+7bJdaEqwj57aHteZtW0t9uOqa9dechdJKkL8fT9e876UHWgBfg6aS159ymHKR1wxMbobQA+9t0muWP3eOJSUlc9r10ntrU1T/WTli9S++5rq5H2wVqUi0DrbfXN5onaT3qMe51PD+U/9lzUwzHdkywDrVwdaslbd0PbSt2uhwrv/WpH5PcqReJtiuG2eHa5ky0tQl45iE3nqV336TqJmsq6/K23v423vh4Y9+jEttg5U6Cfh3PO9nZS7rwtRc5zlup3tLyL0hijxDlHhJNzr/FZxDV1sOWhLnubTkJ55DxJ9VYXrOLJ2V55kr1MkfLtoV17HEynWvqAK4csA63IlcS7SPO3d60OeHg+wpuUwRYNY39aV8piKTyJiIvlWkaOfooD7Xk81V9vOZ5bP3Pmli8sJiIiIiIiyizb1MFto7+ljXb7vrMETyPzyCLdHZzKQ2xj5udTNy8PQS9Bao76cl47x97vaO5CP0ppvNfVSCLkMuKrUjyo9oyW12czcvG26kv75zyxXuZ7KXmMIsLBI+rzWd7jW+PUrfys8zalXVvHHJnnfnTshTVP9cBTLECc9755XYxtk5fzPdOpg9aVZeksrsTO+X+vosbtxZT8MedERMvCfp120dg7Wk8D+HnOAGY0vNcM/F9SnJUa/ew41wrHmyXOOTEm/uxQnEBCrBU7+2xlvynzPhSLcy1TvEXiLFAfzNtSotltKYXnst+UIdYllP2ijp8O4gQy1NGM+7Fzx6UI506bvB6XulQsdyBO2b8Qg9dbfkRERERERFFx1UEiIiIiIqLMONAiIiIiIiLKrNpAS0TOFJHvishhEXlQRK6ote0xROQkEfmhiNzXxPlZ65j6iMilIvITEXlYRD5lHU+XKOUOxIoVAETkBBG5R0S+aR1LHxE5VURuEpGHmry+xTqmPhHyCQAi8qiI3C8i94rIj6zj6RMln0CMWCOUe7Dj5zlNLte/ficiV1rH1SVC/QTi9PcicoWIPNDUUZdlDsQ4xwMAEflEk8sHROR6ETnJOqYuVud41ZZ3B/A8gP+pqodE5KUADorIHar644oxpPh/AN6mqs+IyIkAfiAi/1dV77QOrE1ETgDwvwFcAuAIgLtF5BsO8xml3IFYsQLAFQAOAzjFOpAB/wvAbar630TkxQD2Wgc0IEI+1/6rqj5tHcQWkfIZJVbv5R7i+AkAqvoTAG8E/no8/QWAW02D6helfrrv70XkXAD/HcCFAJ4FcJuI/B9V/altZMeKco4nImcA+B8A3qCqfxKRGwFcBuCrpoF1MznHq3ZHS1WfVNVDzZ9/j1WncUat7afSlWeav57Y/PK4YsiFAB5W1UdU9VkANwB4n3FMx4lS7kCsWEVkP4B3A7jKOpY+InIKgAMArgYAVX1WVX9jG1W3CPmMJFI+I8XqXaDj56aLAfyHqmZdVTmHKPUzUH//egB3quofVfV5AN8H8H7jmLqEOMdr7AFwsojswWpw/YRxPJ2szvFMntESkbMAnAfgLovtb9Pcpr8XwFEAd6iqxzjPAPB46+9H4HRQsOa93NsCxPpFAJ8E8BfrQAa8FsBTAL7STHu5SkT2WQfVI0I+1xTA7SJyUEQ+ah1Mj0j5jBJrhHKPcvzcdBmA662D6BGlfkbp7x8AcEBEThORvQDeBeBM45i6hDjHU9VfAPg8gMcAPAngt6p6u21U29U8x6s+0BKRlwC4GcCVqvq72ttPoap/VtU3AtgP4MLmVrM3XS8HcHvlMEK5r3mPVUTeA+Coqh60jmWLPQDOB/AlVT0PwB8AuJtnHiifaxep6vkA3gngchE5YB1QW6R8RooVzst9Lcjx86+aKW7vBfAv1rFsClY/Q/T3qnoYwOcA3AHgNgD3YTWlzJsQ53gi8jKs7rSdDeBVAPaJyIdsoxpW+xyv6kCrmbN9M4DrVPWWmtueornt/T0AlxqH0uUIjr0Ksx9Ob9dGKvcgsV4E4L0i8ihW0wneJiJfsw2p0xEAR1pXtG/C6kDsTZR8AgBU9Ynm96NYPVNyoW1Ex4mUzzCxBij3Yzg/fra9E8AhVf1P60A6hKmfiNPfQ1WvVtXzVfUAgF8BcPV8ViPKOd7bAfxMVZ9S1ecA3ALgrcYx9bI4x6u56qBgNXf3sKp+odZ2xxKRl4vIqc2fT8aqEj1kG1WnuwG8TkTObq7IXQbgG8YxHSdKuQNxYlXVT6vqflU9C6ty/46quruCpKq/BPC4iJzT/NPFAFw9yAvEyScAiMi+5iFeNNNy3oHVVBg3IuUzSqwRyh0Idfxs+yCcThuMUj+BOP09AIjIK5rfXw3gA/BZ/iHO8bCaMvhmEdnbnENdjNWzT+5YnePVXHXwIgAfBnB/M38bAD6jqt+qGEOK0wFc26z48iIAN6qquyVVVfV5EfkYgG8DOAHANar6oHFYXaKUOxAr1ig+DuC65kDxCICPGMcT3SsB3Lo6XmAPgK+r6m22IVEFUco9xPFzrXlG5xIAf28dy0JE6e9vFpHTADwH4HJV/bV1QJuinOOp6l0ichOAQ1hNwbwHwJdto+plco4nqu6mfBIREREREYVmsuogERERERHRknGgRURERERElBkHWkRERERERJlxoEVERERERJQZB1pERERERESZcaBFRERERESUGQdaREREREREmXGgRURERERElBkHWkRERERERJlxoEVERERERJTZnjEfFhGds7ELLrjguH87ePDg5O9TVen697FxdsW1ySLOlLjW5sS3liufwPbYS+QTyFf2OfIJ5M3ppqEcj40/R5w16uvcOFNj3Iyv/XMpsZcs966Y+myLNWdb2pS7D6hV9l3GxFq7LY213pcSdXRq3EP5rXFcKnn8BMr393369mtOTmu1I6DOudOQ1Hhr9PfA/D6/5jleakxdcrclq1jXRDU95rmVpm9bIlvj7Pu+WZVmzL433zvq863tTIqzVnyt7WVphKlx585n853JsQ7FOTeXrW0U64Bzxr/0ttQzBfAvAAAN+ElEQVT87KRtdmxr62dqHHhT9mdbrCVPDrfFV7OOzi37MbHWbktjrfclRx2t0aZytqXc5yAb3z27LeUu+779qnVOMiamnu2NjrNE+5naj9bu71vb7fuOqud4KTH1fH+249LU+pAab8pAa9QdrTlKHzzGmBpL++dynYjntI7PY2xdosUbiapWyevcthSx7GvllvLwdOwZUjLOEvU1Sl7XvMe7K/FF7D+tYvZYJzzGVELOMi8+0Mp9VXOuKJVkTpyWA8Il5DfaQYDqsz5ZyHE3y8qWuxejPr9NlP4oh5rlnTOv1heEvLYT2m0e+66556W7ej7KxTCIiIiIiIgy26mBlpfRbU0R9tlTjLy6SWTDUz9gYWzfIyLH/KLxlpw3z/u26229j6r+9deQbW2+RH5zfKenck/pO3PFW3TqoLdpgzmVvg0qIlnnRK+/0yvrqVhLw1yOs5mvJfddNYxddKD2AXhseXtiXfdKP1yeW6SyzcW6jliZsxCXZT2ZOhW87zxxV8s/1WZ+cp5vd6m2GIYHpZOZW7R4oyi58tQuSckj629+nnM65VmY2s/PjDlh8SZaHxUtXis1V0Sd891j2uq2fap9cXXbtobiLxlnhIt6OftGrwt1tVZjzR5LsYGW14PWZjKnXmH1UPmj2VaReVcrnff21f6711gjYi7H2ax/Xu6oTY0hUh/pPU4v8dV6NUYJQ7FE7/ut85z4GpFJP7fLLPJj9oyWdWXYNjfTOr61nHFE7vRKivaMw7arikMnk6wD3dp58XqRxXvZ5byblbNNbpuL77W8+1i345Q7A95ytgQe8jryXUjZvmsu67z18d6nA9PfmVX7GbJcStSVIne0PDWwJRjx4rTCkWw39ypW6Su2HnJUSmrHluuZvdS7BX0/W9u2uulxyfQo9TVKnJsiL/u9Gbv3eC14nybusc/Zxls8bdHvoqXyvI/t+lFyOt4Qb/nZqWe0lm7sHGraDdFehFrSnANx6X2N9m6itbFxezgILvFiYKRphTV4qGdzRC5Lb+0rYtuIOKW0LwYvA2CrHHGgRdV4aWibcURYlREYf/Xdw9Vcj3PIp3T61jFHNybf1ne0I5e1h77MQwxLECWP3vrSvv49Sj7JXrsO5Rik79R7tIiIiIiIiGqoPtDi1YTlm1vGHu58RcI2VU6tB9Cj1vlIcXu/mxWpHaeuiOZ50Q4Ppq4sF5WHMllSPmvYtphQ+3PULfvUwV2pxBHn/FJcEduVx2mDY0SKlebzUN65lp+vcXxKnYLLKVvzMYfL1C7PiMf4pcpdFnxGqwcr/TxWq81M4f3glXvZbPLHy8PC1lg/u3k9IRtTbz0PFiwXkRqbQ4/5S1Ur/oj96ba8RNsfekHVqYOROwiaxluZR+usIq4iaT1lKAer5ee3/b/XMs9hyfuWU+pUnlptsB2Pp7hSeOmrcr6fqrax/ZKH+D3EMJbn/t9TXH0zaSxj5B2tHeGpIXjjdTAz5WAQ8QBCL2A7teP5jksfr1fuU95ZZ51nb3eRot/Zih5/FEOrKnq8QEgVB1pLLAx2FmmmTCP0klurl4JOeQ+Jh3eXeB20tnk8MY0qtUw95HzMM0Ve6moKr9O0rQeBObftcbDlEQdblKJmuXuoYy6Wd1/fvvfUwXgonKVJnVriWY16OuYB8/WfPbWdJWA+6/LWJ3g8JvXxHOdQuUYbhNWeirmN5zL30J49xEArXutqihz1KOtAa9tUgfaf+w5kng4a0RpqpAUQLGLKfYWzRD0de9fPS1uJgvnabVP6nVp1ZurgzsuJbR+r2Epst2b/kfq8m9c+zXOdBHgsoG4l6oWLO1pERERERERLUnWgNebqi5erDduuKnmJM9LdLCuer3CWvjLJ57P8tNW5Ik8r6lK7bnicwhypvKbwlOshY1ZPrMVLHFNYr0Bpvf2aItcTz3LUkawDrdwF7akRRBhsUT5dSxaX6sisDzY5eB9kkR3PdSPSYh6RRchfV12wrp9LwBzm5a0tbXtkqHa8ueubu2e0SvBWqTxijvIa27CYf/9YRtTH4+IDS6qvnG1BtAwR2+yYWXSl+l3TgZbHA1w0zE0dnh86HhKxY6T5vJR7pDbjaboYECt3faIPsjyUgYcYiMaKUm+Hzu1y9VPZ36M15Z0eKT9j/TLJbftj8U4ID+9NmspzI1yXZc3lfD3nY672vnmuk0sS4R01XuMbao8e8+p1MBOpT0t9L6GX9yiuWZdxCsupw5HqYArPbd3r+/yA7vFDzTiLvbA40snjUIfqbR9S4rFudEOsDyBjXlpaU18HsLRnAzdj9lxXiWrp65esLzBO4ekCoNf+foj1MXLI2MFntHel0XSe6+3alBtAORQbaKXw0BCGrlyO4WWhBK8HZA9lXYrHhy9zfk9JEU8kKY3Xq69zWc5e2Nyut37V48InkS76RrR5YdDDCfcSy3sJ/WmtWL21+aIDrRK36XIe5HLFxEHWME8VHsjXCL3mG/AdWxdvU7I8xTKVt5xa89YPbUrplzzug6c7WEMxzM2d9eqtbR5y2sVD/fQQw5JNbUsWr/HwEqf7VQeJiIiIiIiiqTbQWup0KC/xeIljbb2Si5crCrm34S3fS+Dlhc1LKltvV3e9LmbkRbS6F+Fu1tqcWJayH1542QcvcUSTmjfrl3+PPe6XirPqM1pzphKWSMDcBS9KV57UW5/eOos5JzW1H5QGYi4wsoT52rWllHeU/HldrMejnDmqdREoSh86xEscbXNWRa5hW9lb5zRHv+PlvGkJLOvDUF2wrqdtHuoDpw7SzvP2/hyinCwOMtYHNqIceFzIi/mkXTT2jtbTAH6eY8MZGtxrBv5vVJyFG/+sOCt2TEvIJzCjjhaIO1tON2WOdXaclepplnxWiLVYuW+auS870ZYqn9ztVB2NEuemyvUTWFhOvff3Edv8Np7Omds8t6W1gvVhW6yr7fPKIxERERERUV6cOkhERERERJQZB1pERERERESZVR1oicipInKTiDwkIodF5C01t59CRE4SkR+KyH0i8qCIfNY6pj4icoWIPNDEeaV1PH2ixAnEqKNrInKCiNwjIt+0jqWLiJwpIt9t8vigiFxhHVMfEflEE+MDInK9iJxkHVMXEblGRI6KyAPWsWzjvX4C4fr7R0XkfhG5V0R+ZB1Pn0BxRurr3eeUbSm/KOdOInKpiPxERB4WkU9ZxzPEItaqz2iJyLUA/k1VrxKRFwPYq6q/qRZAAlk9NbdPVZ8RkRMB/ADAFap6p3FoxxCRcwHcAOBCAM8CuA3AP6jqT00D2xAlzrUIdXRNRP4RwJsAnKKq77GOZ5OInA7gdFU9JCIvBXAQwN+p6o+NQzuGiJyBVTt/g6r+SURuBPAtVf2qbWTHE5EDAJ4B8M+qeq51PEO8108gTn8PrE4OAbxJVZ+2jmVIoDgj9fWPwnlO2ZbyinLuJCInAPh3AJcAOALgbgAf9HacB+xirfnC4lMAHABwNQCo6rMeOzVdeab564nNL48rhrwewJ2q+kdVfR7A9wG83zimLlHiDFNHAUBE9gN4N4CrrGPpo6pPquqh5s+/B3AYwBm2UfXaA+BkEdkDYC+AJ4zj6aSq/wrgV9ZxbBOhfgKh+nvKKFJfHwXbUnZRzp0uBPCwqj6iqs9iNTh8n3FMfUxirTl18LUAngLwlWY6yVUisq/i9pM1U17uBXAUwB2qepd1TB0eAHBARE4Tkb0A3gXgTOOYukSJEwhURwF8EcAnAfzFOpAUInIWgPMAuGtLqvoLAJ8H8BiAJwH8VlVvt40qvDD1M0h/D6xOWm8XkYMi8lHrYAZEiDNSXw/EyCnbUl5Rzp3OAPB46+9H4PeCqkmsNQdaewCcD+BLqnoegD8AcDmXU1X/rKpvBLAfwIXNLVxXVPUwgM8BuAOrW8r3AXjeNKgOUeJshKijIvIeAEdV9aB1LClE5CUAbgZwpar+zjqeTSLyMqyuap0N4FUA9onIh2yjiita/YzQ3zcuUtXzAbwTwOXNNFKPIsQZoq9viZBTtqWMAp07db2kyuudTJNYaw60jgA40rrCcRNWHZ1bzVSC7wG41DiUTqp6taqer6oHsJpO5Gru7lqUOBGnjl4E4L3NPPMbALxNRL5mG1K3Zq7+zQCuU9VbrOPp8XYAP1PVp1T1OQC3AHircUyRhamfbQH6+yea348CuBWraTDuBIkzSl8PIExO/4ptKY8g505HcOydtv1wOvUeRrFWG2ip6i8BPC4i5zT/dDEAjw/LvVxETm3+fDJWJ2EP2UbVTURe0fz+agAfAHC9bUTdosQZpY6q6qdVdb+qngXgMgDfUVV3d2Cah6OvBnBYVb9gHc+AxwC8WUT2NjFfjNXzZDRBlPoJxOnvRWRfs6AMmilu78BqapErUeKM0tcDcXLKtpRfkHOnuwG8TkTObhaVuQzAN4xj6mMS657SG9jwcQDXNTv4CICPVN5+itMBXNusTvIiADeqqtfliW8WkdMAPAfgclX9tXVAPaLECcSoo1FcBODDAO5v5u0DwGdU9VuGMR1HVe8SkZsAHMJqasY9AL5sG1U3EbkewN8C+BsROQLgn1T1atuoQovS378SwK2r6wDYA+DrqnqbbUidosQJxOnro+SUbSk/9+dOqvq8iHwMwLcBnADgGlV90DisTlaxVl3enYiIiIiIaBdUfWExERERERHRLuBAi4iIiIiIKDMOtIiIiIiIiDLjQIuIiIiIiCgzDrSIiIiIiIgy40CLiIiIiIgoMw60iIiIiIiIMuNAi4iIiIiIKLP/D+OH0ydHsExtAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x216 with 60 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Size of dataset: {len} images\".format(len=len(images)))\n",
    "\n",
    "PLOT_SIZE = 60\n",
    "ROW_WIDTH = 20\n",
    "plt.figure(figsize=(15, PLOT_SIZE / ROW_WIDTH))\n",
    "for i in range(PLOT_SIZE):\n",
    "    plt.subplot(PLOT_SIZE / ROW_WIDTH, ROW_WIDTH, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.xlabel(labels[i])\n",
    "    plt.imshow(images[i], cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot Eencoding\n",
    "\n",
    "To compare the labels with the predictions of the neural network we need to 'one-hot' encode the labels:\n",
    "\n",
    "The index of the `1` is the correct label of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One hot encoded labels:\n",
      "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "onehot = np.zeros((len(labels), __LABELS))\n",
    "onehot[range(len(labels)), labels] = 1\n",
    "\n",
    "print('One hot encoded labels:')\n",
    "print(onehot[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine the data and the labels to be able to shuffle them without forgetting which labels belong to which images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([np.array([images[i], onehot[i]]) for i in range(len(images))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (1920, 32, 32, 1)\n",
      "Labels shape: (1920, 10)\n"
     ]
    }
   ],
   "source": [
    "shuffled_data = np.random.permutation(data)\n",
    "\n",
    "X = np.array([x[0] for x in shuffled_data])\n",
    "X = np.reshape(X, (len(X), 32, 32, 1))\n",
    "Y = np.array([x[1] for x in shuffled_data])\n",
    "\n",
    "print('Images shape: {}'.format(X.shape))\n",
    "print('Labels shape: {}'.format(Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into train, test, and validation sets.\n",
    "\n",
    "We use the train set to train; the test set to cross-validate during training; and the validation set to validate the model after the model is done training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split X (train, test, validation): (1387, 32, 32, 1) (245, 32, 32, 1) (288, 32, 32, 1)\n",
      "Split Y (train, test, validation): (1387, 10) (245, 10) (288, 10)\n"
     ]
    }
   ],
   "source": [
    "split = int(len(X) * .85)\n",
    "X_train = X[:split]\n",
    "X_valid = X[split:]\n",
    "Y_train = Y[:split]\n",
    "Y_valid = Y[split:]\n",
    "\n",
    "split = int(len(X_train) * .85)\n",
    "X_test = X_train[split:]\n",
    "X_train = X_train[:split]\n",
    "Y_test = Y_train[split:]\n",
    "Y_train = Y_train[:split]\n",
    "\n",
    "print('Split X (train, test, validation):', X_train.shape, X_test.shape, X_valid.shape)\n",
    "print('Split Y (train, test, validation):', Y_train.shape, Y_test.shape, Y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that returns a random batch of a certain size. This batch is used in training to let the model more easily adapt to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, y, size):\n",
    "    batch = np.array([(x[i], y[i]) for i in range(len(x))])\n",
    "    random_batch = np.random.permutation(batch)[:size]\n",
    "    return np.array([x[0] for x in random_batch]), np.array([x[1] for x in random_batch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "We designed several models:\n",
    "\n",
    "rank | name | layers | score\n",
    "--- | --- | --- | ---\n",
    "5 | model_8 | (64, Tanh, Drop=.2) (128, Tanh, Drop=.3) (256, Tanh, Drop=.4) (512, Tanh, Drop=.5) (64, Tanh) | 0.93714285\n",
    "4 | model_7 | (64, ReLU) (128, ReLU) (256, ReLU) (512, ReLU, Drop=.3) (64, ReLU) | 0.94057140\n",
    "3 | model_6 | (200, Tanh) (300, Tanh) (600, Tanh) | 0.97828573\n",
    "1 | model_5_2 | (600, Tanh, Drop=.3) (300, Tanh, Drop=.3) (200, Tanh, Drop=.3) | 0.98742855\n",
    "2 | model_5 | (600, Tanh) (300, Tanh) (200, Tanh) | 0.97942860\n",
    "6 | model_4 | (128, Tanh) (64, Tanh) (32, Tanh) | 0.87771430\n",
    "7 | model_3 | (12, ReLU) (24, ReLU) (48, ReLU, Drop=.1) (96, ReLU) | 0.73028570\n",
    "8 | model_2 | (128, ReLU) (64, ReLU) (32, ReLU) | 0.82400000\n",
    "9 | model_1 | (128, Sigmoid) | 0.66628570\n",
    "\n",
    "Dropout has a positive effect on the score as can be seen in the table. We also found that the tanh activation function performed well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model 1: BLUE\n",
    "- Model 2: RED\n",
    "- Model 3: LIGHT BLUE\n",
    "- Model 4: PINK\n",
    "- Model 5: GREEN\n",
    "- Model 6: GRAY\n",
    "- Model 7: ORANGE\n",
    "- Model 8: ORANGE\n",
    "\n",
    "### Batch Accuracy\n",
    "\n",
    "\n",
    "### Batch Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_1(x, output_shape):\n",
    "    \"\"\"\n",
    "    One Conv2d layer with a kernel size of 3 and \n",
    "    \"\"\"\n",
    "    l_1 = tf.layers.conv2d(x, filters=32, kernel_size=3, strides=1, padding=\"same\", \n",
    "                           activation=tf.nn.relu)\n",
    "    return tf.layers.dense(l_1, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2(x, output_shape):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    conv1 = tf.layers.conv2d(x, filters=32, kernel_size=5, \n",
    "                             padding=\"same\", activation=tf.nn.relu)\n",
    "    pool1 = tf.layers.max_pooling2d(conv1, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    flatten = tf.contrib.layers.flatten(pool1)\n",
    "    return tf.layers.dense(flatten, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_3(x, output_shape):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #32 convolution filters used each of size 3x3\n",
    "    conv1 = tf.layers.conv2d(x, filters=32, kernel_size = 3,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "    \n",
    "    #64 convolution filters used each of size 3x3\n",
    "    conv2 = tf.layers.conv2d(conv1, filters=64, kernel_size = 3,\n",
    "                         strides = 1, padding=\"valid\",\n",
    "                         activation = tf.nn.relu)\n",
    "    \n",
    "    #choose the best features via pooling\n",
    "    pool1 = tf.layers.max_pooling2d(conv2, pool_size=2, strides=2)\n",
    "    \n",
    "    #randomly turn neurons on and off to improve convergence\n",
    "    dropout1 = tf.layers.dropout(pool1, rate=.25)\n",
    "    \n",
    "    #flatten since too many dimensions, we only want a classification output\n",
    "    flatten = tf.contrib.layers.flatten(dropout1)\n",
    "\n",
    "    #fully connected to get all relevant data\n",
    "    dense = tf.layers.dense(flatten, units=128, activation=tf.nn.relu)\n",
    "    \n",
    "    #one more dropout for convergence' sake :) \n",
    "    dropout2 = tf.layers.dropout(dense, rate=.5)\n",
    "\n",
    "    return tf.layers.dense(dropout2, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_4(x, output_shape):\n",
    "    \"\"\"\n",
    "    IMPLEMENT BATCH NORMALIZATION AFTER CONV LAYERS\n",
    "    \"\"\"\n",
    "    conv1 = tf.layers.conv2d(x, filters=32, kernel_size = 3,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(conv1, filters=32, kernel_size = 3,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "        \n",
    "    pool1 = tf.layers.max_pooling2d(conv2, pool_size=2, strides=2)\n",
    "\n",
    "    conv3 = tf.layers.conv2d(pool1, filters=64, kernel_size = 3,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "\n",
    "    conv4 = tf.layers.conv2d(conv3, filters=64, kernel_size = 3,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "\n",
    "    pool2 = tf.layers.max_pooling2d(conv4, pool_size=2, strides=2)\n",
    "\n",
    "    flatten = tf.contrib.layers.flatten(pool2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    dense = tf.layers.dense(flatten, units=512, activation=tf.nn.relu)\n",
    "\n",
    "    dropout = tf.layers.dropout(dense, rate=.2)\n",
    "    \n",
    "    return tf.layers.dense(dropout, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_5(x, output_shape):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    conv1 = tf.layers.conv2d(x, filters=32, kernel_size = 5,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "    conv2 = tf.layers.conv2d(conv1, filters=32, kernel_size = 5,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "    pool1 = tf.layers.max_pooling2d(conv2, pool_size=2, strides=2)\n",
    "    dropout1 = tf.layers.dropout(pool1, rate=.25)\n",
    "    \n",
    "    conv3 = tf.layers.conv2d(x, filters=64, kernel_size = 3,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "    conv4 = tf.layers.conv2d(conv3, filters=64, kernel_size = 3,\n",
    "                         strides = 1, padding=\"same\",\n",
    "                         activation = tf.nn.relu)\n",
    "    pool2 = tf.layers.max_pooling2d(conv4, pool_size=2, strides=2)\n",
    "    dropout2 = tf.layers.dropout(pool2, rate=.25)\n",
    "    \n",
    "    flatten = tf.contrib.layers.flatten(dropout2)\n",
    "    dense = tf.layers.dense(flatten, units=256, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(dense, rate=.5)\n",
    "\n",
    "    return tf.layers.dense(dropout, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the placeholder for our 5-dice input and 7-class output and choose a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, X_train.shape[1], X_train.shape[2], 1], name='x')\n",
    "y = tf.placeholder(tf.float32, shape=[None, Y_train.shape[1]], name='y')\n",
    "\n",
    "model_fn = model_5\n",
    "y_pred = model_fn(x, Y_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose an optimizer, a loss functon and metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=y_pred)\n",
    "loss_fn = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# Optimizer minimizes the loss\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=.0005).minimize(loss_fn)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001).minimize(loss_fn)\n",
    "\n",
    "# Accuracy metric\n",
    "#   checks if the indices of the highest values in the real \n",
    "#   and predicted arrays are equal\n",
    "prediction = tf.equal(tf.argmax(y, axis=1), tf.argmax(y_pred, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model using a certain batch size and for a number of iterations while posting scalars to TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - i: 1  Accuracy: 0.12244898\n",
      "Validation - i: 51  Accuracy: 0.7673469\n",
      "Validation - i: 101  Accuracy: 0.89387757\n",
      "Validation - i: 151  Accuracy: 0.93877554\n",
      "Validation - i: 201  Accuracy: 0.955102\n",
      "Validation - i: 251  Accuracy: 0.955102\n",
      "Validation - i: 301  Accuracy: 0.93061227\n",
      "Validation - i: 351  Accuracy: 0.955102\n",
      "Validation - i: 401  Accuracy: 0.9632653\n",
      "Validation - i: 451  Accuracy: 0.9755102\n",
      "Validation - i: 501  Accuracy: 0.9510204\n",
      "Validation - i: 551  Accuracy: 0.955102\n",
      "Validation - i: 601  Accuracy: 0.9632653\n",
      "Validation - i: 651  Accuracy: 0.9632653\n",
      "Validation - i: 701  Accuracy: 0.94285715\n",
      "Validation - i: 751  Accuracy: 0.93061227\n",
      "Validation - i: 801  Accuracy: 0.9632653\n",
      "Validation - i: 851  Accuracy: 0.9755102\n",
      "Validation - i: 901  Accuracy: 0.96734697\n",
      "Validation - i: 951  Accuracy: 0.9714286\n",
      "Validation - i: 1001  Accuracy: 0.9591837\n",
      "Validation - i: 1051  Accuracy: 0.955102\n",
      "Validation - i: 1101  Accuracy: 0.9346939\n",
      "Validation - i: 1151  Accuracy: 0.9510204\n",
      "Validation - i: 1201  Accuracy: 0.9510204\n",
      "Validation - i: 1251  Accuracy: 0.9510204\n",
      "Validation - i: 1301  Accuracy: 0.94693875\n",
      "Validation - i: 1351  Accuracy: 0.9591837\n",
      "Validation - i: 1401  Accuracy: 0.98367345\n",
      "Validation - i: 1451  Accuracy: 0.9755102\n",
      "Accuracy: [0.9722222]\n"
     ]
    }
   ],
   "source": [
    "iters = 1500\n",
    "train_batch_size = 20\n",
    "\n",
    "session = tf.Session()\n",
    "with session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    sum_loss_train = tf.summary.scalar('loss_train', loss_fn)\n",
    "    sum_loss_test = tf.summary.scalar('loss_test', loss_fn)\n",
    "    sum_acc_train = tf.summary.scalar('acc_train', accuracy)\n",
    "    sum_acc_test = tf.summary.scalar('acc_test', accuracy)\n",
    "    tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(os.path.join(__TENSOR_LOG_DIR, model_fn.__name__), session.graph)\n",
    "    \n",
    "    for i in range(iters):\n",
    "        x_batch, y_batch = get_batch(X_train, Y_train, train_batch_size)\n",
    "                \n",
    "        loss_val, _, acc_val, sum_1, sum_2 = session.run([loss_fn, optimizer, accuracy, \n",
    "                                                          sum_loss_train, sum_acc_train], \n",
    "                                                         feed_dict={x: x_batch, y: y_batch})\n",
    "\n",
    "        writer.add_summary(sum_1, global_step=i)\n",
    "        writer.add_summary(sum_2, global_step=i)\n",
    "    #     print('Training - i:', i+1, 'Loss:', loss_val, 'Accuracy:', acc_val)\n",
    "\n",
    "        # Validate every 50 iterations\n",
    "        if i % 50 == 0:\n",
    "            acc_val, sum_1, sum_2 = session.run([accuracy, sum_loss_test, sum_acc_test], \n",
    "                                                feed_dict={x: X_test, y: Y_test})\n",
    "\n",
    "            writer.add_summary(sum_1, global_step=i)\n",
    "            writer.add_summary(sum_2, global_step=i)\n",
    "            print('Validation - i:', i+1, ' Accuracy:', acc_val)\n",
    "    \n",
    "\n",
    "    # Validate the model with unseen data\n",
    "    acc_val = session.run([accuracy], feed_dict={x: X_valid, y: Y_valid})\n",
    "\n",
    "    # Print test metrics\n",
    "    print('Accuracy:', acc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We validate the model with the data it has not seen yet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with session:\n",
    "#     # Validate the model with unseen data\n",
    "#     acc_val = session.run([accuracy], feed_dict={x: X_valid, y: Y_valid})\n",
    "\n",
    "#     # Print test metrics\n",
    "#     print('Accuracy:', acc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting & Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '{}.ckpt'.format(os.path.join(__MODEL_PATH, model_fn.__name__, model_fn.__name__))\n",
    "\n",
    "model_to_load = model_5_2\n",
    "\n",
    "load_path = '{}.ckpt'.format(os.path.join(__MODEL_PATH, model_to_load.__name__, model_to_load.__name__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the model that worked best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with session:\n",
    "#     tf.train.Saver().save(session, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the model that worked best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as saved_session:\n",
    "    tf.train.Saver().restore(saved_session, load_path)\n",
    "\n",
    "    # Validate the model with unseen data\n",
    "    acc_val = saved_session.run([accuracy], feed_dict={x: X_valid, y: Y_valid})\n",
    "\n",
    "    # Print test metrics\n",
    "    print('Accuracy:', acc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
