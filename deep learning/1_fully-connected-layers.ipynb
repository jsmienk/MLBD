{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yahtzee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "__MODEL_PATH = 'models'\n",
    "__TENSOR_LOG_DIR = 'logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Let's start with looking at the provided dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['nothing' 'small-straight' 'three-of-a-kind' 'large-straight'\n",
      " 'full-house' 'four-of-a-kind' 'yathzee']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dice1</th>\n",
       "      <th>dice2</th>\n",
       "      <th>dice3</th>\n",
       "      <th>dice4</th>\n",
       "      <th>dice5</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>small-straight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>three-of-a-kind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>nothing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dice1  dice2  dice3  dice4  dice5            label\n",
       "0      3      6      6      2      5          nothing\n",
       "1      3      6      1      3      4          nothing\n",
       "2      2      2      5      5      3          nothing\n",
       "3      1      3      6      6      1          nothing\n",
       "4      1      4      6      3      5   small-straight\n",
       "5      4      1      4      3      1          nothing\n",
       "6      4      4      4      6      2  three-of-a-kind\n",
       "7      3      2      5      6      3          nothing\n",
       "8      3      4      3      6      2          nothing\n",
       "9      3      3      1      5      4          nothing"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('yahtzee-dataset.csv')\n",
    "print('Labels:', df.label.unique())\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to classify these categorical labels, we have to 'one-hot encode' them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dice1</th>\n",
       "      <th>dice2</th>\n",
       "      <th>dice3</th>\n",
       "      <th>dice4</th>\n",
       "      <th>dice5</th>\n",
       "      <th>label_four-of-a-kind</th>\n",
       "      <th>label_full-house</th>\n",
       "      <th>label_large-straight</th>\n",
       "      <th>label_nothing</th>\n",
       "      <th>label_small-straight</th>\n",
       "      <th>label_three-of-a-kind</th>\n",
       "      <th>label_yathzee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dice1  dice2  dice3  dice4  dice5  label_four-of-a-kind  label_full-house  \\\n",
       "0      3      6      6      2      5                     0                 0   \n",
       "1      3      6      1      3      4                     0                 0   \n",
       "2      2      2      5      5      3                     0                 0   \n",
       "3      1      3      6      6      1                     0                 0   \n",
       "4      1      4      6      3      5                     0                 0   \n",
       "5      4      1      4      3      1                     0                 0   \n",
       "6      4      4      4      6      2                     0                 0   \n",
       "7      3      2      5      6      3                     0                 0   \n",
       "8      3      4      3      6      2                     0                 0   \n",
       "9      3      3      1      5      4                     0                 0   \n",
       "\n",
       "   label_large-straight  label_nothing  label_small-straight  \\\n",
       "0                     0              1                     0   \n",
       "1                     0              1                     0   \n",
       "2                     0              1                     0   \n",
       "3                     0              1                     0   \n",
       "4                     0              0                     1   \n",
       "5                     0              1                     0   \n",
       "6                     0              0                     0   \n",
       "7                     0              1                     0   \n",
       "8                     0              1                     0   \n",
       "9                     0              1                     0   \n",
       "\n",
       "   label_three-of-a-kind  label_yathzee  \n",
       "0                      0              0  \n",
       "1                      0              0  \n",
       "2                      0              0  \n",
       "3                      0              0  \n",
       "4                      0              0  \n",
       "5                      0              0  \n",
       "6                      1              0  \n",
       "7                      0              0  \n",
       "8                      0              0  \n",
       "9                      0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_df = pd.get_dummies(df, prefix=['label'])\n",
    "one_hot_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can train any model, we have to split the data and the labels into X and Y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dice1</th>\n",
       "      <th>dice2</th>\n",
       "      <th>dice3</th>\n",
       "      <th>dice4</th>\n",
       "      <th>dice5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5754</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5200</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4108</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4463</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      dice1  dice2  dice3  dice4  dice5\n",
       "5754      2      6      1      4      2\n",
       "5200      2      1      3      6      4\n",
       "4108      5      5      1      2      6\n",
       "459       3      6      1      1      3\n",
       "4463      2      4      2      6      3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled = one_hot_df.sample(frac=1.)\n",
    "X = shuffled.iloc[:,:5].copy()\n",
    "Y = shuffled.iloc[:,5:].copy()\n",
    "\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also split the dataset into a 9:1 split for training and validating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split X: (5248, 5) (584, 5)\n",
      "Split Y: (5248, 7) (584, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_four-of-a-kind</th>\n",
       "      <th>label_full-house</th>\n",
       "      <th>label_large-straight</th>\n",
       "      <th>label_nothing</th>\n",
       "      <th>label_small-straight</th>\n",
       "      <th>label_three-of-a-kind</th>\n",
       "      <th>label_yathzee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5754</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5200</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4108</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4463</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label_four-of-a-kind  label_full-house  label_large-straight  \\\n",
       "5754                     0                 0                     0   \n",
       "5200                     0                 0                     0   \n",
       "4108                     0                 0                     0   \n",
       "459                      0                 0                     0   \n",
       "4463                     0                 0                     0   \n",
       "\n",
       "      label_nothing  label_small-straight  label_three-of-a-kind  \\\n",
       "5754              1                     0                      0   \n",
       "5200              0                     1                      0   \n",
       "4108              1                     0                      0   \n",
       "459               1                     0                      0   \n",
       "4463              1                     0                      0   \n",
       "\n",
       "      label_yathzee  \n",
       "5754              0  \n",
       "5200              0  \n",
       "4108              0  \n",
       "459               0  \n",
       "4463              0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = int(len(X.index) * .9)\n",
    "X_train = X.iloc[:split]\n",
    "X_valid = X.iloc[split:]\n",
    "Y_train = Y.iloc[:split]\n",
    "Y_valid = Y.iloc[split:]\n",
    "\n",
    "print('Split X:', X_train.shape, X_valid.shape)\n",
    "print('Split Y:', Y_train.shape, Y_valid.shape)\n",
    "\n",
    "Y_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, labels, batch_size):\n",
    "    x_batch = data.sample(frac=batch_size / len(data.index))\n",
    "    return x_batch, labels.loc[x_batch.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "We designed several models:\n",
    "\n",
    "rank | name | layers | score\n",
    "--- | --- | --- | ---\n",
    "1 | model_6 | Dense(32, tanh), Dense(256, tanh), Dropout(0.1), Dense(256, tanh), Dropout(0.1), Dense(128, tanh), Dense(512, tanh), Dropout(0.1), Dense(512, tanh), Dropout(0.5), Dense(128, tanh)| 0.691780821918\n",
    "2 | model_3 | Dense(32, ReLu), Dense(64, ReLu), Dropout(0.1), Dense(128, ReLu), Dropout(0.1), Dense(512, ReLu)| 0.683219178082\n",
    "3 | model_4 | Dense(1024, tanh), Dense(512, tanh), Dense(128, tanh) | 0.679794520548\n",
    "4 | model_2 | Dense(128, ReLu), Dense(256, ReLu), Dense(32, ReLu) | 0.676369863014\n",
    "5 | model_1 | Dense(128, Sigmoid) | 0.674657534247\n",
    "6 | model_5 | Dense(16, Sigmoid), Dense(32, Sigmoid | 0.674657534247\n",
    "\n",
    "Dropout has a positive effect on the score as can be seen in the table. We also found that the tanh activation function performed well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_1(x, output_shape):\n",
    "    \"\"\"\n",
    "    Single hidden layer with 128 neurons and Sigmoid activation function.\n",
    "    \"\"\"\n",
    "    l_1 = tf.layers.dense(x, units=128, activation=tf.nn.sigmoid)\n",
    "    return tf.layers.dense(l_1, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2(x, output_shape):\n",
    "    \"\"\"\n",
    "    Three hidden layers with different amounts of neurons and relu activation functions.\n",
    "    \"\"\"\n",
    "    l_1 = tf.layers.dense(x, units=128, activation=tf.nn.relu)\n",
    "    l_2 = tf.layers.dense(l_1, units=64, activation=tf.nn.relu)\n",
    "    l_3 = tf.layers.dense(l_2, units=32, activation=tf.nn.relu)\n",
    "    return tf.layers.dense(l_3, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_3(x, output_shape):\n",
    "    \"\"\"\n",
    "    Six hidden layers with different amounts of neurons and \n",
    "    relu activation functions and 2 dropout layers.\n",
    "    \"\"\"\n",
    "    l_1 = tf.layers.dense(x, units=12, activation=tf.nn.relu)\n",
    "    l_2 = tf.layers.dense(l_1, units=24, activation=tf.nn.relu)\n",
    "    l_3 = tf.layers.dense(l_2, units=48, activation=tf.nn.relu)\n",
    "    d_3 = tf.layers.dropout(l_3, rate=.1)\n",
    "    l_4 = tf.layers.dense(d_3, units=96, activation=tf.nn.relu)\n",
    "    return tf.layers.dense(l_4, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_4(x, output_shape):\n",
    "    \"\"\"\n",
    "    Three hidden layers with different amounts of neurons and relu activation functions.\n",
    "    \"\"\"\n",
    "    l_1 = tf.layers.dense(x, units=128, activation=tf.nn.tanh)\n",
    "    l_2 = tf.layers.dense(l_1, units=64, activation=tf.nn.tanh)\n",
    "    l_3 = tf.layers.dense(l_2, units=32, activation=tf.nn.tanh)\n",
    "    return tf.layers.dense(l_3, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_5(x, output_shape):\n",
    "    \"\"\"\n",
    "    High number of neurons in layers, decreasing per layer\n",
    "    \"\"\"\n",
    "    l_1 = tf.layers.dense(x, units=1000, activation=tf.nn.tanh)\n",
    "    l_2 = tf.layers.dense(l_1, units=500, activation=tf.nn.tanh)\n",
    "    l_3 = tf.layers.dense(l_2, units=250, activation=tf.nn.tanh)\n",
    "    return tf.layers.dense(l_3, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_6(x, output_shape):\n",
    "    \"\"\"\n",
    "    High number of neurons in layers, increasing per layer\n",
    "    \"\"\"\n",
    "    l_1 = tf.layers.dense(x, units=250, activation=tf.nn.tanh)\n",
    "    l_2 = tf.layers.dense(l_1, units=500, activation=tf.nn.tanh)\n",
    "    l_3 = tf.layers.dense(l_2, units=1000, activation=tf.nn.tanh)\n",
    "    return tf.layers.dense(l_3, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_7(x, output_shape):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    l_1 = tf.layers.dense(x, units=64, activation=tf.nn.relu)\n",
    "    l_2 = tf.layers.dense(l_1, units=128, activation=tf.nn.relu)\n",
    "    l_3 = tf.layers.dense(l_2, units=256, activation=tf.nn.relu)\n",
    "    l_4 = tf.layers.dense(l_3, units=512, activation=tf.nn.relu)\n",
    "    d_4 = tf.layers.dropout(l_4, rate=.3)\n",
    "    l_5 = tf.layers.dense(d_4, units=64, activation=tf.nn.relu)\n",
    "    return tf.layers.dense(l_5, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_8(x, output_shape):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    l_1 = tf.layers.dense(x, units=64, activation=tf.nn.relu)\n",
    "    d_1 = tf.layers.dropout(l_1, rate=.0)\n",
    "    l_2 = tf.layers.dense(d_1, units=128, activation=tf.nn.relu)\n",
    "    d_2 = tf.layers.dropout(l_2, rate=.1)\n",
    "    l_3 = tf.layers.dense(d_2, units=256, activation=tf.nn.relu)\n",
    "    d_3 = tf.layers.dropout(l_3, rate=.2)\n",
    "    l_4 = tf.layers.dense(d_3, units=512, activation=tf.nn.relu)\n",
    "    d_4 = tf.layers.dropout(l_4, rate=.3)\n",
    "    l_5 = tf.layers.dense(d_4, units=64, activation=tf.nn.relu)\n",
    "    return tf.layers.dense(l_5, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the placeholder for our 5-dice input and 7-class output and choose a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, X.shape[1]], name='x')\n",
    "y = tf.placeholder(tf.float32, shape=[None, Y.shape[1]], name='y')\n",
    "\n",
    "model_fn = model_8\n",
    "y_pred = model_fn(x, Y.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose an optimizer, a loss functon and metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=y_pred)\n",
    "loss_fn = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# Optimizer minimizes the loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=.001).minimize(loss_fn)\n",
    "\n",
    "# Accuracy metric\n",
    "#   checks if the indices of the highest values in the real \n",
    "#   and predicted arrays are equal\n",
    "prediction = tf.equal(tf.argmax(y, axis=1), tf.argmax(y_pred, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model using a certain batch size and for a number of iterations while posting scalars to TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - i: 1 Loss: 2.192661 Accuracy: 0.02\n",
      "Validation - i: 1  Accuracy: [0.69]\n",
      "Training - i: 2 Loss: 1.7579706 Accuracy: 0.63\n",
      "Training - i: 3 Loss: 1.4803977 Accuracy: 0.69\n",
      "Training - i: 4 Loss: 1.3888452 Accuracy: 0.61\n",
      "Training - i: 5 Loss: 1.2159992 Accuracy: 0.675\n",
      "Training - i: 6 Loss: 1.1466622 Accuracy: 0.69\n",
      "Training - i: 7 Loss: 1.41533 Accuracy: 0.63\n",
      "Training - i: 8 Loss: 1.1347055 Accuracy: 0.69\n",
      "Training - i: 9 Loss: 1.2810593 Accuracy: 0.615\n",
      "Training - i: 10 Loss: 1.135078 Accuracy: 0.64\n",
      "Training - i: 11 Loss: 1.1256759 Accuracy: 0.695\n",
      "Training - i: 12 Loss: 1.0870862 Accuracy: 0.645\n",
      "Training - i: 13 Loss: 1.1492213 Accuracy: 0.655\n",
      "Training - i: 14 Loss: 1.0986248 Accuracy: 0.665\n",
      "Training - i: 15 Loss: 1.009088 Accuracy: 0.705\n",
      "Training - i: 16 Loss: 0.9887791 Accuracy: 0.73\n",
      "Training - i: 17 Loss: 1.2981027 Accuracy: 0.575\n",
      "Training - i: 18 Loss: 1.0644974 Accuracy: 0.7\n",
      "Training - i: 19 Loss: 1.0845075 Accuracy: 0.68\n",
      "Training - i: 20 Loss: 1.0568337 Accuracy: 0.685\n",
      "Training - i: 21 Loss: 1.0943114 Accuracy: 0.665\n",
      "Training - i: 22 Loss: 1.1199456 Accuracy: 0.67\n",
      "Training - i: 23 Loss: 1.071923 Accuracy: 0.66\n",
      "Training - i: 24 Loss: 1.0482328 Accuracy: 0.685\n",
      "Training - i: 25 Loss: 1.1506755 Accuracy: 0.635\n",
      "Training - i: 26 Loss: 0.9638858 Accuracy: 0.69\n",
      "Training - i: 27 Loss: 0.9479471 Accuracy: 0.725\n",
      "Training - i: 28 Loss: 1.0167197 Accuracy: 0.69\n",
      "Training - i: 29 Loss: 1.2554091 Accuracy: 0.61\n",
      "Training - i: 30 Loss: 1.1845958 Accuracy: 0.635\n",
      "Training - i: 31 Loss: 1.0402002 Accuracy: 0.67\n",
      "Training - i: 32 Loss: 0.9894657 Accuracy: 0.69\n",
      "Training - i: 33 Loss: 1.0261992 Accuracy: 0.695\n",
      "Training - i: 34 Loss: 1.1853328 Accuracy: 0.62\n",
      "Training - i: 35 Loss: 1.0050393 Accuracy: 0.695\n",
      "Training - i: 36 Loss: 1.0900044 Accuracy: 0.635\n",
      "Training - i: 37 Loss: 1.1432147 Accuracy: 0.63\n",
      "Training - i: 38 Loss: 1.0428538 Accuracy: 0.69\n",
      "Training - i: 39 Loss: 1.1206664 Accuracy: 0.65\n",
      "Training - i: 40 Loss: 1.1842259 Accuracy: 0.575\n",
      "Training - i: 41 Loss: 1.0101156 Accuracy: 0.71\n",
      "Training - i: 42 Loss: 1.171436 Accuracy: 0.595\n",
      "Training - i: 43 Loss: 1.0755439 Accuracy: 0.655\n",
      "Training - i: 44 Loss: 1.0609403 Accuracy: 0.64\n",
      "Training - i: 45 Loss: 1.1129556 Accuracy: 0.64\n",
      "Training - i: 46 Loss: 0.94686043 Accuracy: 0.725\n",
      "Training - i: 47 Loss: 0.96728754 Accuracy: 0.725\n",
      "Training - i: 48 Loss: 1.0829217 Accuracy: 0.665\n",
      "Training - i: 49 Loss: 1.1121007 Accuracy: 0.635\n",
      "Training - i: 50 Loss: 1.1786342 Accuracy: 0.615\n",
      "Training - i: 51 Loss: 1.0269988 Accuracy: 0.665\n",
      "Validation - i: 51  Accuracy: [0.68]\n",
      "Training - i: 52 Loss: 1.1119201 Accuracy: 0.625\n",
      "Training - i: 53 Loss: 0.9794811 Accuracy: 0.69\n",
      "Training - i: 54 Loss: 1.0598508 Accuracy: 0.635\n",
      "Training - i: 55 Loss: 1.2099341 Accuracy: 0.585\n",
      "Training - i: 56 Loss: 1.0738289 Accuracy: 0.64\n",
      "Training - i: 57 Loss: 1.0505024 Accuracy: 0.67\n",
      "Training - i: 58 Loss: 1.1392902 Accuracy: 0.655\n",
      "Training - i: 59 Loss: 0.8978145 Accuracy: 0.73\n",
      "Training - i: 60 Loss: 0.95724106 Accuracy: 0.705\n",
      "Training - i: 61 Loss: 1.1446714 Accuracy: 0.58\n",
      "Training - i: 62 Loss: 0.9851331 Accuracy: 0.65\n",
      "Training - i: 63 Loss: 1.0590249 Accuracy: 0.675\n",
      "Training - i: 64 Loss: 1.0813771 Accuracy: 0.66\n",
      "Training - i: 65 Loss: 1.003812 Accuracy: 0.68\n",
      "Training - i: 66 Loss: 0.954617 Accuracy: 0.69\n",
      "Training - i: 67 Loss: 1.042209 Accuracy: 0.64\n",
      "Training - i: 68 Loss: 1.0636035 Accuracy: 0.64\n",
      "Training - i: 69 Loss: 1.0483769 Accuracy: 0.655\n",
      "Training - i: 70 Loss: 0.96848816 Accuracy: 0.69\n",
      "Training - i: 71 Loss: 0.9916275 Accuracy: 0.685\n",
      "Training - i: 72 Loss: 0.91458356 Accuracy: 0.71\n",
      "Training - i: 73 Loss: 1.0670292 Accuracy: 0.665\n",
      "Training - i: 74 Loss: 1.0888903 Accuracy: 0.65\n",
      "Training - i: 75 Loss: 1.0635251 Accuracy: 0.64\n",
      "Training - i: 76 Loss: 0.9425899 Accuracy: 0.705\n",
      "Training - i: 77 Loss: 1.0406665 Accuracy: 0.68\n",
      "Training - i: 78 Loss: 1.0655628 Accuracy: 0.66\n",
      "Training - i: 79 Loss: 0.9875297 Accuracy: 0.695\n",
      "Training - i: 80 Loss: 1.0960112 Accuracy: 0.635\n",
      "Training - i: 81 Loss: 0.9360683 Accuracy: 0.695\n",
      "Training - i: 82 Loss: 1.108887 Accuracy: 0.615\n",
      "Training - i: 83 Loss: 1.0424787 Accuracy: 0.67\n",
      "Training - i: 84 Loss: 1.0398128 Accuracy: 0.655\n",
      "Training - i: 85 Loss: 1.0688418 Accuracy: 0.64\n",
      "Training - i: 86 Loss: 0.945468 Accuracy: 0.69\n",
      "Training - i: 87 Loss: 0.99775237 Accuracy: 0.68\n",
      "Training - i: 88 Loss: 0.8385628 Accuracy: 0.75\n",
      "Training - i: 89 Loss: 1.0273398 Accuracy: 0.66\n",
      "Training - i: 90 Loss: 1.1981894 Accuracy: 0.615\n",
      "Training - i: 91 Loss: 1.1793102 Accuracy: 0.61\n",
      "Training - i: 92 Loss: 0.9745949 Accuracy: 0.66\n",
      "Training - i: 93 Loss: 1.0850449 Accuracy: 0.63\n",
      "Training - i: 94 Loss: 1.0197939 Accuracy: 0.66\n",
      "Training - i: 95 Loss: 1.1199226 Accuracy: 0.66\n",
      "Training - i: 96 Loss: 1.1328018 Accuracy: 0.625\n",
      "Training - i: 97 Loss: 0.9572206 Accuracy: 0.675\n",
      "Training - i: 98 Loss: 0.9799744 Accuracy: 0.695\n",
      "Training - i: 99 Loss: 0.9413788 Accuracy: 0.685\n",
      "Training - i: 100 Loss: 1.1687436 Accuracy: 0.6\n",
      "Training - i: 101 Loss: 0.9773147 Accuracy: 0.685\n",
      "Validation - i: 101  Accuracy: [0.67]\n",
      "Training - i: 102 Loss: 0.9785192 Accuracy: 0.66\n",
      "Training - i: 103 Loss: 1.0531151 Accuracy: 0.64\n",
      "Training - i: 104 Loss: 1.0075601 Accuracy: 0.715\n",
      "Training - i: 105 Loss: 1.1103772 Accuracy: 0.64\n",
      "Training - i: 106 Loss: 1.0731467 Accuracy: 0.67\n",
      "Training - i: 107 Loss: 0.93989044 Accuracy: 0.71\n",
      "Training - i: 108 Loss: 1.0541546 Accuracy: 0.675\n",
      "Training - i: 109 Loss: 1.0038362 Accuracy: 0.67\n",
      "Training - i: 110 Loss: 0.9416278 Accuracy: 0.695\n",
      "Training - i: 111 Loss: 0.981782 Accuracy: 0.69\n",
      "Training - i: 112 Loss: 1.05092 Accuracy: 0.675\n",
      "Training - i: 113 Loss: 1.0367606 Accuracy: 0.645\n",
      "Training - i: 114 Loss: 0.9887745 Accuracy: 0.68\n",
      "Training - i: 115 Loss: 1.0963414 Accuracy: 0.615\n",
      "Training - i: 116 Loss: 1.0544798 Accuracy: 0.635\n",
      "Training - i: 117 Loss: 1.095042 Accuracy: 0.655\n",
      "Training - i: 118 Loss: 1.0447078 Accuracy: 0.635\n",
      "Training - i: 119 Loss: 1.0543735 Accuracy: 0.65\n",
      "Training - i: 120 Loss: 1.0460005 Accuracy: 0.66\n",
      "Training - i: 121 Loss: 1.0831859 Accuracy: 0.655\n",
      "Training - i: 122 Loss: 0.8414988 Accuracy: 0.74\n",
      "Training - i: 123 Loss: 0.9781734 Accuracy: 0.69\n",
      "Training - i: 124 Loss: 1.1499169 Accuracy: 0.635\n",
      "Training - i: 125 Loss: 0.9444373 Accuracy: 0.72\n",
      "Training - i: 126 Loss: 0.9139404 Accuracy: 0.705\n",
      "Training - i: 127 Loss: 1.0321196 Accuracy: 0.68\n",
      "Training - i: 128 Loss: 1.0787963 Accuracy: 0.61\n",
      "Training - i: 129 Loss: 1.0250664 Accuracy: 0.655\n",
      "Training - i: 130 Loss: 1.0064745 Accuracy: 0.66\n",
      "Training - i: 131 Loss: 1.0531976 Accuracy: 0.665\n",
      "Training - i: 132 Loss: 1.11529 Accuracy: 0.61\n",
      "Training - i: 133 Loss: 0.99018997 Accuracy: 0.695\n",
      "Training - i: 134 Loss: 0.92609817 Accuracy: 0.72\n",
      "Training - i: 135 Loss: 0.9739071 Accuracy: 0.675\n",
      "Training - i: 136 Loss: 0.9760432 Accuracy: 0.685\n",
      "Training - i: 137 Loss: 0.8705181 Accuracy: 0.72\n",
      "Training - i: 138 Loss: 1.073673 Accuracy: 0.675\n",
      "Training - i: 139 Loss: 1.0703609 Accuracy: 0.615\n",
      "Training - i: 140 Loss: 0.9892548 Accuracy: 0.695\n",
      "Training - i: 141 Loss: 1.1279798 Accuracy: 0.625\n",
      "Training - i: 142 Loss: 1.0090916 Accuracy: 0.685\n",
      "Training - i: 143 Loss: 0.92861986 Accuracy: 0.71\n",
      "Training - i: 144 Loss: 1.0407362 Accuracy: 0.63\n",
      "Training - i: 145 Loss: 1.0071062 Accuracy: 0.66\n",
      "Training - i: 146 Loss: 0.96642596 Accuracy: 0.68\n",
      "Training - i: 147 Loss: 1.0923301 Accuracy: 0.63\n",
      "Training - i: 148 Loss: 0.873871 Accuracy: 0.69\n",
      "Training - i: 149 Loss: 1.0789661 Accuracy: 0.635\n",
      "Training - i: 150 Loss: 1.1585132 Accuracy: 0.62\n",
      "Training - i: 151 Loss: 1.0237437 Accuracy: 0.645\n",
      "Validation - i: 151  Accuracy: [0.675]\n",
      "Training - i: 152 Loss: 1.0020779 Accuracy: 0.68\n",
      "Training - i: 153 Loss: 1.1545054 Accuracy: 0.605\n",
      "Training - i: 154 Loss: 1.0314428 Accuracy: 0.665\n",
      "Training - i: 155 Loss: 1.1255922 Accuracy: 0.615\n",
      "Training - i: 156 Loss: 0.99656343 Accuracy: 0.65\n",
      "Training - i: 157 Loss: 1.0303906 Accuracy: 0.63\n",
      "Training - i: 158 Loss: 0.99600035 Accuracy: 0.675\n",
      "Training - i: 159 Loss: 1.1319152 Accuracy: 0.585\n",
      "Training - i: 160 Loss: 0.96470624 Accuracy: 0.71\n",
      "Training - i: 161 Loss: 0.99398345 Accuracy: 0.655\n",
      "Training - i: 162 Loss: 0.9936079 Accuracy: 0.665\n",
      "Training - i: 163 Loss: 0.90000236 Accuracy: 0.705\n",
      "Training - i: 164 Loss: 0.9822246 Accuracy: 0.655\n",
      "Training - i: 165 Loss: 1.0100851 Accuracy: 0.65\n",
      "Training - i: 166 Loss: 0.9303054 Accuracy: 0.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - i: 167 Loss: 1.0733719 Accuracy: 0.64\n",
      "Training - i: 168 Loss: 0.95824945 Accuracy: 0.67\n",
      "Training - i: 169 Loss: 1.0354418 Accuracy: 0.645\n",
      "Training - i: 170 Loss: 0.9547444 Accuracy: 0.665\n",
      "Training - i: 171 Loss: 1.0507964 Accuracy: 0.66\n",
      "Training - i: 172 Loss: 0.9085906 Accuracy: 0.685\n",
      "Training - i: 173 Loss: 1.1529697 Accuracy: 0.625\n",
      "Training - i: 174 Loss: 0.9295259 Accuracy: 0.68\n",
      "Training - i: 175 Loss: 1.0066146 Accuracy: 0.66\n",
      "Training - i: 176 Loss: 0.9916388 Accuracy: 0.665\n",
      "Training - i: 177 Loss: 0.94854224 Accuracy: 0.66\n",
      "Training - i: 178 Loss: 1.0641519 Accuracy: 0.665\n",
      "Training - i: 179 Loss: 0.91304564 Accuracy: 0.695\n",
      "Training - i: 180 Loss: 1.0520914 Accuracy: 0.665\n",
      "Training - i: 181 Loss: 0.86249316 Accuracy: 0.7\n",
      "Training - i: 182 Loss: 1.0112658 Accuracy: 0.685\n",
      "Training - i: 183 Loss: 0.9611797 Accuracy: 0.685\n",
      "Training - i: 184 Loss: 1.0820553 Accuracy: 0.605\n",
      "Training - i: 185 Loss: 0.90476346 Accuracy: 0.7\n",
      "Training - i: 186 Loss: 0.9225441 Accuracy: 0.69\n",
      "Training - i: 187 Loss: 1.0070813 Accuracy: 0.68\n",
      "Training - i: 188 Loss: 0.8850539 Accuracy: 0.71\n",
      "Training - i: 189 Loss: 1.0704771 Accuracy: 0.645\n",
      "Training - i: 190 Loss: 0.90544677 Accuracy: 0.705\n",
      "Training - i: 191 Loss: 0.99370956 Accuracy: 0.62\n",
      "Training - i: 192 Loss: 0.9841597 Accuracy: 0.675\n",
      "Training - i: 193 Loss: 0.8234975 Accuracy: 0.75\n",
      "Training - i: 194 Loss: 0.904713 Accuracy: 0.665\n",
      "Training - i: 195 Loss: 0.9744732 Accuracy: 0.68\n",
      "Training - i: 196 Loss: 1.0483193 Accuracy: 0.665\n",
      "Training - i: 197 Loss: 0.8597668 Accuracy: 0.715\n",
      "Training - i: 198 Loss: 0.96163887 Accuracy: 0.655\n",
      "Training - i: 199 Loss: 0.945443 Accuracy: 0.655\n",
      "Training - i: 200 Loss: 1.164525 Accuracy: 0.615\n",
      "Training - i: 201 Loss: 0.96678954 Accuracy: 0.685\n",
      "Validation - i: 201  Accuracy: [0.66]\n",
      "Training - i: 202 Loss: 1.1265581 Accuracy: 0.62\n",
      "Training - i: 203 Loss: 1.0001335 Accuracy: 0.655\n",
      "Training - i: 204 Loss: 0.9368109 Accuracy: 0.71\n",
      "Training - i: 205 Loss: 0.99242264 Accuracy: 0.695\n",
      "Training - i: 206 Loss: 0.9831681 Accuracy: 0.7\n",
      "Training - i: 207 Loss: 0.9543316 Accuracy: 0.71\n",
      "Training - i: 208 Loss: 1.0253382 Accuracy: 0.665\n",
      "Training - i: 209 Loss: 1.0343537 Accuracy: 0.605\n",
      "Training - i: 210 Loss: 0.9230214 Accuracy: 0.695\n",
      "Training - i: 211 Loss: 1.0539922 Accuracy: 0.63\n",
      "Training - i: 212 Loss: 0.86694354 Accuracy: 0.72\n",
      "Training - i: 213 Loss: 1.1344231 Accuracy: 0.65\n",
      "Training - i: 214 Loss: 0.9061586 Accuracy: 0.685\n",
      "Training - i: 215 Loss: 0.95011365 Accuracy: 0.685\n",
      "Training - i: 216 Loss: 0.8805717 Accuracy: 0.705\n",
      "Training - i: 217 Loss: 1.0896316 Accuracy: 0.62\n",
      "Training - i: 218 Loss: 1.0998183 Accuracy: 0.61\n",
      "Training - i: 219 Loss: 0.93899095 Accuracy: 0.68\n",
      "Training - i: 220 Loss: 1.0905339 Accuracy: 0.64\n",
      "Training - i: 221 Loss: 1.0918182 Accuracy: 0.645\n",
      "Training - i: 222 Loss: 1.0044178 Accuracy: 0.645\n",
      "Training - i: 223 Loss: 0.8872743 Accuracy: 0.715\n",
      "Training - i: 224 Loss: 1.0648482 Accuracy: 0.62\n",
      "Training - i: 225 Loss: 0.96256256 Accuracy: 0.68\n",
      "Training - i: 226 Loss: 1.0268894 Accuracy: 0.64\n",
      "Training - i: 227 Loss: 1.1211324 Accuracy: 0.625\n",
      "Training - i: 228 Loss: 0.99931055 Accuracy: 0.665\n",
      "Training - i: 229 Loss: 0.872399 Accuracy: 0.7\n",
      "Training - i: 230 Loss: 1.0210379 Accuracy: 0.65\n",
      "Training - i: 231 Loss: 0.96356016 Accuracy: 0.675\n",
      "Training - i: 232 Loss: 0.96961904 Accuracy: 0.685\n",
      "Training - i: 233 Loss: 0.981777 Accuracy: 0.635\n",
      "Training - i: 234 Loss: 1.0266556 Accuracy: 0.635\n",
      "Training - i: 235 Loss: 0.73505384 Accuracy: 0.765\n",
      "Training - i: 236 Loss: 0.9566501 Accuracy: 0.665\n",
      "Training - i: 237 Loss: 1.0233897 Accuracy: 0.625\n",
      "Training - i: 238 Loss: 0.99225587 Accuracy: 0.655\n",
      "Training - i: 239 Loss: 0.9903815 Accuracy: 0.635\n",
      "Training - i: 240 Loss: 0.8561514 Accuracy: 0.735\n",
      "Training - i: 241 Loss: 0.97816956 Accuracy: 0.695\n",
      "Training - i: 242 Loss: 0.9861444 Accuracy: 0.65\n",
      "Training - i: 243 Loss: 1.0018128 Accuracy: 0.68\n",
      "Training - i: 244 Loss: 0.9041756 Accuracy: 0.685\n",
      "Training - i: 245 Loss: 0.93378866 Accuracy: 0.665\n",
      "Training - i: 246 Loss: 0.95830965 Accuracy: 0.68\n",
      "Training - i: 247 Loss: 1.0664371 Accuracy: 0.635\n",
      "Training - i: 248 Loss: 0.891548 Accuracy: 0.695\n",
      "Training - i: 249 Loss: 0.85753417 Accuracy: 0.68\n",
      "Training - i: 250 Loss: 0.9010028 Accuracy: 0.675\n",
      "Training - i: 251 Loss: 0.9456714 Accuracy: 0.685\n",
      "Validation - i: 251  Accuracy: [0.695]\n",
      "Training - i: 252 Loss: 0.99388796 Accuracy: 0.68\n",
      "Training - i: 253 Loss: 0.8124058 Accuracy: 0.73\n",
      "Training - i: 254 Loss: 1.0372648 Accuracy: 0.64\n",
      "Training - i: 255 Loss: 0.95282346 Accuracy: 0.675\n",
      "Training - i: 256 Loss: 0.92252177 Accuracy: 0.69\n",
      "Training - i: 257 Loss: 0.9166702 Accuracy: 0.665\n",
      "Training - i: 258 Loss: 0.9033817 Accuracy: 0.71\n",
      "Training - i: 259 Loss: 0.93482184 Accuracy: 0.69\n",
      "Training - i: 260 Loss: 0.9134328 Accuracy: 0.71\n",
      "Training - i: 261 Loss: 1.0222777 Accuracy: 0.64\n",
      "Training - i: 262 Loss: 0.876716 Accuracy: 0.695\n",
      "Training - i: 263 Loss: 0.81502795 Accuracy: 0.725\n",
      "Training - i: 264 Loss: 0.94873124 Accuracy: 0.66\n",
      "Training - i: 265 Loss: 0.85996413 Accuracy: 0.71\n",
      "Training - i: 266 Loss: 0.97581846 Accuracy: 0.72\n",
      "Training - i: 267 Loss: 0.9814047 Accuracy: 0.625\n",
      "Training - i: 268 Loss: 0.9186857 Accuracy: 0.7\n",
      "Training - i: 269 Loss: 1.0320758 Accuracy: 0.625\n",
      "Training - i: 270 Loss: 0.89866203 Accuracy: 0.72\n",
      "Training - i: 271 Loss: 0.9643201 Accuracy: 0.66\n",
      "Training - i: 272 Loss: 0.92524904 Accuracy: 0.695\n",
      "Training - i: 273 Loss: 0.9574225 Accuracy: 0.665\n",
      "Training - i: 274 Loss: 1.0133191 Accuracy: 0.635\n",
      "Training - i: 275 Loss: 0.9246471 Accuracy: 0.72\n",
      "Training - i: 276 Loss: 0.9825771 Accuracy: 0.665\n",
      "Training - i: 277 Loss: 0.9448599 Accuracy: 0.68\n",
      "Training - i: 278 Loss: 0.9693494 Accuracy: 0.65\n",
      "Training - i: 279 Loss: 0.9418642 Accuracy: 0.69\n",
      "Training - i: 280 Loss: 0.7802121 Accuracy: 0.755\n",
      "Training - i: 281 Loss: 0.9773901 Accuracy: 0.655\n",
      "Training - i: 282 Loss: 0.99127626 Accuracy: 0.64\n",
      "Training - i: 283 Loss: 0.8942414 Accuracy: 0.69\n",
      "Training - i: 284 Loss: 0.9078957 Accuracy: 0.73\n",
      "Training - i: 285 Loss: 1.0201455 Accuracy: 0.645\n",
      "Training - i: 286 Loss: 1.0409131 Accuracy: 0.655\n",
      "Training - i: 287 Loss: 0.9766965 Accuracy: 0.675\n",
      "Training - i: 288 Loss: 1.0048007 Accuracy: 0.67\n",
      "Training - i: 289 Loss: 0.86171603 Accuracy: 0.7\n",
      "Training - i: 290 Loss: 1.0183033 Accuracy: 0.685\n",
      "Training - i: 291 Loss: 0.86112714 Accuracy: 0.705\n",
      "Training - i: 292 Loss: 0.83250344 Accuracy: 0.715\n",
      "Training - i: 293 Loss: 0.9180522 Accuracy: 0.66\n",
      "Training - i: 294 Loss: 0.8218278 Accuracy: 0.745\n",
      "Training - i: 295 Loss: 0.92270845 Accuracy: 0.69\n",
      "Training - i: 296 Loss: 1.0098609 Accuracy: 0.65\n",
      "Training - i: 297 Loss: 0.93574446 Accuracy: 0.66\n",
      "Training - i: 298 Loss: 0.868761 Accuracy: 0.695\n",
      "Training - i: 299 Loss: 0.98854464 Accuracy: 0.66\n",
      "Training - i: 300 Loss: 0.8918598 Accuracy: 0.715\n",
      "Training - i: 301 Loss: 0.8827431 Accuracy: 0.69\n",
      "Validation - i: 301  Accuracy: [0.72]\n",
      "Training - i: 302 Loss: 0.89237905 Accuracy: 0.71\n",
      "Training - i: 303 Loss: 0.95581347 Accuracy: 0.675\n",
      "Training - i: 304 Loss: 0.92563903 Accuracy: 0.69\n",
      "Training - i: 305 Loss: 0.9541236 Accuracy: 0.66\n",
      "Training - i: 306 Loss: 1.0477791 Accuracy: 0.64\n",
      "Training - i: 307 Loss: 0.9897865 Accuracy: 0.67\n",
      "Training - i: 308 Loss: 0.8552462 Accuracy: 0.685\n",
      "Training - i: 309 Loss: 0.92557716 Accuracy: 0.705\n",
      "Training - i: 310 Loss: 0.941972 Accuracy: 0.64\n",
      "Training - i: 311 Loss: 0.78635406 Accuracy: 0.755\n",
      "Training - i: 312 Loss: 0.81599 Accuracy: 0.725\n",
      "Training - i: 313 Loss: 0.8352188 Accuracy: 0.695\n",
      "Training - i: 314 Loss: 0.9395172 Accuracy: 0.665\n",
      "Training - i: 315 Loss: 0.9186516 Accuracy: 0.67\n",
      "Training - i: 316 Loss: 0.9862041 Accuracy: 0.66\n",
      "Training - i: 317 Loss: 0.9879974 Accuracy: 0.625\n",
      "Training - i: 318 Loss: 0.91225904 Accuracy: 0.685\n",
      "Training - i: 319 Loss: 0.86311126 Accuracy: 0.695\n",
      "Training - i: 320 Loss: 0.9100323 Accuracy: 0.695\n",
      "Training - i: 321 Loss: 0.86083895 Accuracy: 0.675\n",
      "Training - i: 322 Loss: 0.938482 Accuracy: 0.655\n",
      "Training - i: 323 Loss: 0.99082065 Accuracy: 0.63\n",
      "Training - i: 324 Loss: 0.91158766 Accuracy: 0.675\n",
      "Training - i: 325 Loss: 0.7839534 Accuracy: 0.715\n",
      "Training - i: 326 Loss: 0.9133187 Accuracy: 0.675\n",
      "Training - i: 327 Loss: 0.91677034 Accuracy: 0.655\n",
      "Training - i: 328 Loss: 0.8547074 Accuracy: 0.71\n",
      "Training - i: 329 Loss: 0.8292233 Accuracy: 0.715\n",
      "Training - i: 330 Loss: 0.9279917 Accuracy: 0.7\n",
      "Training - i: 331 Loss: 0.85653305 Accuracy: 0.695\n",
      "Training - i: 332 Loss: 0.7890039 Accuracy: 0.73\n",
      "Training - i: 333 Loss: 1.0371237 Accuracy: 0.605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - i: 334 Loss: 0.81539255 Accuracy: 0.72\n",
      "Training - i: 335 Loss: 0.82162726 Accuracy: 0.77\n",
      "Training - i: 336 Loss: 0.9016156 Accuracy: 0.7\n",
      "Training - i: 337 Loss: 0.82832694 Accuracy: 0.69\n",
      "Training - i: 338 Loss: 0.8240779 Accuracy: 0.745\n",
      "Training - i: 339 Loss: 0.89304304 Accuracy: 0.69\n",
      "Training - i: 340 Loss: 0.85470843 Accuracy: 0.68\n",
      "Training - i: 341 Loss: 0.8787743 Accuracy: 0.685\n",
      "Training - i: 342 Loss: 0.8028795 Accuracy: 0.725\n",
      "Training - i: 343 Loss: 0.83281344 Accuracy: 0.73\n",
      "Training - i: 344 Loss: 0.82390755 Accuracy: 0.73\n",
      "Training - i: 345 Loss: 0.86113197 Accuracy: 0.7\n",
      "Training - i: 346 Loss: 0.81306845 Accuracy: 0.715\n",
      "Training - i: 347 Loss: 0.7019093 Accuracy: 0.795\n",
      "Training - i: 348 Loss: 0.9795627 Accuracy: 0.645\n",
      "Training - i: 349 Loss: 0.86348146 Accuracy: 0.705\n",
      "Training - i: 350 Loss: 0.866639 Accuracy: 0.705\n",
      "Training - i: 351 Loss: 0.8624703 Accuracy: 0.68\n",
      "Validation - i: 351  Accuracy: [0.725]\n",
      "Training - i: 352 Loss: 0.69788843 Accuracy: 0.755\n",
      "Training - i: 353 Loss: 0.9106751 Accuracy: 0.675\n",
      "Training - i: 354 Loss: 0.848685 Accuracy: 0.705\n",
      "Training - i: 355 Loss: 0.88773483 Accuracy: 0.705\n",
      "Training - i: 356 Loss: 0.8222931 Accuracy: 0.705\n",
      "Training - i: 357 Loss: 0.9108162 Accuracy: 0.67\n",
      "Training - i: 358 Loss: 0.6848877 Accuracy: 0.805\n",
      "Training - i: 359 Loss: 0.9277148 Accuracy: 0.66\n",
      "Training - i: 360 Loss: 0.74446213 Accuracy: 0.725\n",
      "Training - i: 361 Loss: 0.9118865 Accuracy: 0.675\n",
      "Training - i: 362 Loss: 0.8666478 Accuracy: 0.705\n",
      "Training - i: 363 Loss: 0.83426404 Accuracy: 0.755\n",
      "Training - i: 364 Loss: 0.86715335 Accuracy: 0.665\n",
      "Training - i: 365 Loss: 0.82938504 Accuracy: 0.715\n",
      "Training - i: 366 Loss: 0.7716272 Accuracy: 0.74\n",
      "Training - i: 367 Loss: 0.8531668 Accuracy: 0.685\n",
      "Training - i: 368 Loss: 0.7928604 Accuracy: 0.695\n",
      "Training - i: 369 Loss: 0.8811746 Accuracy: 0.705\n",
      "Training - i: 370 Loss: 0.8547417 Accuracy: 0.715\n",
      "Training - i: 371 Loss: 0.86162984 Accuracy: 0.68\n",
      "Training - i: 372 Loss: 0.83248186 Accuracy: 0.745\n",
      "Training - i: 373 Loss: 0.94394 Accuracy: 0.64\n",
      "Training - i: 374 Loss: 0.8141278 Accuracy: 0.725\n",
      "Training - i: 375 Loss: 0.8989411 Accuracy: 0.675\n",
      "Training - i: 376 Loss: 0.8143542 Accuracy: 0.72\n",
      "Training - i: 377 Loss: 0.7752991 Accuracy: 0.73\n",
      "Training - i: 378 Loss: 0.86928266 Accuracy: 0.695\n",
      "Training - i: 379 Loss: 0.8432352 Accuracy: 0.695\n",
      "Training - i: 380 Loss: 0.8412375 Accuracy: 0.74\n",
      "Training - i: 381 Loss: 0.8887017 Accuracy: 0.71\n",
      "Training - i: 382 Loss: 0.8428232 Accuracy: 0.72\n",
      "Training - i: 383 Loss: 0.92336106 Accuracy: 0.675\n",
      "Training - i: 384 Loss: 0.89833635 Accuracy: 0.675\n",
      "Training - i: 385 Loss: 0.9076954 Accuracy: 0.64\n",
      "Training - i: 386 Loss: 0.9522081 Accuracy: 0.65\n",
      "Training - i: 387 Loss: 0.89362735 Accuracy: 0.655\n",
      "Training - i: 388 Loss: 0.90332496 Accuracy: 0.705\n",
      "Training - i: 389 Loss: 0.8096884 Accuracy: 0.73\n",
      "Training - i: 390 Loss: 0.8754265 Accuracy: 0.715\n",
      "Training - i: 391 Loss: 0.8006975 Accuracy: 0.735\n",
      "Training - i: 392 Loss: 0.8080733 Accuracy: 0.71\n",
      "Training - i: 393 Loss: 0.82509124 Accuracy: 0.705\n",
      "Training - i: 394 Loss: 0.7408039 Accuracy: 0.755\n",
      "Training - i: 395 Loss: 0.8210221 Accuracy: 0.71\n",
      "Training - i: 396 Loss: 0.8045356 Accuracy: 0.71\n",
      "Training - i: 397 Loss: 0.79840034 Accuracy: 0.715\n",
      "Training - i: 398 Loss: 0.8338762 Accuracy: 0.67\n",
      "Training - i: 399 Loss: 0.79078317 Accuracy: 0.735\n",
      "Training - i: 400 Loss: 0.8175758 Accuracy: 0.745\n",
      "Training - i: 401 Loss: 0.7667072 Accuracy: 0.755\n",
      "Validation - i: 401  Accuracy: [0.74]\n",
      "Training - i: 402 Loss: 0.68206865 Accuracy: 0.785\n",
      "Training - i: 403 Loss: 0.7878928 Accuracy: 0.73\n",
      "Training - i: 404 Loss: 0.89301056 Accuracy: 0.655\n",
      "Training - i: 405 Loss: 0.85010666 Accuracy: 0.715\n",
      "Training - i: 406 Loss: 0.7176009 Accuracy: 0.72\n",
      "Training - i: 407 Loss: 0.7919736 Accuracy: 0.735\n",
      "Training - i: 408 Loss: 0.8314448 Accuracy: 0.72\n",
      "Training - i: 409 Loss: 0.73453444 Accuracy: 0.715\n",
      "Training - i: 410 Loss: 0.7151773 Accuracy: 0.77\n",
      "Training - i: 411 Loss: 0.76074827 Accuracy: 0.735\n",
      "Training - i: 412 Loss: 0.96512634 Accuracy: 0.655\n",
      "Training - i: 413 Loss: 0.78701246 Accuracy: 0.69\n",
      "Training - i: 414 Loss: 0.81459594 Accuracy: 0.73\n",
      "Training - i: 415 Loss: 0.75050235 Accuracy: 0.74\n",
      "Training - i: 416 Loss: 0.838495 Accuracy: 0.715\n",
      "Training - i: 417 Loss: 0.80496126 Accuracy: 0.7\n",
      "Training - i: 418 Loss: 0.7153652 Accuracy: 0.765\n",
      "Training - i: 419 Loss: 0.8600064 Accuracy: 0.685\n",
      "Training - i: 420 Loss: 0.7584015 Accuracy: 0.715\n",
      "Training - i: 421 Loss: 0.78404534 Accuracy: 0.735\n",
      "Training - i: 422 Loss: 0.7995542 Accuracy: 0.755\n",
      "Training - i: 423 Loss: 0.84294915 Accuracy: 0.67\n",
      "Training - i: 424 Loss: 0.7786895 Accuracy: 0.72\n",
      "Training - i: 425 Loss: 0.82107466 Accuracy: 0.68\n",
      "Training - i: 426 Loss: 0.75147337 Accuracy: 0.765\n",
      "Training - i: 427 Loss: 0.83354706 Accuracy: 0.67\n",
      "Training - i: 428 Loss: 0.9006639 Accuracy: 0.69\n",
      "Training - i: 429 Loss: 0.77435446 Accuracy: 0.725\n",
      "Training - i: 430 Loss: 0.70580477 Accuracy: 0.735\n",
      "Training - i: 431 Loss: 0.79665726 Accuracy: 0.71\n",
      "Training - i: 432 Loss: 0.7548851 Accuracy: 0.71\n",
      "Training - i: 433 Loss: 0.747012 Accuracy: 0.745\n",
      "Training - i: 434 Loss: 0.7845816 Accuracy: 0.71\n",
      "Training - i: 435 Loss: 0.8525707 Accuracy: 0.675\n",
      "Training - i: 436 Loss: 0.8207234 Accuracy: 0.725\n",
      "Training - i: 437 Loss: 0.7867385 Accuracy: 0.67\n",
      "Training - i: 438 Loss: 0.92967194 Accuracy: 0.625\n",
      "Training - i: 439 Loss: 0.7676819 Accuracy: 0.75\n",
      "Training - i: 440 Loss: 0.6735346 Accuracy: 0.755\n",
      "Training - i: 441 Loss: 0.8313553 Accuracy: 0.7\n",
      "Training - i: 442 Loss: 0.7513262 Accuracy: 0.765\n",
      "Training - i: 443 Loss: 0.7614878 Accuracy: 0.74\n",
      "Training - i: 444 Loss: 0.8593552 Accuracy: 0.7\n",
      "Training - i: 445 Loss: 0.63687867 Accuracy: 0.79\n",
      "Training - i: 446 Loss: 0.71039665 Accuracy: 0.78\n",
      "Training - i: 447 Loss: 0.7694299 Accuracy: 0.735\n",
      "Training - i: 448 Loss: 0.7013046 Accuracy: 0.72\n",
      "Training - i: 449 Loss: 0.74673486 Accuracy: 0.74\n",
      "Training - i: 450 Loss: 0.7758069 Accuracy: 0.725\n",
      "Training - i: 451 Loss: 0.7395674 Accuracy: 0.72\n",
      "Validation - i: 451  Accuracy: [0.735]\n",
      "Training - i: 452 Loss: 0.7329692 Accuracy: 0.725\n",
      "Training - i: 453 Loss: 0.7289938 Accuracy: 0.77\n",
      "Training - i: 454 Loss: 0.77241987 Accuracy: 0.705\n",
      "Training - i: 455 Loss: 0.750939 Accuracy: 0.77\n",
      "Training - i: 456 Loss: 0.7046599 Accuracy: 0.765\n",
      "Training - i: 457 Loss: 0.77831453 Accuracy: 0.71\n",
      "Training - i: 458 Loss: 0.7493821 Accuracy: 0.725\n",
      "Training - i: 459 Loss: 0.74770457 Accuracy: 0.745\n",
      "Training - i: 460 Loss: 0.72575563 Accuracy: 0.745\n",
      "Training - i: 461 Loss: 0.815065 Accuracy: 0.685\n",
      "Training - i: 462 Loss: 0.8182919 Accuracy: 0.725\n",
      "Training - i: 463 Loss: 0.8577725 Accuracy: 0.695\n",
      "Training - i: 464 Loss: 0.7726768 Accuracy: 0.72\n",
      "Training - i: 465 Loss: 0.7234859 Accuracy: 0.725\n",
      "Training - i: 466 Loss: 0.84669554 Accuracy: 0.685\n",
      "Training - i: 467 Loss: 0.7678606 Accuracy: 0.72\n",
      "Training - i: 468 Loss: 0.822369 Accuracy: 0.68\n",
      "Training - i: 469 Loss: 0.66613555 Accuracy: 0.765\n",
      "Training - i: 470 Loss: 0.7036381 Accuracy: 0.765\n",
      "Training - i: 471 Loss: 0.6841803 Accuracy: 0.75\n",
      "Training - i: 472 Loss: 0.6501328 Accuracy: 0.805\n",
      "Training - i: 473 Loss: 0.68202543 Accuracy: 0.775\n",
      "Training - i: 474 Loss: 0.59981865 Accuracy: 0.78\n",
      "Training - i: 475 Loss: 0.68721545 Accuracy: 0.755\n",
      "Training - i: 476 Loss: 0.68954206 Accuracy: 0.745\n",
      "Training - i: 477 Loss: 0.72204053 Accuracy: 0.755\n",
      "Training - i: 478 Loss: 0.65709704 Accuracy: 0.75\n",
      "Training - i: 479 Loss: 0.759363 Accuracy: 0.705\n",
      "Training - i: 480 Loss: 0.6581761 Accuracy: 0.78\n",
      "Training - i: 481 Loss: 0.8594161 Accuracy: 0.695\n",
      "Training - i: 482 Loss: 0.703335 Accuracy: 0.76\n",
      "Training - i: 483 Loss: 0.69634324 Accuracy: 0.735\n",
      "Training - i: 484 Loss: 0.70168626 Accuracy: 0.755\n",
      "Training - i: 485 Loss: 0.7163344 Accuracy: 0.73\n",
      "Training - i: 486 Loss: 0.6152001 Accuracy: 0.825\n",
      "Training - i: 487 Loss: 0.73148954 Accuracy: 0.74\n",
      "Training - i: 488 Loss: 0.6027228 Accuracy: 0.78\n",
      "Training - i: 489 Loss: 0.6773169 Accuracy: 0.755\n",
      "Training - i: 490 Loss: 0.70058084 Accuracy: 0.745\n",
      "Training - i: 491 Loss: 0.7394133 Accuracy: 0.695\n",
      "Training - i: 492 Loss: 0.7511681 Accuracy: 0.735\n",
      "Training - i: 493 Loss: 0.6753213 Accuracy: 0.775\n",
      "Training - i: 494 Loss: 0.6558197 Accuracy: 0.775\n",
      "Training - i: 495 Loss: 0.7898457 Accuracy: 0.71\n",
      "Training - i: 496 Loss: 0.7805133 Accuracy: 0.72\n",
      "Training - i: 497 Loss: 0.7436249 Accuracy: 0.74\n",
      "Training - i: 498 Loss: 0.7339312 Accuracy: 0.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - i: 499 Loss: 0.7038064 Accuracy: 0.765\n",
      "Training - i: 500 Loss: 0.80887926 Accuracy: 0.7\n",
      "Training - i: 501 Loss: 0.81070787 Accuracy: 0.69\n",
      "Validation - i: 501  Accuracy: [0.705]\n",
      "Training - i: 502 Loss: 0.6114567 Accuracy: 0.8\n",
      "Training - i: 503 Loss: 0.6338581 Accuracy: 0.765\n",
      "Training - i: 504 Loss: 0.70519686 Accuracy: 0.765\n",
      "Training - i: 505 Loss: 0.6685213 Accuracy: 0.78\n",
      "Training - i: 506 Loss: 0.6805468 Accuracy: 0.755\n",
      "Training - i: 507 Loss: 0.8068964 Accuracy: 0.69\n",
      "Training - i: 508 Loss: 0.6132643 Accuracy: 0.785\n",
      "Training - i: 509 Loss: 0.6398418 Accuracy: 0.765\n",
      "Training - i: 510 Loss: 0.7825974 Accuracy: 0.705\n",
      "Training - i: 511 Loss: 0.63337237 Accuracy: 0.755\n",
      "Training - i: 512 Loss: 0.6403935 Accuracy: 0.785\n",
      "Training - i: 513 Loss: 0.6187208 Accuracy: 0.795\n",
      "Training - i: 514 Loss: 0.6879503 Accuracy: 0.78\n",
      "Training - i: 515 Loss: 0.65581214 Accuracy: 0.765\n",
      "Training - i: 516 Loss: 0.63339925 Accuracy: 0.76\n",
      "Training - i: 517 Loss: 0.5630359 Accuracy: 0.81\n",
      "Training - i: 518 Loss: 0.64108795 Accuracy: 0.77\n",
      "Training - i: 519 Loss: 0.6580051 Accuracy: 0.755\n",
      "Training - i: 520 Loss: 0.7030577 Accuracy: 0.76\n",
      "Training - i: 521 Loss: 0.74740267 Accuracy: 0.74\n",
      "Training - i: 522 Loss: 0.6699177 Accuracy: 0.77\n",
      "Training - i: 523 Loss: 0.7556851 Accuracy: 0.69\n",
      "Training - i: 524 Loss: 0.71132493 Accuracy: 0.725\n",
      "Training - i: 525 Loss: 0.69255465 Accuracy: 0.745\n",
      "Training - i: 526 Loss: 0.7425565 Accuracy: 0.735\n",
      "Training - i: 527 Loss: 0.6504065 Accuracy: 0.765\n",
      "Training - i: 528 Loss: 0.7255466 Accuracy: 0.71\n",
      "Training - i: 529 Loss: 0.7837279 Accuracy: 0.71\n",
      "Training - i: 530 Loss: 0.6387097 Accuracy: 0.755\n",
      "Training - i: 531 Loss: 0.6301823 Accuracy: 0.785\n",
      "Training - i: 532 Loss: 0.69643265 Accuracy: 0.735\n",
      "Training - i: 533 Loss: 0.688816 Accuracy: 0.775\n",
      "Training - i: 534 Loss: 0.62528646 Accuracy: 0.765\n",
      "Training - i: 535 Loss: 0.6039305 Accuracy: 0.76\n",
      "Training - i: 536 Loss: 0.5565614 Accuracy: 0.8\n",
      "Training - i: 537 Loss: 0.7227518 Accuracy: 0.735\n",
      "Training - i: 538 Loss: 0.75723666 Accuracy: 0.75\n",
      "Training - i: 539 Loss: 0.6954373 Accuracy: 0.76\n",
      "Training - i: 540 Loss: 0.7521992 Accuracy: 0.68\n",
      "Training - i: 541 Loss: 0.6016311 Accuracy: 0.78\n",
      "Training - i: 542 Loss: 0.5932997 Accuracy: 0.8\n",
      "Training - i: 543 Loss: 0.6622709 Accuracy: 0.79\n",
      "Training - i: 544 Loss: 0.6514819 Accuracy: 0.725\n",
      "Training - i: 545 Loss: 0.5719955 Accuracy: 0.795\n",
      "Training - i: 546 Loss: 0.63073426 Accuracy: 0.76\n",
      "Training - i: 547 Loss: 0.56227344 Accuracy: 0.79\n",
      "Training - i: 548 Loss: 0.7290339 Accuracy: 0.745\n",
      "Training - i: 549 Loss: 0.6231099 Accuracy: 0.76\n",
      "Training - i: 550 Loss: 0.60391736 Accuracy: 0.77\n",
      "Training - i: 551 Loss: 0.7147429 Accuracy: 0.725\n",
      "Validation - i: 551  Accuracy: [0.72]\n",
      "Training - i: 552 Loss: 0.6225911 Accuracy: 0.78\n",
      "Training - i: 553 Loss: 0.63429254 Accuracy: 0.78\n",
      "Training - i: 554 Loss: 0.6630397 Accuracy: 0.765\n",
      "Training - i: 555 Loss: 0.58487374 Accuracy: 0.775\n",
      "Training - i: 556 Loss: 0.64194727 Accuracy: 0.725\n",
      "Training - i: 557 Loss: 0.72882426 Accuracy: 0.69\n",
      "Training - i: 558 Loss: 0.6396509 Accuracy: 0.76\n",
      "Training - i: 559 Loss: 0.5333103 Accuracy: 0.815\n",
      "Training - i: 560 Loss: 0.5539419 Accuracy: 0.81\n",
      "Training - i: 561 Loss: 0.58331156 Accuracy: 0.815\n",
      "Training - i: 562 Loss: 0.6979847 Accuracy: 0.71\n",
      "Training - i: 563 Loss: 0.6342777 Accuracy: 0.78\n",
      "Training - i: 564 Loss: 0.64120424 Accuracy: 0.795\n",
      "Training - i: 565 Loss: 0.6203344 Accuracy: 0.805\n",
      "Training - i: 566 Loss: 0.6339168 Accuracy: 0.795\n",
      "Training - i: 567 Loss: 0.6617926 Accuracy: 0.75\n",
      "Training - i: 568 Loss: 0.5453725 Accuracy: 0.82\n",
      "Training - i: 569 Loss: 0.5341773 Accuracy: 0.8\n",
      "Training - i: 570 Loss: 0.6110269 Accuracy: 0.765\n",
      "Training - i: 571 Loss: 0.5699368 Accuracy: 0.775\n",
      "Training - i: 572 Loss: 0.6863457 Accuracy: 0.735\n",
      "Training - i: 573 Loss: 0.6468759 Accuracy: 0.725\n",
      "Training - i: 574 Loss: 0.55976367 Accuracy: 0.785\n",
      "Training - i: 575 Loss: 0.6132405 Accuracy: 0.75\n",
      "Training - i: 576 Loss: 0.59545815 Accuracy: 0.785\n",
      "Training - i: 577 Loss: 0.5559396 Accuracy: 0.79\n",
      "Training - i: 578 Loss: 0.6242911 Accuracy: 0.765\n",
      "Training - i: 579 Loss: 0.49052957 Accuracy: 0.815\n",
      "Training - i: 580 Loss: 0.5633299 Accuracy: 0.805\n",
      "Training - i: 581 Loss: 0.52875054 Accuracy: 0.785\n",
      "Training - i: 582 Loss: 0.58780724 Accuracy: 0.775\n",
      "Training - i: 583 Loss: 0.58317596 Accuracy: 0.81\n",
      "Training - i: 584 Loss: 0.5390804 Accuracy: 0.785\n",
      "Training - i: 585 Loss: 0.5733582 Accuracy: 0.78\n",
      "Training - i: 586 Loss: 0.5294583 Accuracy: 0.81\n",
      "Training - i: 587 Loss: 0.6670305 Accuracy: 0.73\n",
      "Training - i: 588 Loss: 0.5795475 Accuracy: 0.8\n",
      "Training - i: 589 Loss: 0.6276623 Accuracy: 0.745\n",
      "Training - i: 590 Loss: 0.56807053 Accuracy: 0.77\n",
      "Training - i: 591 Loss: 0.54277015 Accuracy: 0.795\n",
      "Training - i: 592 Loss: 0.5745763 Accuracy: 0.765\n",
      "Training - i: 593 Loss: 0.49414447 Accuracy: 0.83\n",
      "Training - i: 594 Loss: 0.632933 Accuracy: 0.76\n",
      "Training - i: 595 Loss: 0.5830404 Accuracy: 0.775\n",
      "Training - i: 596 Loss: 0.6215864 Accuracy: 0.78\n",
      "Training - i: 597 Loss: 0.59368616 Accuracy: 0.775\n",
      "Training - i: 598 Loss: 0.6269266 Accuracy: 0.77\n",
      "Training - i: 599 Loss: 0.5379776 Accuracy: 0.815\n",
      "Training - i: 600 Loss: 0.54819983 Accuracy: 0.785\n",
      "Training - i: 601 Loss: 0.52788657 Accuracy: 0.83\n",
      "Validation - i: 601  Accuracy: [0.82]\n",
      "Training - i: 602 Loss: 0.54819524 Accuracy: 0.795\n",
      "Training - i: 603 Loss: 0.61859953 Accuracy: 0.765\n",
      "Training - i: 604 Loss: 0.678011 Accuracy: 0.71\n",
      "Training - i: 605 Loss: 0.4707226 Accuracy: 0.825\n",
      "Training - i: 606 Loss: 0.48317054 Accuracy: 0.82\n",
      "Training - i: 607 Loss: 0.50950044 Accuracy: 0.835\n",
      "Training - i: 608 Loss: 0.6068624 Accuracy: 0.73\n",
      "Training - i: 609 Loss: 0.5768918 Accuracy: 0.735\n",
      "Training - i: 610 Loss: 0.5833293 Accuracy: 0.77\n",
      "Training - i: 611 Loss: 0.49890283 Accuracy: 0.83\n",
      "Training - i: 612 Loss: 0.6106023 Accuracy: 0.76\n",
      "Training - i: 613 Loss: 0.5919738 Accuracy: 0.775\n",
      "Training - i: 614 Loss: 0.5216294 Accuracy: 0.825\n",
      "Training - i: 615 Loss: 0.46440223 Accuracy: 0.825\n",
      "Training - i: 616 Loss: 0.5662496 Accuracy: 0.8\n",
      "Training - i: 617 Loss: 0.5935231 Accuracy: 0.79\n",
      "Training - i: 618 Loss: 0.49276778 Accuracy: 0.82\n",
      "Training - i: 619 Loss: 0.49483418 Accuracy: 0.825\n",
      "Training - i: 620 Loss: 0.52798223 Accuracy: 0.795\n",
      "Training - i: 621 Loss: 0.48978174 Accuracy: 0.83\n",
      "Training - i: 622 Loss: 0.5743414 Accuracy: 0.79\n",
      "Training - i: 623 Loss: 0.49780238 Accuracy: 0.825\n",
      "Training - i: 624 Loss: 0.60361534 Accuracy: 0.76\n",
      "Training - i: 625 Loss: 0.5133983 Accuracy: 0.8\n",
      "Training - i: 626 Loss: 0.55432194 Accuracy: 0.755\n",
      "Training - i: 627 Loss: 0.5542881 Accuracy: 0.79\n",
      "Training - i: 628 Loss: 0.5178972 Accuracy: 0.81\n",
      "Training - i: 629 Loss: 0.48086122 Accuracy: 0.79\n",
      "Training - i: 630 Loss: 0.5595636 Accuracy: 0.79\n",
      "Training - i: 631 Loss: 0.5575692 Accuracy: 0.78\n",
      "Training - i: 632 Loss: 0.5826959 Accuracy: 0.79\n",
      "Training - i: 633 Loss: 0.5503955 Accuracy: 0.765\n",
      "Training - i: 634 Loss: 0.4587406 Accuracy: 0.83\n",
      "Training - i: 635 Loss: 0.4628302 Accuracy: 0.83\n",
      "Training - i: 636 Loss: 0.4489008 Accuracy: 0.83\n",
      "Training - i: 637 Loss: 0.53852683 Accuracy: 0.78\n",
      "Training - i: 638 Loss: 0.45623153 Accuracy: 0.835\n",
      "Training - i: 639 Loss: 0.4673187 Accuracy: 0.81\n",
      "Training - i: 640 Loss: 0.50377274 Accuracy: 0.775\n",
      "Training - i: 641 Loss: 0.48235923 Accuracy: 0.815\n",
      "Training - i: 642 Loss: 0.55555856 Accuracy: 0.785\n",
      "Training - i: 643 Loss: 0.4753516 Accuracy: 0.815\n",
      "Training - i: 644 Loss: 0.4931262 Accuracy: 0.78\n",
      "Training - i: 645 Loss: 0.5396454 Accuracy: 0.795\n",
      "Training - i: 646 Loss: 0.5254136 Accuracy: 0.8\n",
      "Training - i: 647 Loss: 0.5208584 Accuracy: 0.805\n",
      "Training - i: 648 Loss: 0.5098661 Accuracy: 0.815\n",
      "Training - i: 649 Loss: 0.4519927 Accuracy: 0.83\n",
      "Training - i: 650 Loss: 0.5354015 Accuracy: 0.825\n",
      "Training - i: 651 Loss: 0.46959388 Accuracy: 0.785\n",
      "Validation - i: 651  Accuracy: [0.775]\n",
      "Training - i: 652 Loss: 0.43326873 Accuracy: 0.82\n",
      "Training - i: 653 Loss: 0.51263434 Accuracy: 0.81\n",
      "Training - i: 654 Loss: 0.42947197 Accuracy: 0.85\n",
      "Training - i: 655 Loss: 0.48528922 Accuracy: 0.8\n",
      "Training - i: 656 Loss: 0.37653267 Accuracy: 0.835\n",
      "Training - i: 657 Loss: 0.50403404 Accuracy: 0.825\n",
      "Training - i: 658 Loss: 0.47450462 Accuracy: 0.815\n",
      "Training - i: 659 Loss: 0.4569706 Accuracy: 0.815\n",
      "Training - i: 660 Loss: 0.43122864 Accuracy: 0.815\n",
      "Training - i: 661 Loss: 0.42606267 Accuracy: 0.825\n",
      "Training - i: 662 Loss: 0.45732415 Accuracy: 0.83\n",
      "Training - i: 663 Loss: 0.43989295 Accuracy: 0.83\n",
      "Training - i: 664 Loss: 0.49583504 Accuracy: 0.805\n",
      "Training - i: 665 Loss: 0.51410896 Accuracy: 0.785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - i: 666 Loss: 0.4864408 Accuracy: 0.82\n",
      "Training - i: 667 Loss: 0.39032993 Accuracy: 0.84\n",
      "Training - i: 668 Loss: 0.49640855 Accuracy: 0.81\n",
      "Training - i: 669 Loss: 0.46478814 Accuracy: 0.795\n",
      "Training - i: 670 Loss: 0.4752145 Accuracy: 0.805\n",
      "Training - i: 671 Loss: 0.42010644 Accuracy: 0.85\n",
      "Training - i: 672 Loss: 0.55807066 Accuracy: 0.785\n",
      "Training - i: 673 Loss: 0.4559298 Accuracy: 0.825\n",
      "Training - i: 674 Loss: 0.4202536 Accuracy: 0.825\n",
      "Training - i: 675 Loss: 0.50721496 Accuracy: 0.78\n",
      "Training - i: 676 Loss: 0.45355576 Accuracy: 0.83\n",
      "Training - i: 677 Loss: 0.4396129 Accuracy: 0.82\n",
      "Training - i: 678 Loss: 0.43884417 Accuracy: 0.825\n",
      "Training - i: 679 Loss: 0.44921395 Accuracy: 0.805\n",
      "Training - i: 680 Loss: 0.40139842 Accuracy: 0.845\n",
      "Training - i: 681 Loss: 0.44949067 Accuracy: 0.825\n",
      "Training - i: 682 Loss: 0.47627786 Accuracy: 0.825\n",
      "Training - i: 683 Loss: 0.44344848 Accuracy: 0.83\n",
      "Training - i: 684 Loss: 0.4092369 Accuracy: 0.85\n",
      "Training - i: 685 Loss: 0.43245193 Accuracy: 0.805\n",
      "Training - i: 686 Loss: 0.47130555 Accuracy: 0.8\n",
      "Training - i: 687 Loss: 0.44968787 Accuracy: 0.815\n",
      "Training - i: 688 Loss: 0.4600825 Accuracy: 0.815\n",
      "Training - i: 689 Loss: 0.424833 Accuracy: 0.84\n",
      "Training - i: 690 Loss: 0.4060153 Accuracy: 0.81\n",
      "Training - i: 691 Loss: 0.42405507 Accuracy: 0.84\n",
      "Training - i: 692 Loss: 0.36199555 Accuracy: 0.87\n",
      "Training - i: 693 Loss: 0.46421865 Accuracy: 0.815\n",
      "Training - i: 694 Loss: 0.42199135 Accuracy: 0.84\n",
      "Training - i: 695 Loss: 0.48451614 Accuracy: 0.795\n",
      "Training - i: 696 Loss: 0.46421856 Accuracy: 0.825\n",
      "Training - i: 697 Loss: 0.48237395 Accuracy: 0.815\n",
      "Training - i: 698 Loss: 0.44854766 Accuracy: 0.825\n",
      "Training - i: 699 Loss: 0.4653442 Accuracy: 0.815\n",
      "Training - i: 700 Loss: 0.4355249 Accuracy: 0.845\n",
      "Training - i: 701 Loss: 0.45729378 Accuracy: 0.795\n",
      "Validation - i: 701  Accuracy: [0.79]\n",
      "Training - i: 702 Loss: 0.4728344 Accuracy: 0.815\n",
      "Training - i: 703 Loss: 0.44205606 Accuracy: 0.83\n",
      "Training - i: 704 Loss: 0.34159005 Accuracy: 0.875\n",
      "Training - i: 705 Loss: 0.34896833 Accuracy: 0.875\n",
      "Training - i: 706 Loss: 0.3861203 Accuracy: 0.86\n",
      "Training - i: 707 Loss: 0.33793104 Accuracy: 0.88\n",
      "Training - i: 708 Loss: 0.41261613 Accuracy: 0.81\n",
      "Training - i: 709 Loss: 0.3603465 Accuracy: 0.895\n",
      "Training - i: 710 Loss: 0.35628417 Accuracy: 0.87\n",
      "Training - i: 711 Loss: 0.4298066 Accuracy: 0.82\n",
      "Training - i: 712 Loss: 0.38729686 Accuracy: 0.875\n",
      "Training - i: 713 Loss: 0.3248538 Accuracy: 0.89\n",
      "Training - i: 714 Loss: 0.39093873 Accuracy: 0.85\n",
      "Training - i: 715 Loss: 0.35152268 Accuracy: 0.855\n",
      "Training - i: 716 Loss: 0.47330275 Accuracy: 0.81\n",
      "Training - i: 717 Loss: 0.44187996 Accuracy: 0.83\n",
      "Training - i: 718 Loss: 0.39421242 Accuracy: 0.86\n",
      "Training - i: 719 Loss: 0.45676327 Accuracy: 0.795\n",
      "Training - i: 720 Loss: 0.4905424 Accuracy: 0.815\n",
      "Training - i: 721 Loss: 0.39744592 Accuracy: 0.85\n",
      "Training - i: 722 Loss: 0.37675172 Accuracy: 0.865\n",
      "Training - i: 723 Loss: 0.3770564 Accuracy: 0.86\n",
      "Training - i: 724 Loss: 0.44228646 Accuracy: 0.825\n",
      "Training - i: 725 Loss: 0.41452816 Accuracy: 0.84\n",
      "Training - i: 726 Loss: 0.5119963 Accuracy: 0.805\n",
      "Training - i: 727 Loss: 0.41101563 Accuracy: 0.84\n",
      "Training - i: 728 Loss: 0.4034763 Accuracy: 0.835\n",
      "Training - i: 729 Loss: 0.44136733 Accuracy: 0.825\n",
      "Training - i: 730 Loss: 0.39862683 Accuracy: 0.815\n",
      "Training - i: 731 Loss: 0.37046227 Accuracy: 0.875\n",
      "Training - i: 732 Loss: 0.45805266 Accuracy: 0.81\n",
      "Training - i: 733 Loss: 0.43596157 Accuracy: 0.82\n",
      "Training - i: 734 Loss: 0.4962424 Accuracy: 0.82\n",
      "Training - i: 735 Loss: 0.3488996 Accuracy: 0.865\n",
      "Training - i: 736 Loss: 0.42686403 Accuracy: 0.83\n",
      "Training - i: 737 Loss: 0.4850927 Accuracy: 0.785\n",
      "Training - i: 738 Loss: 0.36134568 Accuracy: 0.845\n",
      "Training - i: 739 Loss: 0.41392195 Accuracy: 0.82\n",
      "Training - i: 740 Loss: 0.36382 Accuracy: 0.88\n",
      "Training - i: 741 Loss: 0.4531867 Accuracy: 0.835\n",
      "Training - i: 742 Loss: 0.36068976 Accuracy: 0.855\n",
      "Training - i: 743 Loss: 0.45505127 Accuracy: 0.81\n",
      "Training - i: 744 Loss: 0.38921952 Accuracy: 0.85\n",
      "Training - i: 745 Loss: 0.33704978 Accuracy: 0.86\n",
      "Training - i: 746 Loss: 0.39759973 Accuracy: 0.85\n",
      "Training - i: 747 Loss: 0.35464668 Accuracy: 0.86\n",
      "Training - i: 748 Loss: 0.43009844 Accuracy: 0.825\n",
      "Training - i: 749 Loss: 0.38560608 Accuracy: 0.87\n",
      "Training - i: 750 Loss: 0.35413447 Accuracy: 0.88\n",
      "Training - i: 751 Loss: 0.4279808 Accuracy: 0.85\n",
      "Validation - i: 751  Accuracy: [0.825]\n",
      "Training - i: 752 Loss: 0.42102188 Accuracy: 0.815\n",
      "Training - i: 753 Loss: 0.33220786 Accuracy: 0.87\n",
      "Training - i: 754 Loss: 0.36717126 Accuracy: 0.86\n",
      "Training - i: 755 Loss: 0.31224933 Accuracy: 0.905\n",
      "Training - i: 756 Loss: 0.38079354 Accuracy: 0.85\n",
      "Training - i: 757 Loss: 0.3784636 Accuracy: 0.82\n",
      "Training - i: 758 Loss: 0.38597307 Accuracy: 0.84\n",
      "Training - i: 759 Loss: 0.32475448 Accuracy: 0.875\n",
      "Training - i: 760 Loss: 0.39469293 Accuracy: 0.82\n",
      "Training - i: 761 Loss: 0.35453892 Accuracy: 0.89\n",
      "Training - i: 762 Loss: 0.3157035 Accuracy: 0.89\n",
      "Training - i: 763 Loss: 0.34461686 Accuracy: 0.845\n",
      "Training - i: 764 Loss: 0.29906622 Accuracy: 0.87\n",
      "Training - i: 765 Loss: 0.30936483 Accuracy: 0.895\n",
      "Training - i: 766 Loss: 0.35585228 Accuracy: 0.83\n",
      "Training - i: 767 Loss: 0.3586007 Accuracy: 0.84\n",
      "Training - i: 768 Loss: 0.31860816 Accuracy: 0.89\n",
      "Training - i: 769 Loss: 0.31712666 Accuracy: 0.875\n",
      "Training - i: 770 Loss: 0.3282856 Accuracy: 0.86\n",
      "Training - i: 771 Loss: 0.35817978 Accuracy: 0.855\n",
      "Training - i: 772 Loss: 0.33991802 Accuracy: 0.885\n",
      "Training - i: 773 Loss: 0.32867774 Accuracy: 0.835\n",
      "Training - i: 774 Loss: 0.33622873 Accuracy: 0.86\n",
      "Training - i: 775 Loss: 0.32431382 Accuracy: 0.855\n",
      "Training - i: 776 Loss: 0.32859594 Accuracy: 0.835\n",
      "Training - i: 777 Loss: 0.29847157 Accuracy: 0.88\n",
      "Training - i: 778 Loss: 0.29410267 Accuracy: 0.875\n",
      "Training - i: 779 Loss: 0.2993862 Accuracy: 0.89\n",
      "Training - i: 780 Loss: 0.34312835 Accuracy: 0.875\n",
      "Training - i: 781 Loss: 0.25384426 Accuracy: 0.895\n",
      "Training - i: 782 Loss: 0.3347963 Accuracy: 0.88\n",
      "Training - i: 783 Loss: 0.4327891 Accuracy: 0.83\n",
      "Training - i: 784 Loss: 0.40413988 Accuracy: 0.83\n",
      "Training - i: 785 Loss: 0.37362844 Accuracy: 0.825\n",
      "Training - i: 786 Loss: 0.37153575 Accuracy: 0.845\n",
      "Training - i: 787 Loss: 0.4589864 Accuracy: 0.81\n",
      "Training - i: 788 Loss: 0.30494583 Accuracy: 0.88\n",
      "Training - i: 789 Loss: 0.56893104 Accuracy: 0.785\n",
      "Training - i: 790 Loss: 0.2938015 Accuracy: 0.88\n",
      "Training - i: 791 Loss: 0.3383319 Accuracy: 0.865\n",
      "Training - i: 792 Loss: 0.4115434 Accuracy: 0.8\n",
      "Training - i: 793 Loss: 0.3129322 Accuracy: 0.885\n",
      "Training - i: 794 Loss: 0.401939 Accuracy: 0.83\n",
      "Training - i: 795 Loss: 0.3395557 Accuracy: 0.85\n",
      "Training - i: 796 Loss: 0.3813711 Accuracy: 0.91\n",
      "Training - i: 797 Loss: 0.3020351 Accuracy: 0.89\n",
      "Training - i: 798 Loss: 0.29311198 Accuracy: 0.89\n",
      "Training - i: 799 Loss: 0.33495182 Accuracy: 0.865\n",
      "Training - i: 800 Loss: 0.36196327 Accuracy: 0.85\n",
      "Training - i: 801 Loss: 0.35363114 Accuracy: 0.835\n",
      "Validation - i: 801  Accuracy: [0.825]\n",
      "Training - i: 802 Loss: 0.3835911 Accuracy: 0.855\n",
      "Training - i: 803 Loss: 0.34723595 Accuracy: 0.85\n",
      "Training - i: 804 Loss: 0.32087833 Accuracy: 0.87\n",
      "Training - i: 805 Loss: 0.28083637 Accuracy: 0.88\n",
      "Training - i: 806 Loss: 0.28907675 Accuracy: 0.9\n",
      "Training - i: 807 Loss: 0.33048266 Accuracy: 0.875\n",
      "Training - i: 808 Loss: 0.34323683 Accuracy: 0.89\n",
      "Training - i: 809 Loss: 0.3547699 Accuracy: 0.835\n",
      "Training - i: 810 Loss: 0.3665214 Accuracy: 0.85\n",
      "Training - i: 811 Loss: 0.2804825 Accuracy: 0.89\n",
      "Training - i: 812 Loss: 0.3792659 Accuracy: 0.855\n",
      "Training - i: 813 Loss: 0.32669804 Accuracy: 0.855\n",
      "Training - i: 814 Loss: 0.3159267 Accuracy: 0.88\n",
      "Training - i: 815 Loss: 0.32077327 Accuracy: 0.86\n",
      "Training - i: 816 Loss: 0.34883898 Accuracy: 0.845\n",
      "Training - i: 817 Loss: 0.30628404 Accuracy: 0.91\n",
      "Training - i: 818 Loss: 0.29481354 Accuracy: 0.9\n",
      "Training - i: 819 Loss: 0.2864278 Accuracy: 0.915\n",
      "Training - i: 820 Loss: 0.3730751 Accuracy: 0.84\n",
      "Training - i: 821 Loss: 0.29822862 Accuracy: 0.895\n",
      "Training - i: 822 Loss: 0.36689967 Accuracy: 0.84\n",
      "Training - i: 823 Loss: 0.3249636 Accuracy: 0.86\n",
      "Training - i: 824 Loss: 0.38733658 Accuracy: 0.83\n",
      "Training - i: 825 Loss: 0.3676587 Accuracy: 0.86\n",
      "Training - i: 826 Loss: 0.38200447 Accuracy: 0.84\n",
      "Training - i: 827 Loss: 0.3268082 Accuracy: 0.86\n",
      "Training - i: 828 Loss: 0.30636978 Accuracy: 0.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - i: 829 Loss: 0.34313118 Accuracy: 0.855\n",
      "Training - i: 830 Loss: 0.38761795 Accuracy: 0.83\n",
      "Training - i: 831 Loss: 0.32912904 Accuracy: 0.87\n",
      "Training - i: 832 Loss: 0.32376304 Accuracy: 0.865\n",
      "Training - i: 833 Loss: 0.2705047 Accuracy: 0.915\n",
      "Training - i: 834 Loss: 0.29360694 Accuracy: 0.88\n",
      "Training - i: 835 Loss: 0.33472466 Accuracy: 0.83\n",
      "Training - i: 836 Loss: 0.29238814 Accuracy: 0.86\n",
      "Training - i: 837 Loss: 0.25769413 Accuracy: 0.89\n",
      "Training - i: 838 Loss: 0.36346132 Accuracy: 0.88\n",
      "Training - i: 839 Loss: 0.32164112 Accuracy: 0.87\n",
      "Training - i: 840 Loss: 0.25883842 Accuracy: 0.895\n",
      "Training - i: 841 Loss: 0.25349367 Accuracy: 0.91\n",
      "Training - i: 842 Loss: 0.33130932 Accuracy: 0.865\n",
      "Training - i: 843 Loss: 0.24581437 Accuracy: 0.92\n",
      "Training - i: 844 Loss: 0.25387442 Accuracy: 0.92\n",
      "Training - i: 845 Loss: 0.27298042 Accuracy: 0.875\n",
      "Training - i: 846 Loss: 0.32292306 Accuracy: 0.845\n",
      "Training - i: 847 Loss: 0.26923004 Accuracy: 0.905\n",
      "Training - i: 848 Loss: 0.26887217 Accuracy: 0.915\n",
      "Training - i: 849 Loss: 0.33665884 Accuracy: 0.865\n",
      "Training - i: 850 Loss: 0.23024468 Accuracy: 0.905\n",
      "Training - i: 851 Loss: 0.31862316 Accuracy: 0.83\n",
      "Validation - i: 851  Accuracy: [0.885]\n",
      "Training - i: 852 Loss: 0.22514027 Accuracy: 0.935\n",
      "Training - i: 853 Loss: 0.25766656 Accuracy: 0.915\n",
      "Training - i: 854 Loss: 0.28026226 Accuracy: 0.895\n",
      "Training - i: 855 Loss: 0.24390563 Accuracy: 0.91\n",
      "Training - i: 856 Loss: 0.28293124 Accuracy: 0.885\n",
      "Training - i: 857 Loss: 0.3058708 Accuracy: 0.885\n",
      "Training - i: 858 Loss: 0.25987002 Accuracy: 0.9\n",
      "Training - i: 859 Loss: 0.31376582 Accuracy: 0.85\n",
      "Training - i: 860 Loss: 0.26721245 Accuracy: 0.905\n",
      "Training - i: 861 Loss: 0.370338 Accuracy: 0.855\n",
      "Training - i: 862 Loss: 0.25658298 Accuracy: 0.915\n",
      "Training - i: 863 Loss: 0.3847265 Accuracy: 0.85\n",
      "Training - i: 864 Loss: 0.32369927 Accuracy: 0.87\n",
      "Training - i: 865 Loss: 0.3865862 Accuracy: 0.83\n",
      "Training - i: 866 Loss: 0.26436314 Accuracy: 0.9\n",
      "Training - i: 867 Loss: 0.29872856 Accuracy: 0.88\n",
      "Training - i: 868 Loss: 0.2898922 Accuracy: 0.88\n",
      "Training - i: 869 Loss: 0.2500677 Accuracy: 0.885\n",
      "Training - i: 870 Loss: 0.2644137 Accuracy: 0.915\n",
      "Training - i: 871 Loss: 0.28233823 Accuracy: 0.9\n",
      "Training - i: 872 Loss: 0.2563234 Accuracy: 0.915\n",
      "Training - i: 873 Loss: 0.34535113 Accuracy: 0.865\n",
      "Training - i: 874 Loss: 0.25697213 Accuracy: 0.885\n",
      "Training - i: 875 Loss: 0.2564321 Accuracy: 0.915\n",
      "Training - i: 876 Loss: 0.32934147 Accuracy: 0.86\n",
      "Training - i: 877 Loss: 0.31739053 Accuracy: 0.87\n",
      "Training - i: 878 Loss: 0.31013235 Accuracy: 0.885\n",
      "Training - i: 879 Loss: 0.29018348 Accuracy: 0.88\n",
      "Training - i: 880 Loss: 0.31468058 Accuracy: 0.875\n",
      "Training - i: 881 Loss: 0.27519235 Accuracy: 0.915\n",
      "Training - i: 882 Loss: 0.30751583 Accuracy: 0.86\n",
      "Training - i: 883 Loss: 0.23590283 Accuracy: 0.905\n",
      "Training - i: 884 Loss: 0.2707391 Accuracy: 0.89\n",
      "Training - i: 885 Loss: 0.24630828 Accuracy: 0.915\n",
      "Training - i: 886 Loss: 0.2818313 Accuracy: 0.9\n",
      "Training - i: 887 Loss: 0.23182452 Accuracy: 0.915\n",
      "Training - i: 888 Loss: 0.24748658 Accuracy: 0.88\n",
      "Training - i: 889 Loss: 0.17167625 Accuracy: 0.95\n",
      "Training - i: 890 Loss: 0.24679726 Accuracy: 0.9\n",
      "Training - i: 891 Loss: 0.22461693 Accuracy: 0.9\n",
      "Training - i: 892 Loss: 0.22342426 Accuracy: 0.93\n",
      "Training - i: 893 Loss: 0.23769146 Accuracy: 0.91\n",
      "Training - i: 894 Loss: 0.28980634 Accuracy: 0.86\n",
      "Training - i: 895 Loss: 0.18477759 Accuracy: 0.93\n",
      "Training - i: 896 Loss: 0.28919393 Accuracy: 0.88\n",
      "Training - i: 897 Loss: 0.22894473 Accuracy: 0.915\n",
      "Training - i: 898 Loss: 0.20741104 Accuracy: 0.93\n",
      "Training - i: 899 Loss: 0.2524797 Accuracy: 0.9\n",
      "Training - i: 900 Loss: 0.2799276 Accuracy: 0.895\n",
      "Training - i: 901 Loss: 0.24560085 Accuracy: 0.905\n",
      "Validation - i: 901  Accuracy: [0.86]\n",
      "Training - i: 902 Loss: 0.2905233 Accuracy: 0.88\n",
      "Training - i: 903 Loss: 0.22445413 Accuracy: 0.9\n",
      "Training - i: 904 Loss: 0.26676178 Accuracy: 0.85\n",
      "Training - i: 905 Loss: 0.2711506 Accuracy: 0.88\n",
      "Training - i: 906 Loss: 0.30790645 Accuracy: 0.875\n",
      "Training - i: 907 Loss: 0.33154082 Accuracy: 0.875\n",
      "Training - i: 908 Loss: 0.2532141 Accuracy: 0.895\n",
      "Training - i: 909 Loss: 0.31580827 Accuracy: 0.865\n",
      "Training - i: 910 Loss: 0.21323219 Accuracy: 0.935\n",
      "Training - i: 911 Loss: 0.30725345 Accuracy: 0.865\n",
      "Training - i: 912 Loss: 0.2648429 Accuracy: 0.89\n",
      "Training - i: 913 Loss: 0.26059777 Accuracy: 0.9\n",
      "Training - i: 914 Loss: 0.2136838 Accuracy: 0.905\n",
      "Training - i: 915 Loss: 0.22694263 Accuracy: 0.915\n",
      "Training - i: 916 Loss: 0.2630604 Accuracy: 0.9\n",
      "Training - i: 917 Loss: 0.26755255 Accuracy: 0.885\n",
      "Training - i: 918 Loss: 0.18608849 Accuracy: 0.93\n",
      "Training - i: 919 Loss: 0.2791357 Accuracy: 0.885\n",
      "Training - i: 920 Loss: 0.24080081 Accuracy: 0.9\n",
      "Training - i: 921 Loss: 0.23801109 Accuracy: 0.905\n",
      "Training - i: 922 Loss: 0.2055919 Accuracy: 0.925\n",
      "Training - i: 923 Loss: 0.3087453 Accuracy: 0.86\n",
      "Training - i: 924 Loss: 0.18465042 Accuracy: 0.95\n",
      "Training - i: 925 Loss: 0.19945343 Accuracy: 0.935\n",
      "Training - i: 926 Loss: 0.2394997 Accuracy: 0.915\n",
      "Training - i: 927 Loss: 0.23811162 Accuracy: 0.935\n",
      "Training - i: 928 Loss: 0.25858223 Accuracy: 0.885\n",
      "Training - i: 929 Loss: 0.20936434 Accuracy: 0.915\n",
      "Training - i: 930 Loss: 0.2650758 Accuracy: 0.88\n",
      "Training - i: 931 Loss: 0.28898942 Accuracy: 0.87\n",
      "Training - i: 932 Loss: 0.20384549 Accuracy: 0.945\n",
      "Training - i: 933 Loss: 0.21277419 Accuracy: 0.91\n",
      "Training - i: 934 Loss: 0.23531912 Accuracy: 0.92\n",
      "Training - i: 935 Loss: 0.24509415 Accuracy: 0.88\n",
      "Training - i: 936 Loss: 0.29382333 Accuracy: 0.895\n",
      "Training - i: 937 Loss: 0.22655925 Accuracy: 0.93\n",
      "Training - i: 938 Loss: 0.27691492 Accuracy: 0.915\n",
      "Training - i: 939 Loss: 0.21048282 Accuracy: 0.945\n",
      "Training - i: 940 Loss: 0.22735275 Accuracy: 0.91\n",
      "Training - i: 941 Loss: 0.25345704 Accuracy: 0.87\n",
      "Training - i: 942 Loss: 0.26974782 Accuracy: 0.87\n",
      "Training - i: 943 Loss: 0.24991392 Accuracy: 0.885\n",
      "Training - i: 944 Loss: 0.28309095 Accuracy: 0.9\n",
      "Training - i: 945 Loss: 0.20712131 Accuracy: 0.925\n",
      "Training - i: 946 Loss: 0.29485402 Accuracy: 0.865\n",
      "Training - i: 947 Loss: 0.29612622 Accuracy: 0.875\n",
      "Training - i: 948 Loss: 0.2802732 Accuracy: 0.88\n",
      "Training - i: 949 Loss: 0.28942403 Accuracy: 0.875\n",
      "Training - i: 950 Loss: 0.28356293 Accuracy: 0.855\n",
      "Training - i: 951 Loss: 0.38717 Accuracy: 0.85\n",
      "Validation - i: 951  Accuracy: [0.845]\n",
      "Training - i: 952 Loss: 0.29519153 Accuracy: 0.9\n",
      "Training - i: 953 Loss: 0.3332941 Accuracy: 0.88\n",
      "Training - i: 954 Loss: 0.35379985 Accuracy: 0.845\n",
      "Training - i: 955 Loss: 0.3378632 Accuracy: 0.855\n",
      "Training - i: 956 Loss: 0.22745849 Accuracy: 0.915\n",
      "Training - i: 957 Loss: 0.22375728 Accuracy: 0.88\n",
      "Training - i: 958 Loss: 0.25114858 Accuracy: 0.905\n",
      "Training - i: 959 Loss: 0.34619376 Accuracy: 0.865\n",
      "Training - i: 960 Loss: 0.2511749 Accuracy: 0.89\n",
      "Training - i: 961 Loss: 0.25647688 Accuracy: 0.89\n",
      "Training - i: 962 Loss: 0.25167933 Accuracy: 0.87\n",
      "Training - i: 963 Loss: 0.29852936 Accuracy: 0.865\n",
      "Training - i: 964 Loss: 0.30376214 Accuracy: 0.865\n",
      "Training - i: 965 Loss: 0.28617093 Accuracy: 0.885\n",
      "Training - i: 966 Loss: 0.20703344 Accuracy: 0.91\n",
      "Training - i: 967 Loss: 0.3238934 Accuracy: 0.855\n",
      "Training - i: 968 Loss: 0.36626017 Accuracy: 0.865\n",
      "Training - i: 969 Loss: 0.35124016 Accuracy: 0.835\n",
      "Training - i: 970 Loss: 0.2463134 Accuracy: 0.9\n",
      "Training - i: 971 Loss: 0.23509823 Accuracy: 0.9\n",
      "Training - i: 972 Loss: 0.31091374 Accuracy: 0.855\n",
      "Training - i: 973 Loss: 0.25736392 Accuracy: 0.885\n",
      "Training - i: 974 Loss: 0.34393224 Accuracy: 0.845\n",
      "Training - i: 975 Loss: 0.23502137 Accuracy: 0.925\n",
      "Training - i: 976 Loss: 0.20839515 Accuracy: 0.92\n",
      "Training - i: 977 Loss: 0.2303846 Accuracy: 0.89\n",
      "Training - i: 978 Loss: 0.33503228 Accuracy: 0.86\n",
      "Training - i: 979 Loss: 0.29500094 Accuracy: 0.875\n",
      "Training - i: 980 Loss: 0.28187606 Accuracy: 0.88\n",
      "Training - i: 981 Loss: 0.31284007 Accuracy: 0.865\n",
      "Training - i: 982 Loss: 0.2220068 Accuracy: 0.91\n",
      "Training - i: 983 Loss: 0.3072065 Accuracy: 0.865\n",
      "Training - i: 984 Loss: 0.25722274 Accuracy: 0.925\n",
      "Training - i: 985 Loss: 0.3023531 Accuracy: 0.89\n",
      "Training - i: 986 Loss: 0.28755963 Accuracy: 0.885\n",
      "Training - i: 987 Loss: 0.30853805 Accuracy: 0.875\n",
      "Training - i: 988 Loss: 0.22599691 Accuracy: 0.915\n",
      "Training - i: 989 Loss: 0.27730292 Accuracy: 0.885\n",
      "Training - i: 990 Loss: 0.2747259 Accuracy: 0.885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - i: 991 Loss: 0.3041543 Accuracy: 0.895\n",
      "Training - i: 992 Loss: 0.26848924 Accuracy: 0.885\n",
      "Training - i: 993 Loss: 0.19362754 Accuracy: 0.915\n",
      "Training - i: 994 Loss: 0.27810135 Accuracy: 0.895\n",
      "Training - i: 995 Loss: 0.24862891 Accuracy: 0.915\n",
      "Training - i: 996 Loss: 0.28411826 Accuracy: 0.89\n",
      "Training - i: 997 Loss: 0.26206985 Accuracy: 0.895\n",
      "Training - i: 998 Loss: 0.29949534 Accuracy: 0.87\n",
      "Training - i: 999 Loss: 0.24893284 Accuracy: 0.895\n",
      "Training - i: 1000 Loss: 0.25014627 Accuracy: 0.89\n",
      "Training - i: 1001 Loss: 0.21285117 Accuracy: 0.915\n",
      "Validation - i: 1001  Accuracy: [0.85]\n",
      "Training - i: 1002 Loss: 0.28216445 Accuracy: 0.895\n",
      "Training - i: 1003 Loss: 0.28753197 Accuracy: 0.9\n",
      "Training - i: 1004 Loss: 0.28313103 Accuracy: 0.91\n",
      "Training - i: 1005 Loss: 0.26429117 Accuracy: 0.905\n",
      "Training - i: 1006 Loss: 0.26068366 Accuracy: 0.915\n",
      "Training - i: 1007 Loss: 0.17743433 Accuracy: 0.95\n",
      "Training - i: 1008 Loss: 0.24589592 Accuracy: 0.875\n",
      "Training - i: 1009 Loss: 0.26206073 Accuracy: 0.89\n",
      "Training - i: 1010 Loss: 0.26617435 Accuracy: 0.89\n",
      "Training - i: 1011 Loss: 0.20355445 Accuracy: 0.91\n",
      "Training - i: 1012 Loss: 0.15731329 Accuracy: 0.96\n",
      "Training - i: 1013 Loss: 0.23099189 Accuracy: 0.9\n",
      "Training - i: 1014 Loss: 0.25354302 Accuracy: 0.92\n",
      "Training - i: 1015 Loss: 0.20977362 Accuracy: 0.91\n",
      "Training - i: 1016 Loss: 0.2270208 Accuracy: 0.895\n",
      "Training - i: 1017 Loss: 0.18993694 Accuracy: 0.915\n",
      "Training - i: 1018 Loss: 0.30845618 Accuracy: 0.885\n",
      "Training - i: 1019 Loss: 0.23965283 Accuracy: 0.895\n",
      "Training - i: 1020 Loss: 0.21599495 Accuracy: 0.92\n",
      "Training - i: 1021 Loss: 0.25719061 Accuracy: 0.92\n",
      "Training - i: 1022 Loss: 0.26024413 Accuracy: 0.9\n",
      "Training - i: 1023 Loss: 0.23160583 Accuracy: 0.91\n",
      "Training - i: 1024 Loss: 0.22076844 Accuracy: 0.9\n",
      "Training - i: 1025 Loss: 0.292137 Accuracy: 0.865\n",
      "Training - i: 1026 Loss: 0.21517585 Accuracy: 0.935\n",
      "Training - i: 1027 Loss: 0.2129142 Accuracy: 0.93\n",
      "Training - i: 1028 Loss: 0.2537545 Accuracy: 0.9\n",
      "Training - i: 1029 Loss: 0.25099626 Accuracy: 0.895\n",
      "Training - i: 1030 Loss: 0.20848854 Accuracy: 0.9\n",
      "Training - i: 1031 Loss: 0.22491713 Accuracy: 0.915\n",
      "Training - i: 1032 Loss: 0.17983055 Accuracy: 0.925\n",
      "Training - i: 1033 Loss: 0.1577844 Accuracy: 0.935\n",
      "Training - i: 1034 Loss: 0.22219719 Accuracy: 0.895\n",
      "Training - i: 1035 Loss: 0.17614023 Accuracy: 0.93\n",
      "Training - i: 1036 Loss: 0.17597598 Accuracy: 0.94\n",
      "Training - i: 1037 Loss: 0.1724902 Accuracy: 0.94\n",
      "Training - i: 1038 Loss: 0.19610073 Accuracy: 0.935\n",
      "Training - i: 1039 Loss: 0.24600995 Accuracy: 0.89\n",
      "Training - i: 1040 Loss: 0.21330647 Accuracy: 0.92\n",
      "Training - i: 1041 Loss: 0.19499637 Accuracy: 0.945\n",
      "Training - i: 1042 Loss: 0.1790952 Accuracy: 0.93\n",
      "Training - i: 1043 Loss: 0.23725225 Accuracy: 0.895\n",
      "Training - i: 1044 Loss: 0.16971992 Accuracy: 0.935\n",
      "Training - i: 1045 Loss: 0.17817082 Accuracy: 0.935\n",
      "Training - i: 1046 Loss: 0.21183926 Accuracy: 0.925\n",
      "Training - i: 1047 Loss: 0.19048485 Accuracy: 0.93\n",
      "Training - i: 1048 Loss: 0.25359872 Accuracy: 0.905\n",
      "Training - i: 1049 Loss: 0.12332415 Accuracy: 0.955\n",
      "Training - i: 1050 Loss: 0.22815989 Accuracy: 0.895\n",
      "Training - i: 1051 Loss: 0.174888 Accuracy: 0.92\n",
      "Validation - i: 1051  Accuracy: [0.88]\n",
      "Training - i: 1052 Loss: 0.17527984 Accuracy: 0.95\n",
      "Training - i: 1053 Loss: 0.19808573 Accuracy: 0.92\n",
      "Training - i: 1054 Loss: 0.21232454 Accuracy: 0.905\n",
      "Training - i: 1055 Loss: 0.14987601 Accuracy: 0.96\n",
      "Training - i: 1056 Loss: 0.1932732 Accuracy: 0.91\n",
      "Training - i: 1057 Loss: 0.20231411 Accuracy: 0.925\n",
      "Training - i: 1058 Loss: 0.16140974 Accuracy: 0.945\n",
      "Training - i: 1059 Loss: 0.22434682 Accuracy: 0.91\n",
      "Training - i: 1060 Loss: 0.1808845 Accuracy: 0.935\n",
      "Training - i: 1061 Loss: 0.1503722 Accuracy: 0.945\n",
      "Training - i: 1062 Loss: 0.16538635 Accuracy: 0.945\n",
      "Training - i: 1063 Loss: 0.16084188 Accuracy: 0.95\n",
      "Training - i: 1064 Loss: 0.18116155 Accuracy: 0.905\n",
      "Training - i: 1065 Loss: 0.13700372 Accuracy: 0.95\n",
      "Training - i: 1066 Loss: 0.19470884 Accuracy: 0.91\n",
      "Training - i: 1067 Loss: 0.19777597 Accuracy: 0.93\n",
      "Training - i: 1068 Loss: 0.14011286 Accuracy: 0.955\n",
      "Training - i: 1069 Loss: 0.14724499 Accuracy: 0.97\n",
      "Training - i: 1070 Loss: 0.1661251 Accuracy: 0.935\n",
      "Training - i: 1071 Loss: 0.17793746 Accuracy: 0.915\n",
      "Training - i: 1072 Loss: 0.16106063 Accuracy: 0.925\n",
      "Training - i: 1073 Loss: 0.12517448 Accuracy: 0.965\n",
      "Training - i: 1074 Loss: 0.19540855 Accuracy: 0.915\n",
      "Training - i: 1075 Loss: 0.20091766 Accuracy: 0.955\n",
      "Training - i: 1076 Loss: 0.18242103 Accuracy: 0.935\n",
      "Training - i: 1077 Loss: 0.22443902 Accuracy: 0.905\n",
      "Training - i: 1078 Loss: 0.13818617 Accuracy: 0.95\n",
      "Training - i: 1079 Loss: 0.20398693 Accuracy: 0.905\n",
      "Training - i: 1080 Loss: 0.12952708 Accuracy: 0.975\n",
      "Training - i: 1081 Loss: 0.20102686 Accuracy: 0.915\n",
      "Training - i: 1082 Loss: 0.1843579 Accuracy: 0.94\n",
      "Training - i: 1083 Loss: 0.14531893 Accuracy: 0.94\n",
      "Training - i: 1084 Loss: 0.12971458 Accuracy: 0.96\n",
      "Training - i: 1085 Loss: 0.14107032 Accuracy: 0.96\n",
      "Training - i: 1086 Loss: 0.18891293 Accuracy: 0.915\n",
      "Training - i: 1087 Loss: 0.14584076 Accuracy: 0.955\n",
      "Training - i: 1088 Loss: 0.18313617 Accuracy: 0.93\n",
      "Training - i: 1089 Loss: 0.17684826 Accuracy: 0.92\n",
      "Training - i: 1090 Loss: 0.14786263 Accuracy: 0.95\n",
      "Training - i: 1091 Loss: 0.17367733 Accuracy: 0.925\n",
      "Training - i: 1092 Loss: 0.16009612 Accuracy: 0.945\n",
      "Training - i: 1093 Loss: 0.1696302 Accuracy: 0.955\n",
      "Training - i: 1094 Loss: 0.20245239 Accuracy: 0.91\n",
      "Training - i: 1095 Loss: 0.22793727 Accuracy: 0.895\n",
      "Training - i: 1096 Loss: 0.16169022 Accuracy: 0.915\n",
      "Training - i: 1097 Loss: 0.2518872 Accuracy: 0.89\n",
      "Training - i: 1098 Loss: 0.159378 Accuracy: 0.95\n",
      "Training - i: 1099 Loss: 0.1690911 Accuracy: 0.935\n",
      "Training - i: 1100 Loss: 0.13172495 Accuracy: 0.96\n",
      "Training - i: 1101 Loss: 0.20649357 Accuracy: 0.93\n",
      "Validation - i: 1101  Accuracy: [0.87]\n",
      "Training - i: 1102 Loss: 0.20486633 Accuracy: 0.905\n",
      "Training - i: 1103 Loss: 0.25309592 Accuracy: 0.895\n",
      "Training - i: 1104 Loss: 0.1672868 Accuracy: 0.94\n",
      "Training - i: 1105 Loss: 0.18171659 Accuracy: 0.91\n",
      "Training - i: 1106 Loss: 0.20999968 Accuracy: 0.92\n",
      "Training - i: 1107 Loss: 0.166325 Accuracy: 0.945\n",
      "Training - i: 1108 Loss: 0.16504097 Accuracy: 0.94\n",
      "Training - i: 1109 Loss: 0.15718816 Accuracy: 0.945\n",
      "Training - i: 1110 Loss: 0.26862496 Accuracy: 0.865\n",
      "Training - i: 1111 Loss: 0.19656616 Accuracy: 0.92\n",
      "Training - i: 1112 Loss: 0.18528202 Accuracy: 0.93\n",
      "Training - i: 1113 Loss: 0.19361784 Accuracy: 0.91\n",
      "Training - i: 1114 Loss: 0.16325486 Accuracy: 0.905\n",
      "Training - i: 1115 Loss: 0.15209302 Accuracy: 0.955\n",
      "Training - i: 1116 Loss: 0.20110482 Accuracy: 0.925\n",
      "Training - i: 1117 Loss: 0.20092621 Accuracy: 0.925\n",
      "Training - i: 1118 Loss: 0.14184229 Accuracy: 0.95\n",
      "Training - i: 1119 Loss: 0.17885315 Accuracy: 0.925\n",
      "Training - i: 1120 Loss: 0.14545034 Accuracy: 0.935\n",
      "Training - i: 1121 Loss: 0.20254865 Accuracy: 0.925\n",
      "Training - i: 1122 Loss: 0.1860455 Accuracy: 0.925\n",
      "Training - i: 1123 Loss: 0.15028894 Accuracy: 0.94\n",
      "Training - i: 1124 Loss: 0.1682424 Accuracy: 0.94\n",
      "Training - i: 1125 Loss: 0.2114037 Accuracy: 0.905\n",
      "Training - i: 1126 Loss: 0.17528614 Accuracy: 0.95\n",
      "Training - i: 1127 Loss: 0.19507039 Accuracy: 0.925\n",
      "Training - i: 1128 Loss: 0.19681554 Accuracy: 0.905\n",
      "Training - i: 1129 Loss: 0.15176956 Accuracy: 0.945\n",
      "Training - i: 1130 Loss: 0.2241312 Accuracy: 0.895\n",
      "Training - i: 1131 Loss: 0.19910696 Accuracy: 0.91\n",
      "Training - i: 1132 Loss: 0.19660091 Accuracy: 0.92\n",
      "Training - i: 1133 Loss: 0.16199012 Accuracy: 0.965\n",
      "Training - i: 1134 Loss: 0.16587189 Accuracy: 0.945\n",
      "Training - i: 1135 Loss: 0.1898882 Accuracy: 0.91\n",
      "Training - i: 1136 Loss: 0.1737482 Accuracy: 0.935\n",
      "Training - i: 1137 Loss: 0.23696694 Accuracy: 0.905\n",
      "Training - i: 1138 Loss: 0.172935 Accuracy: 0.955\n",
      "Training - i: 1139 Loss: 0.20367433 Accuracy: 0.895\n",
      "Training - i: 1140 Loss: 0.18896116 Accuracy: 0.93\n",
      "Training - i: 1141 Loss: 0.235984 Accuracy: 0.87\n",
      "Training - i: 1142 Loss: 0.12997742 Accuracy: 0.955\n",
      "Training - i: 1143 Loss: 0.20892425 Accuracy: 0.925\n",
      "Training - i: 1144 Loss: 0.13722694 Accuracy: 0.945\n",
      "Training - i: 1145 Loss: 0.14695197 Accuracy: 0.94\n",
      "Training - i: 1146 Loss: 0.23443656 Accuracy: 0.895\n",
      "Training - i: 1147 Loss: 0.16590457 Accuracy: 0.94\n",
      "Training - i: 1148 Loss: 0.1835938 Accuracy: 0.945\n",
      "Training - i: 1149 Loss: 0.2111331 Accuracy: 0.94\n",
      "Training - i: 1150 Loss: 0.21972375 Accuracy: 0.9\n",
      "Training - i: 1151 Loss: 0.1765816 Accuracy: 0.925\n",
      "Validation - i: 1151  Accuracy: [0.845]\n",
      "Training - i: 1152 Loss: 0.15195078 Accuracy: 0.935\n",
      "Training - i: 1153 Loss: 0.22010075 Accuracy: 0.93\n",
      "Training - i: 1154 Loss: 0.17318396 Accuracy: 0.95\n",
      "Training - i: 1155 Loss: 0.18499911 Accuracy: 0.915\n",
      "Training - i: 1156 Loss: 0.19671738 Accuracy: 0.905\n",
      "Training - i: 1157 Loss: 0.15345046 Accuracy: 0.93\n",
      "Training - i: 1158 Loss: 0.1649617 Accuracy: 0.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - i: 1159 Loss: 0.22250135 Accuracy: 0.93\n",
      "Training - i: 1160 Loss: 0.20861065 Accuracy: 0.92\n",
      "Training - i: 1161 Loss: 0.18330069 Accuracy: 0.92\n",
      "Training - i: 1162 Loss: 0.19718082 Accuracy: 0.9\n",
      "Training - i: 1163 Loss: 0.19346733 Accuracy: 0.935\n",
      "Training - i: 1164 Loss: 0.19901298 Accuracy: 0.94\n",
      "Training - i: 1165 Loss: 0.11958072 Accuracy: 0.97\n",
      "Training - i: 1166 Loss: 0.14689584 Accuracy: 0.945\n",
      "Training - i: 1167 Loss: 0.15175137 Accuracy: 0.95\n",
      "Training - i: 1168 Loss: 0.15534917 Accuracy: 0.935\n",
      "Training - i: 1169 Loss: 0.15284112 Accuracy: 0.935\n",
      "Training - i: 1170 Loss: 0.144163 Accuracy: 0.965\n",
      "Training - i: 1171 Loss: 0.20074831 Accuracy: 0.92\n",
      "Training - i: 1172 Loss: 0.1748323 Accuracy: 0.95\n",
      "Training - i: 1173 Loss: 0.11964638 Accuracy: 0.955\n",
      "Training - i: 1174 Loss: 0.16733322 Accuracy: 0.925\n",
      "Training - i: 1175 Loss: 0.19177073 Accuracy: 0.915\n",
      "Training - i: 1176 Loss: 0.16656326 Accuracy: 0.92\n",
      "Training - i: 1177 Loss: 0.1955994 Accuracy: 0.905\n",
      "Training - i: 1178 Loss: 0.16442478 Accuracy: 0.935\n",
      "Training - i: 1179 Loss: 0.18927197 Accuracy: 0.925\n",
      "Training - i: 1180 Loss: 0.17960984 Accuracy: 0.915\n",
      "Training - i: 1181 Loss: 0.21600841 Accuracy: 0.895\n",
      "Training - i: 1182 Loss: 0.1213104 Accuracy: 0.95\n",
      "Training - i: 1183 Loss: 0.15235844 Accuracy: 0.925\n",
      "Training - i: 1184 Loss: 0.2436797 Accuracy: 0.88\n",
      "Training - i: 1185 Loss: 0.238675 Accuracy: 0.87\n",
      "Training - i: 1186 Loss: 0.21478353 Accuracy: 0.89\n",
      "Training - i: 1187 Loss: 0.14337684 Accuracy: 0.955\n",
      "Training - i: 1188 Loss: 0.22899374 Accuracy: 0.905\n",
      "Training - i: 1189 Loss: 0.13051243 Accuracy: 0.96\n",
      "Training - i: 1190 Loss: 0.19395156 Accuracy: 0.9\n",
      "Training - i: 1191 Loss: 0.16028982 Accuracy: 0.93\n",
      "Training - i: 1192 Loss: 0.18211196 Accuracy: 0.92\n",
      "Training - i: 1193 Loss: 0.16374603 Accuracy: 0.935\n",
      "Training - i: 1194 Loss: 0.19830464 Accuracy: 0.915\n",
      "Training - i: 1195 Loss: 0.26506466 Accuracy: 0.86\n",
      "Training - i: 1196 Loss: 0.16032912 Accuracy: 0.935\n",
      "Training - i: 1197 Loss: 0.18866703 Accuracy: 0.935\n",
      "Training - i: 1198 Loss: 0.16552669 Accuracy: 0.915\n",
      "Training - i: 1199 Loss: 0.21959816 Accuracy: 0.9\n",
      "Training - i: 1200 Loss: 0.24479532 Accuracy: 0.9\n",
      "Training - i: 1201 Loss: 0.22010761 Accuracy: 0.915\n",
      "Validation - i: 1201  Accuracy: [0.855]\n",
      "Training - i: 1202 Loss: 0.16419943 Accuracy: 0.925\n",
      "Training - i: 1203 Loss: 0.12536383 Accuracy: 0.955\n",
      "Training - i: 1204 Loss: 0.24771342 Accuracy: 0.89\n",
      "Training - i: 1205 Loss: 0.1469743 Accuracy: 0.93\n",
      "Training - i: 1206 Loss: 0.1504913 Accuracy: 0.935\n",
      "Training - i: 1207 Loss: 0.25401345 Accuracy: 0.88\n",
      "Training - i: 1208 Loss: 0.18369415 Accuracy: 0.93\n",
      "Training - i: 1209 Loss: 0.19548346 Accuracy: 0.93\n",
      "Training - i: 1210 Loss: 0.17537947 Accuracy: 0.935\n",
      "Training - i: 1211 Loss: 0.19414559 Accuracy: 0.92\n",
      "Training - i: 1212 Loss: 0.24457413 Accuracy: 0.92\n",
      "Training - i: 1213 Loss: 0.18342821 Accuracy: 0.945\n",
      "Training - i: 1214 Loss: 0.18136604 Accuracy: 0.93\n",
      "Training - i: 1215 Loss: 0.15717311 Accuracy: 0.93\n",
      "Training - i: 1216 Loss: 0.14151698 Accuracy: 0.945\n",
      "Training - i: 1217 Loss: 0.19033527 Accuracy: 0.91\n",
      "Training - i: 1218 Loss: 0.18799426 Accuracy: 0.91\n",
      "Training - i: 1219 Loss: 0.18845831 Accuracy: 0.9\n",
      "Training - i: 1220 Loss: 0.16173981 Accuracy: 0.93\n",
      "Training - i: 1221 Loss: 0.17399257 Accuracy: 0.91\n",
      "Training - i: 1222 Loss: 0.22385979 Accuracy: 0.885\n",
      "Training - i: 1223 Loss: 0.17787945 Accuracy: 0.93\n",
      "Training - i: 1224 Loss: 0.1808479 Accuracy: 0.925\n",
      "Training - i: 1225 Loss: 0.22288159 Accuracy: 0.905\n",
      "Training - i: 1226 Loss: 0.19765034 Accuracy: 0.92\n",
      "Training - i: 1227 Loss: 0.15574187 Accuracy: 0.925\n",
      "Training - i: 1228 Loss: 0.13104892 Accuracy: 0.95\n",
      "Training - i: 1229 Loss: 0.18030664 Accuracy: 0.92\n",
      "Training - i: 1230 Loss: 0.19837636 Accuracy: 0.92\n",
      "Training - i: 1231 Loss: 0.21266803 Accuracy: 0.9\n",
      "Training - i: 1232 Loss: 0.14877054 Accuracy: 0.93\n",
      "Training - i: 1233 Loss: 0.23711655 Accuracy: 0.895\n",
      "Training - i: 1234 Loss: 0.20487253 Accuracy: 0.915\n",
      "Training - i: 1235 Loss: 0.16794226 Accuracy: 0.945\n",
      "Training - i: 1236 Loss: 0.17987372 Accuracy: 0.935\n",
      "Training - i: 1237 Loss: 0.16195595 Accuracy: 0.935\n",
      "Training - i: 1238 Loss: 0.22017479 Accuracy: 0.89\n",
      "Training - i: 1239 Loss: 0.16297872 Accuracy: 0.935\n",
      "Training - i: 1240 Loss: 0.16436386 Accuracy: 0.935\n",
      "Training - i: 1241 Loss: 0.14980008 Accuracy: 0.93\n",
      "Training - i: 1242 Loss: 0.2227848 Accuracy: 0.91\n",
      "Training - i: 1243 Loss: 0.16567843 Accuracy: 0.94\n",
      "Training - i: 1244 Loss: 0.13976026 Accuracy: 0.945\n",
      "Training - i: 1245 Loss: 0.13615993 Accuracy: 0.95\n",
      "Training - i: 1246 Loss: 0.20323788 Accuracy: 0.91\n",
      "Training - i: 1247 Loss: 0.23034741 Accuracy: 0.905\n",
      "Training - i: 1248 Loss: 0.18647571 Accuracy: 0.935\n",
      "Training - i: 1249 Loss: 0.15277252 Accuracy: 0.925\n",
      "Training - i: 1250 Loss: 0.1724339 Accuracy: 0.925\n",
      "Training - i: 1251 Loss: 0.15955065 Accuracy: 0.935\n",
      "Validation - i: 1251  Accuracy: [0.9]\n",
      "Training - i: 1252 Loss: 0.13515471 Accuracy: 0.955\n",
      "Training - i: 1253 Loss: 0.14181346 Accuracy: 0.94\n",
      "Training - i: 1254 Loss: 0.15324683 Accuracy: 0.935\n",
      "Training - i: 1255 Loss: 0.14268878 Accuracy: 0.955\n",
      "Training - i: 1256 Loss: 0.14254752 Accuracy: 0.94\n",
      "Training - i: 1257 Loss: 0.25678185 Accuracy: 0.89\n",
      "Training - i: 1258 Loss: 0.18477432 Accuracy: 0.925\n",
      "Training - i: 1259 Loss: 0.19508648 Accuracy: 0.9\n",
      "Training - i: 1260 Loss: 0.19170024 Accuracy: 0.915\n",
      "Training - i: 1261 Loss: 0.16162373 Accuracy: 0.945\n",
      "Training - i: 1262 Loss: 0.14054637 Accuracy: 0.95\n",
      "Training - i: 1263 Loss: 0.17640144 Accuracy: 0.925\n",
      "Training - i: 1264 Loss: 0.18407278 Accuracy: 0.925\n",
      "Training - i: 1265 Loss: 0.12608775 Accuracy: 0.97\n",
      "Training - i: 1266 Loss: 0.16765553 Accuracy: 0.925\n",
      "Training - i: 1267 Loss: 0.18265077 Accuracy: 0.915\n",
      "Training - i: 1268 Loss: 0.15467557 Accuracy: 0.93\n",
      "Training - i: 1269 Loss: 0.17650296 Accuracy: 0.93\n",
      "Training - i: 1270 Loss: 0.16031061 Accuracy: 0.945\n",
      "Training - i: 1271 Loss: 0.18375596 Accuracy: 0.91\n",
      "Training - i: 1272 Loss: 0.1606366 Accuracy: 0.925\n",
      "Training - i: 1273 Loss: 0.14691773 Accuracy: 0.945\n",
      "Training - i: 1274 Loss: 0.1538252 Accuracy: 0.945\n",
      "Training - i: 1275 Loss: 0.14410985 Accuracy: 0.945\n",
      "Training - i: 1276 Loss: 0.1583701 Accuracy: 0.935\n",
      "Training - i: 1277 Loss: 0.12490515 Accuracy: 0.955\n",
      "Training - i: 1278 Loss: 0.16146621 Accuracy: 0.925\n",
      "Training - i: 1279 Loss: 0.16362149 Accuracy: 0.93\n",
      "Training - i: 1280 Loss: 0.18207723 Accuracy: 0.93\n",
      "Training - i: 1281 Loss: 0.14191777 Accuracy: 0.955\n",
      "Training - i: 1282 Loss: 0.16887508 Accuracy: 0.93\n",
      "Training - i: 1283 Loss: 0.12583964 Accuracy: 0.95\n",
      "Training - i: 1284 Loss: 0.13564798 Accuracy: 0.955\n",
      "Training - i: 1285 Loss: 0.17890957 Accuracy: 0.925\n",
      "Training - i: 1286 Loss: 0.11527024 Accuracy: 0.955\n",
      "Training - i: 1287 Loss: 0.12650429 Accuracy: 0.96\n",
      "Training - i: 1288 Loss: 0.1124241 Accuracy: 0.935\n",
      "Training - i: 1289 Loss: 0.17440858 Accuracy: 0.925\n",
      "Training - i: 1290 Loss: 0.12736185 Accuracy: 0.945\n",
      "Training - i: 1291 Loss: 0.1348006 Accuracy: 0.95\n",
      "Training - i: 1292 Loss: 0.1668368 Accuracy: 0.925\n",
      "Training - i: 1293 Loss: 0.12706864 Accuracy: 0.95\n",
      "Training - i: 1294 Loss: 0.15525717 Accuracy: 0.93\n",
      "Training - i: 1295 Loss: 0.12638365 Accuracy: 0.95\n",
      "Training - i: 1296 Loss: 0.13669032 Accuracy: 0.955\n",
      "Training - i: 1297 Loss: 0.13992107 Accuracy: 0.94\n",
      "Training - i: 1298 Loss: 0.19234614 Accuracy: 0.93\n",
      "Training - i: 1299 Loss: 0.14318131 Accuracy: 0.925\n",
      "Training - i: 1300 Loss: 0.15564078 Accuracy: 0.935\n",
      "Training - i: 1301 Loss: 0.13585092 Accuracy: 0.935\n",
      "Validation - i: 1301  Accuracy: [0.875]\n",
      "Training - i: 1302 Loss: 0.18128437 Accuracy: 0.925\n",
      "Training - i: 1303 Loss: 0.17069755 Accuracy: 0.93\n",
      "Training - i: 1304 Loss: 0.15770274 Accuracy: 0.925\n",
      "Training - i: 1305 Loss: 0.11707783 Accuracy: 0.96\n",
      "Training - i: 1306 Loss: 0.13262387 Accuracy: 0.94\n",
      "Training - i: 1307 Loss: 0.13407083 Accuracy: 0.965\n",
      "Training - i: 1308 Loss: 0.20856999 Accuracy: 0.92\n",
      "Training - i: 1309 Loss: 0.12678091 Accuracy: 0.955\n",
      "Training - i: 1310 Loss: 0.20201126 Accuracy: 0.92\n",
      "Training - i: 1311 Loss: 0.1339193 Accuracy: 0.955\n",
      "Training - i: 1312 Loss: 0.1539853 Accuracy: 0.92\n",
      "Training - i: 1313 Loss: 0.14947182 Accuracy: 0.915\n",
      "Training - i: 1314 Loss: 0.14450818 Accuracy: 0.95\n",
      "Training - i: 1315 Loss: 0.11275919 Accuracy: 0.965\n",
      "Training - i: 1316 Loss: 0.1901882 Accuracy: 0.91\n",
      "Training - i: 1317 Loss: 0.1612866 Accuracy: 0.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - i: 1318 Loss: 0.13221303 Accuracy: 0.955\n",
      "Training - i: 1319 Loss: 0.15775096 Accuracy: 0.945\n",
      "Training - i: 1320 Loss: 0.15569699 Accuracy: 0.94\n",
      "Training - i: 1321 Loss: 0.13181688 Accuracy: 0.96\n",
      "Training - i: 1322 Loss: 0.13141365 Accuracy: 0.97\n",
      "Training - i: 1323 Loss: 0.13986188 Accuracy: 0.94\n",
      "Training - i: 1324 Loss: 0.15155664 Accuracy: 0.935\n",
      "Training - i: 1325 Loss: 0.12887597 Accuracy: 0.955\n",
      "Training - i: 1326 Loss: 0.18185116 Accuracy: 0.93\n",
      "Training - i: 1327 Loss: 0.12609772 Accuracy: 0.965\n",
      "Training - i: 1328 Loss: 0.12860896 Accuracy: 0.95\n",
      "Training - i: 1329 Loss: 0.1248395 Accuracy: 0.95\n",
      "Training - i: 1330 Loss: 0.16015282 Accuracy: 0.91\n",
      "Training - i: 1331 Loss: 0.17870332 Accuracy: 0.93\n",
      "Training - i: 1332 Loss: 0.13369109 Accuracy: 0.965\n",
      "Training - i: 1333 Loss: 0.14300676 Accuracy: 0.93\n",
      "Training - i: 1334 Loss: 0.12355814 Accuracy: 0.935\n",
      "Training - i: 1335 Loss: 0.09888353 Accuracy: 0.97\n",
      "Training - i: 1336 Loss: 0.102731325 Accuracy: 0.975\n",
      "Training - i: 1337 Loss: 0.13574938 Accuracy: 0.94\n",
      "Training - i: 1338 Loss: 0.13273741 Accuracy: 0.96\n",
      "Training - i: 1339 Loss: 0.08711921 Accuracy: 0.975\n",
      "Training - i: 1340 Loss: 0.10927581 Accuracy: 0.935\n",
      "Training - i: 1341 Loss: 0.11867088 Accuracy: 0.96\n",
      "Training - i: 1342 Loss: 0.1399241 Accuracy: 0.94\n",
      "Training - i: 1343 Loss: 0.13482864 Accuracy: 0.965\n",
      "Training - i: 1344 Loss: 0.15214032 Accuracy: 0.925\n",
      "Training - i: 1345 Loss: 0.1406283 Accuracy: 0.925\n",
      "Training - i: 1346 Loss: 0.09256023 Accuracy: 0.975\n",
      "Training - i: 1347 Loss: 0.1801787 Accuracy: 0.92\n",
      "Training - i: 1348 Loss: 0.16295867 Accuracy: 0.93\n",
      "Training - i: 1349 Loss: 0.09867062 Accuracy: 0.96\n",
      "Training - i: 1350 Loss: 0.09425118 Accuracy: 0.975\n",
      "Training - i: 1351 Loss: 0.1411659 Accuracy: 0.945\n",
      "Validation - i: 1351  Accuracy: [0.835]\n",
      "Training - i: 1352 Loss: 0.12418055 Accuracy: 0.955\n",
      "Training - i: 1353 Loss: 0.13714178 Accuracy: 0.945\n",
      "Training - i: 1354 Loss: 0.100392915 Accuracy: 0.97\n",
      "Training - i: 1355 Loss: 0.13658991 Accuracy: 0.94\n",
      "Training - i: 1356 Loss: 0.112276115 Accuracy: 0.955\n",
      "Training - i: 1357 Loss: 0.15607263 Accuracy: 0.94\n",
      "Training - i: 1358 Loss: 0.16263153 Accuracy: 0.935\n",
      "Training - i: 1359 Loss: 0.10181099 Accuracy: 0.94\n",
      "Training - i: 1360 Loss: 0.12585863 Accuracy: 0.95\n",
      "Training - i: 1361 Loss: 0.104617536 Accuracy: 0.965\n",
      "Training - i: 1362 Loss: 0.09536584 Accuracy: 0.965\n",
      "Training - i: 1363 Loss: 0.12800442 Accuracy: 0.93\n",
      "Training - i: 1364 Loss: 0.11404946 Accuracy: 0.955\n",
      "Training - i: 1365 Loss: 0.13417919 Accuracy: 0.935\n",
      "Training - i: 1366 Loss: 0.10966564 Accuracy: 0.94\n",
      "Training - i: 1367 Loss: 0.097457886 Accuracy: 0.97\n",
      "Training - i: 1368 Loss: 0.13386817 Accuracy: 0.94\n",
      "Training - i: 1369 Loss: 0.12748963 Accuracy: 0.94\n",
      "Training - i: 1370 Loss: 0.062042512 Accuracy: 0.985\n",
      "Training - i: 1371 Loss: 0.09204257 Accuracy: 0.955\n",
      "Training - i: 1372 Loss: 0.133655 Accuracy: 0.935\n",
      "Training - i: 1373 Loss: 0.15745333 Accuracy: 0.935\n",
      "Training - i: 1374 Loss: 0.14703616 Accuracy: 0.93\n",
      "Training - i: 1375 Loss: 0.12451415 Accuracy: 0.945\n",
      "Training - i: 1376 Loss: 0.12879561 Accuracy: 0.94\n",
      "Training - i: 1377 Loss: 0.17570171 Accuracy: 0.905\n",
      "Training - i: 1378 Loss: 0.10509627 Accuracy: 0.965\n",
      "Training - i: 1379 Loss: 0.16902345 Accuracy: 0.93\n",
      "Training - i: 1380 Loss: 0.1565527 Accuracy: 0.93\n",
      "Training - i: 1381 Loss: 0.11321552 Accuracy: 0.97\n",
      "Training - i: 1382 Loss: 0.194464 Accuracy: 0.91\n",
      "Training - i: 1383 Loss: 0.11185822 Accuracy: 0.945\n",
      "Training - i: 1384 Loss: 0.16195148 Accuracy: 0.91\n",
      "Training - i: 1385 Loss: 0.11335942 Accuracy: 0.96\n",
      "Training - i: 1386 Loss: 0.11280313 Accuracy: 0.97\n",
      "Training - i: 1387 Loss: 0.12092176 Accuracy: 0.96\n",
      "Training - i: 1388 Loss: 0.10626639 Accuracy: 0.975\n",
      "Training - i: 1389 Loss: 0.085646495 Accuracy: 0.965\n",
      "Training - i: 1390 Loss: 0.12644893 Accuracy: 0.94\n",
      "Training - i: 1391 Loss: 0.12900674 Accuracy: 0.935\n",
      "Training - i: 1392 Loss: 0.11018531 Accuracy: 0.955\n",
      "Training - i: 1393 Loss: 0.08930006 Accuracy: 0.98\n",
      "Training - i: 1394 Loss: 0.1426602 Accuracy: 0.935\n",
      "Training - i: 1395 Loss: 0.09577942 Accuracy: 0.955\n",
      "Training - i: 1396 Loss: 0.07497748 Accuracy: 0.975\n",
      "Training - i: 1397 Loss: 0.08362545 Accuracy: 0.97\n",
      "Training - i: 1398 Loss: 0.10608982 Accuracy: 0.94\n",
      "Training - i: 1399 Loss: 0.12335412 Accuracy: 0.955\n",
      "Training - i: 1400 Loss: 0.107448295 Accuracy: 0.965\n",
      "Training - i: 1401 Loss: 0.092588805 Accuracy: 0.965\n",
      "Validation - i: 1401  Accuracy: [0.905]\n",
      "Training - i: 1402 Loss: 0.15801357 Accuracy: 0.94\n",
      "Training - i: 1403 Loss: 0.11435951 Accuracy: 0.96\n",
      "Training - i: 1404 Loss: 0.14563511 Accuracy: 0.94\n",
      "Training - i: 1405 Loss: 0.077980176 Accuracy: 0.98\n",
      "Training - i: 1406 Loss: 0.14356567 Accuracy: 0.96\n",
      "Training - i: 1407 Loss: 0.1215363 Accuracy: 0.95\n",
      "Training - i: 1408 Loss: 0.1378388 Accuracy: 0.93\n",
      "Training - i: 1409 Loss: 0.15031551 Accuracy: 0.93\n",
      "Training - i: 1410 Loss: 0.09368069 Accuracy: 0.965\n",
      "Training - i: 1411 Loss: 0.18451855 Accuracy: 0.92\n",
      "Training - i: 1412 Loss: 0.10174396 Accuracy: 0.945\n",
      "Training - i: 1413 Loss: 0.09976564 Accuracy: 0.965\n",
      "Training - i: 1414 Loss: 0.15074159 Accuracy: 0.93\n",
      "Training - i: 1415 Loss: 0.11920118 Accuracy: 0.945\n",
      "Training - i: 1416 Loss: 0.17366458 Accuracy: 0.94\n",
      "Training - i: 1417 Loss: 0.13891806 Accuracy: 0.96\n",
      "Training - i: 1418 Loss: 0.1457478 Accuracy: 0.93\n",
      "Training - i: 1419 Loss: 0.15709394 Accuracy: 0.92\n",
      "Training - i: 1420 Loss: 0.12413167 Accuracy: 0.95\n",
      "Training - i: 1421 Loss: 0.16656297 Accuracy: 0.93\n",
      "Training - i: 1422 Loss: 0.1552126 Accuracy: 0.935\n",
      "Training - i: 1423 Loss: 0.116154365 Accuracy: 0.95\n",
      "Training - i: 1424 Loss: 0.11373938 Accuracy: 0.945\n",
      "Training - i: 1425 Loss: 0.21543019 Accuracy: 0.89\n",
      "Training - i: 1426 Loss: 0.15096812 Accuracy: 0.955\n",
      "Training - i: 1427 Loss: 0.26934928 Accuracy: 0.885\n",
      "Training - i: 1428 Loss: 0.121679485 Accuracy: 0.95\n",
      "Training - i: 1429 Loss: 0.22237179 Accuracy: 0.915\n",
      "Training - i: 1430 Loss: 0.17498028 Accuracy: 0.93\n",
      "Training - i: 1431 Loss: 0.152071 Accuracy: 0.92\n",
      "Training - i: 1432 Loss: 0.15363927 Accuracy: 0.93\n",
      "Training - i: 1433 Loss: 0.12529764 Accuracy: 0.965\n",
      "Training - i: 1434 Loss: 0.09147178 Accuracy: 0.97\n",
      "Training - i: 1435 Loss: 0.07371662 Accuracy: 0.99\n",
      "Training - i: 1436 Loss: 0.19996507 Accuracy: 0.91\n",
      "Training - i: 1437 Loss: 0.15543894 Accuracy: 0.925\n",
      "Training - i: 1438 Loss: 0.16845313 Accuracy: 0.94\n",
      "Training - i: 1439 Loss: 0.14587125 Accuracy: 0.95\n",
      "Training - i: 1440 Loss: 0.098848954 Accuracy: 0.965\n",
      "Training - i: 1441 Loss: 0.21683688 Accuracy: 0.895\n",
      "Training - i: 1442 Loss: 0.11310083 Accuracy: 0.95\n",
      "Training - i: 1443 Loss: 0.12775299 Accuracy: 0.965\n",
      "Training - i: 1444 Loss: 0.13387014 Accuracy: 0.96\n",
      "Training - i: 1445 Loss: 0.14671631 Accuracy: 0.955\n",
      "Training - i: 1446 Loss: 0.13105275 Accuracy: 0.94\n",
      "Training - i: 1447 Loss: 0.116210096 Accuracy: 0.94\n",
      "Training - i: 1448 Loss: 0.16010326 Accuracy: 0.945\n",
      "Training - i: 1449 Loss: 0.1251465 Accuracy: 0.945\n",
      "Training - i: 1450 Loss: 0.16510509 Accuracy: 0.935\n",
      "Training - i: 1451 Loss: 0.095690526 Accuracy: 0.955\n",
      "Validation - i: 1451  Accuracy: [0.885]\n",
      "Training - i: 1452 Loss: 0.07742145 Accuracy: 0.97\n",
      "Training - i: 1453 Loss: 0.15036798 Accuracy: 0.915\n",
      "Training - i: 1454 Loss: 0.109762765 Accuracy: 0.96\n",
      "Training - i: 1455 Loss: 0.12978424 Accuracy: 0.945\n",
      "Training - i: 1456 Loss: 0.12445217 Accuracy: 0.96\n",
      "Training - i: 1457 Loss: 0.11684841 Accuracy: 0.97\n",
      "Training - i: 1458 Loss: 0.13113374 Accuracy: 0.95\n",
      "Training - i: 1459 Loss: 0.17319183 Accuracy: 0.92\n",
      "Training - i: 1460 Loss: 0.1259305 Accuracy: 0.945\n",
      "Training - i: 1461 Loss: 0.10361248 Accuracy: 0.95\n",
      "Training - i: 1462 Loss: 0.096517414 Accuracy: 0.965\n",
      "Training - i: 1463 Loss: 0.1684465 Accuracy: 0.94\n",
      "Training - i: 1464 Loss: 0.11029547 Accuracy: 0.965\n",
      "Training - i: 1465 Loss: 0.13868026 Accuracy: 0.94\n",
      "Training - i: 1466 Loss: 0.14494562 Accuracy: 0.95\n",
      "Training - i: 1467 Loss: 0.15333688 Accuracy: 0.925\n",
      "Training - i: 1468 Loss: 0.1291205 Accuracy: 0.955\n",
      "Training - i: 1469 Loss: 0.13860819 Accuracy: 0.955\n",
      "Training - i: 1470 Loss: 0.11551415 Accuracy: 0.955\n",
      "Training - i: 1471 Loss: 0.0958864 Accuracy: 0.965\n",
      "Training - i: 1472 Loss: 0.16666187 Accuracy: 0.93\n",
      "Training - i: 1473 Loss: 0.10292352 Accuracy: 0.965\n",
      "Training - i: 1474 Loss: 0.155131 Accuracy: 0.925\n",
      "Training - i: 1475 Loss: 0.17693573 Accuracy: 0.95\n",
      "Training - i: 1476 Loss: 0.08577431 Accuracy: 0.975\n",
      "Training - i: 1477 Loss: 0.16167039 Accuracy: 0.925\n",
      "Training - i: 1478 Loss: 0.12564738 Accuracy: 0.94\n",
      "Training - i: 1479 Loss: 0.14231712 Accuracy: 0.92\n",
      "Training - i: 1480 Loss: 0.08349003 Accuracy: 0.965\n",
      "Training - i: 1481 Loss: 0.12627466 Accuracy: 0.96\n",
      "Training - i: 1482 Loss: 0.13092002 Accuracy: 0.96\n",
      "Training - i: 1483 Loss: 0.1475305 Accuracy: 0.94\n",
      "Training - i: 1484 Loss: 0.104740016 Accuracy: 0.935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - i: 1485 Loss: 0.08660243 Accuracy: 0.96\n",
      "Training - i: 1486 Loss: 0.16602688 Accuracy: 0.93\n",
      "Training - i: 1487 Loss: 0.15826727 Accuracy: 0.945\n",
      "Training - i: 1488 Loss: 0.121744506 Accuracy: 0.955\n",
      "Training - i: 1489 Loss: 0.08709279 Accuracy: 0.975\n",
      "Training - i: 1490 Loss: 0.08182077 Accuracy: 0.965\n",
      "Training - i: 1491 Loss: 0.099363156 Accuracy: 0.96\n",
      "Training - i: 1492 Loss: 0.12134749 Accuracy: 0.97\n",
      "Training - i: 1493 Loss: 0.11967367 Accuracy: 0.96\n",
      "Training - i: 1494 Loss: 0.11424997 Accuracy: 0.97\n",
      "Training - i: 1495 Loss: 0.15598203 Accuracy: 0.93\n",
      "Training - i: 1496 Loss: 0.12652975 Accuracy: 0.955\n",
      "Training - i: 1497 Loss: 0.13546002 Accuracy: 0.94\n",
      "Training - i: 1498 Loss: 0.14741299 Accuracy: 0.935\n",
      "Training - i: 1499 Loss: 0.14340238 Accuracy: 0.94\n",
      "Training - i: 1500 Loss: 0.116935566 Accuracy: 0.975\n",
      "Training - i: 1501 Loss: 0.1182955 Accuracy: 0.95\n",
      "Validation - i: 1501  Accuracy: [0.915]\n",
      "Training - i: 1502 Loss: 0.0828211 Accuracy: 0.965\n",
      "Training - i: 1503 Loss: 0.12364121 Accuracy: 0.95\n",
      "Training - i: 1504 Loss: 0.18132156 Accuracy: 0.935\n",
      "Training - i: 1505 Loss: 0.10351177 Accuracy: 0.96\n",
      "Training - i: 1506 Loss: 0.07661272 Accuracy: 0.975\n",
      "Training - i: 1507 Loss: 0.13654965 Accuracy: 0.925\n",
      "Training - i: 1508 Loss: 0.118423134 Accuracy: 0.955\n",
      "Training - i: 1509 Loss: 0.110769376 Accuracy: 0.94\n",
      "Training - i: 1510 Loss: 0.13312764 Accuracy: 0.95\n",
      "Training - i: 1511 Loss: 0.11969332 Accuracy: 0.945\n",
      "Training - i: 1512 Loss: 0.10575939 Accuracy: 0.975\n",
      "Training - i: 1513 Loss: 0.116665535 Accuracy: 0.96\n",
      "Training - i: 1514 Loss: 0.098525316 Accuracy: 0.975\n",
      "Training - i: 1515 Loss: 0.10363548 Accuracy: 0.96\n",
      "Training - i: 1516 Loss: 0.13078941 Accuracy: 0.94\n",
      "Training - i: 1517 Loss: 0.07843956 Accuracy: 0.965\n",
      "Training - i: 1518 Loss: 0.1563809 Accuracy: 0.93\n",
      "Training - i: 1519 Loss: 0.12890534 Accuracy: 0.95\n",
      "Training - i: 1520 Loss: 0.13699189 Accuracy: 0.93\n",
      "Training - i: 1521 Loss: 0.09905196 Accuracy: 0.965\n",
      "Training - i: 1522 Loss: 0.15102746 Accuracy: 0.935\n",
      "Training - i: 1523 Loss: 0.11386158 Accuracy: 0.96\n",
      "Training - i: 1524 Loss: 0.088672906 Accuracy: 0.97\n",
      "Training - i: 1525 Loss: 0.1466052 Accuracy: 0.945\n",
      "Training - i: 1526 Loss: 0.10746771 Accuracy: 0.96\n",
      "Training - i: 1527 Loss: 0.09135194 Accuracy: 0.97\n",
      "Training - i: 1528 Loss: 0.096158564 Accuracy: 0.965\n",
      "Training - i: 1529 Loss: 0.10902736 Accuracy: 0.95\n",
      "Training - i: 1530 Loss: 0.121924326 Accuracy: 0.965\n",
      "Training - i: 1531 Loss: 0.07519692 Accuracy: 0.97\n",
      "Training - i: 1532 Loss: 0.10383437 Accuracy: 0.96\n",
      "Training - i: 1533 Loss: 0.10775083 Accuracy: 0.94\n",
      "Training - i: 1534 Loss: 0.10857232 Accuracy: 0.955\n",
      "Training - i: 1535 Loss: 0.11081747 Accuracy: 0.955\n",
      "Training - i: 1536 Loss: 0.08460472 Accuracy: 0.97\n",
      "Training - i: 1537 Loss: 0.091743566 Accuracy: 0.96\n",
      "Training - i: 1538 Loss: 0.098563366 Accuracy: 0.97\n",
      "Training - i: 1539 Loss: 0.10547979 Accuracy: 0.945\n",
      "Training - i: 1540 Loss: 0.117527984 Accuracy: 0.945\n",
      "Training - i: 1541 Loss: 0.106456034 Accuracy: 0.955\n",
      "Training - i: 1542 Loss: 0.14510278 Accuracy: 0.94\n",
      "Training - i: 1543 Loss: 0.12584037 Accuracy: 0.94\n",
      "Training - i: 1544 Loss: 0.1365813 Accuracy: 0.94\n",
      "Training - i: 1545 Loss: 0.069080316 Accuracy: 0.985\n",
      "Training - i: 1546 Loss: 0.07865868 Accuracy: 0.97\n",
      "Training - i: 1547 Loss: 0.13089137 Accuracy: 0.935\n",
      "Training - i: 1548 Loss: 0.14378862 Accuracy: 0.935\n",
      "Training - i: 1549 Loss: 0.11424484 Accuracy: 0.955\n",
      "Training - i: 1550 Loss: 0.10422423 Accuracy: 0.975\n",
      "Training - i: 1551 Loss: 0.14518236 Accuracy: 0.935\n",
      "Validation - i: 1551  Accuracy: [0.875]\n",
      "Training - i: 1552 Loss: 0.10197938 Accuracy: 0.97\n",
      "Training - i: 1553 Loss: 0.10961611 Accuracy: 0.96\n",
      "Training - i: 1554 Loss: 0.16779347 Accuracy: 0.905\n",
      "Training - i: 1555 Loss: 0.12040206 Accuracy: 0.95\n",
      "Training - i: 1556 Loss: 0.11781685 Accuracy: 0.965\n",
      "Training - i: 1557 Loss: 0.12323601 Accuracy: 0.955\n",
      "Training - i: 1558 Loss: 0.09587103 Accuracy: 0.965\n",
      "Training - i: 1559 Loss: 0.109754875 Accuracy: 0.945\n",
      "Training - i: 1560 Loss: 0.11144949 Accuracy: 0.95\n",
      "Training - i: 1561 Loss: 0.12099638 Accuracy: 0.945\n",
      "Training - i: 1562 Loss: 0.12475294 Accuracy: 0.95\n",
      "Training - i: 1563 Loss: 0.14072603 Accuracy: 0.945\n",
      "Training - i: 1564 Loss: 0.12936838 Accuracy: 0.945\n",
      "Training - i: 1565 Loss: 0.10053444 Accuracy: 0.97\n",
      "Training - i: 1566 Loss: 0.12803622 Accuracy: 0.93\n",
      "Training - i: 1567 Loss: 0.113231145 Accuracy: 0.95\n",
      "Training - i: 1568 Loss: 0.19376075 Accuracy: 0.925\n",
      "Training - i: 1569 Loss: 0.14187661 Accuracy: 0.95\n",
      "Training - i: 1570 Loss: 0.12512068 Accuracy: 0.945\n",
      "Training - i: 1571 Loss: 0.0936281 Accuracy: 0.96\n",
      "Training - i: 1572 Loss: 0.09520685 Accuracy: 0.95\n",
      "Training - i: 1573 Loss: 0.08976402 Accuracy: 0.96\n",
      "Training - i: 1574 Loss: 0.13128804 Accuracy: 0.955\n",
      "Training - i: 1575 Loss: 0.13309658 Accuracy: 0.935\n",
      "Training - i: 1576 Loss: 0.12010436 Accuracy: 0.94\n",
      "Training - i: 1577 Loss: 0.11273237 Accuracy: 0.97\n",
      "Training - i: 1578 Loss: 0.13968675 Accuracy: 0.94\n",
      "Training - i: 1579 Loss: 0.117623664 Accuracy: 0.955\n",
      "Training - i: 1580 Loss: 0.08884467 Accuracy: 0.965\n",
      "Training - i: 1581 Loss: 0.08101103 Accuracy: 0.955\n",
      "Training - i: 1582 Loss: 0.13492164 Accuracy: 0.95\n",
      "Training - i: 1583 Loss: 0.11259306 Accuracy: 0.945\n",
      "Training - i: 1584 Loss: 0.14989157 Accuracy: 0.925\n",
      "Training - i: 1585 Loss: 0.105308056 Accuracy: 0.96\n",
      "Training - i: 1586 Loss: 0.13813171 Accuracy: 0.95\n",
      "Training - i: 1587 Loss: 0.08056994 Accuracy: 0.97\n",
      "Training - i: 1588 Loss: 0.14245798 Accuracy: 0.94\n",
      "Training - i: 1589 Loss: 0.16545723 Accuracy: 0.93\n",
      "Training - i: 1590 Loss: 0.13290095 Accuracy: 0.94\n",
      "Training - i: 1591 Loss: 0.13039874 Accuracy: 0.95\n",
      "Training - i: 1592 Loss: 0.14159761 Accuracy: 0.94\n",
      "Training - i: 1593 Loss: 0.12767853 Accuracy: 0.945\n",
      "Training - i: 1594 Loss: 0.14100751 Accuracy: 0.94\n",
      "Training - i: 1595 Loss: 0.123131864 Accuracy: 0.96\n",
      "Training - i: 1596 Loss: 0.12140093 Accuracy: 0.965\n",
      "Training - i: 1597 Loss: 0.11874424 Accuracy: 0.955\n",
      "Training - i: 1598 Loss: 0.12797654 Accuracy: 0.955\n",
      "Training - i: 1599 Loss: 0.119624466 Accuracy: 0.955\n",
      "Training - i: 1600 Loss: 0.13259447 Accuracy: 0.93\n",
      "Training - i: 1601 Loss: 0.15036812 Accuracy: 0.935\n",
      "Validation - i: 1601  Accuracy: [0.875]\n",
      "Training - i: 1602 Loss: 0.1089884 Accuracy: 0.95\n",
      "Training - i: 1603 Loss: 0.12534516 Accuracy: 0.95\n",
      "Training - i: 1604 Loss: 0.19338654 Accuracy: 0.905\n",
      "Training - i: 1605 Loss: 0.0972471 Accuracy: 0.965\n",
      "Training - i: 1606 Loss: 0.11652228 Accuracy: 0.945\n",
      "Training - i: 1607 Loss: 0.16588156 Accuracy: 0.935\n",
      "Training - i: 1608 Loss: 0.13645446 Accuracy: 0.945\n",
      "Training - i: 1609 Loss: 0.122436285 Accuracy: 0.95\n",
      "Training - i: 1610 Loss: 0.16557378 Accuracy: 0.935\n",
      "Training - i: 1611 Loss: 0.13414446 Accuracy: 0.95\n",
      "Training - i: 1612 Loss: 0.1589313 Accuracy: 0.91\n",
      "Training - i: 1613 Loss: 0.11498604 Accuracy: 0.95\n",
      "Training - i: 1614 Loss: 0.12688617 Accuracy: 0.96\n",
      "Training - i: 1615 Loss: 0.18096901 Accuracy: 0.92\n",
      "Training - i: 1616 Loss: 0.08585611 Accuracy: 0.98\n",
      "Training - i: 1617 Loss: 0.09565235 Accuracy: 0.955\n",
      "Training - i: 1618 Loss: 0.15339829 Accuracy: 0.93\n",
      "Training - i: 1619 Loss: 0.13660647 Accuracy: 0.92\n",
      "Training - i: 1620 Loss: 0.083618775 Accuracy: 0.95\n",
      "Training - i: 1621 Loss: 0.11744866 Accuracy: 0.95\n",
      "Training - i: 1622 Loss: 0.21641727 Accuracy: 0.9\n",
      "Training - i: 1623 Loss: 0.0745259 Accuracy: 0.96\n",
      "Training - i: 1624 Loss: 0.1411413 Accuracy: 0.94\n",
      "Training - i: 1625 Loss: 0.12783805 Accuracy: 0.955\n",
      "Training - i: 1626 Loss: 0.10215752 Accuracy: 0.965\n",
      "Training - i: 1627 Loss: 0.08343319 Accuracy: 0.97\n",
      "Training - i: 1628 Loss: 0.17569618 Accuracy: 0.94\n",
      "Training - i: 1629 Loss: 0.076942824 Accuracy: 0.98\n",
      "Training - i: 1630 Loss: 0.17258817 Accuracy: 0.91\n",
      "Training - i: 1631 Loss: 0.085956424 Accuracy: 0.97\n",
      "Training - i: 1632 Loss: 0.11525341 Accuracy: 0.95\n",
      "Training - i: 1633 Loss: 0.15198457 Accuracy: 0.935\n",
      "Training - i: 1634 Loss: 0.115279205 Accuracy: 0.94\n",
      "Training - i: 1635 Loss: 0.1270267 Accuracy: 0.955\n",
      "Training - i: 1636 Loss: 0.12350205 Accuracy: 0.95\n",
      "Training - i: 1637 Loss: 0.10520064 Accuracy: 0.945\n",
      "Training - i: 1638 Loss: 0.11529865 Accuracy: 0.935\n",
      "Training - i: 1639 Loss: 0.15145591 Accuracy: 0.925\n",
      "Training - i: 1640 Loss: 0.106011465 Accuracy: 0.96\n",
      "Training - i: 1641 Loss: 0.14959207 Accuracy: 0.94\n",
      "Training - i: 1642 Loss: 0.18589495 Accuracy: 0.93\n",
      "Training - i: 1643 Loss: 0.15207504 Accuracy: 0.92\n",
      "Training - i: 1644 Loss: 0.15101556 Accuracy: 0.92\n",
      "Training - i: 1645 Loss: 0.10213955 Accuracy: 0.96\n",
      "Training - i: 1646 Loss: 0.13855472 Accuracy: 0.945\n",
      "Training - i: 1647 Loss: 0.11385122 Accuracy: 0.96\n",
      "Training - i: 1648 Loss: 0.14341311 Accuracy: 0.935\n",
      "Training - i: 1649 Loss: 0.12057049 Accuracy: 0.945\n",
      "Training - i: 1650 Loss: 0.11308609 Accuracy: 0.96\n",
      "Training - i: 1651 Loss: 0.11227329 Accuracy: 0.935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - i: 1651  Accuracy: [0.92]\n",
      "Training - i: 1652 Loss: 0.11962755 Accuracy: 0.945\n",
      "Training - i: 1653 Loss: 0.08270972 Accuracy: 0.97\n",
      "Training - i: 1654 Loss: 0.16192858 Accuracy: 0.93\n",
      "Training - i: 1655 Loss: 0.103224106 Accuracy: 0.97\n",
      "Training - i: 1656 Loss: 0.09148213 Accuracy: 0.965\n",
      "Training - i: 1657 Loss: 0.12449107 Accuracy: 0.95\n",
      "Training - i: 1658 Loss: 0.16726924 Accuracy: 0.91\n",
      "Training - i: 1659 Loss: 0.115882725 Accuracy: 0.94\n",
      "Training - i: 1660 Loss: 0.17244673 Accuracy: 0.945\n",
      "Training - i: 1661 Loss: 0.13481551 Accuracy: 0.95\n",
      "Training - i: 1662 Loss: 0.12868614 Accuracy: 0.96\n",
      "Training - i: 1663 Loss: 0.17965093 Accuracy: 0.935\n",
      "Training - i: 1664 Loss: 0.13167423 Accuracy: 0.95\n",
      "Training - i: 1665 Loss: 0.2004569 Accuracy: 0.915\n",
      "Training - i: 1666 Loss: 0.08392263 Accuracy: 0.97\n",
      "Training - i: 1667 Loss: 0.11001255 Accuracy: 0.94\n",
      "Training - i: 1668 Loss: 0.14098604 Accuracy: 0.94\n",
      "Training - i: 1669 Loss: 0.11146733 Accuracy: 0.965\n",
      "Training - i: 1670 Loss: 0.15404697 Accuracy: 0.915\n",
      "Training - i: 1671 Loss: 0.12491267 Accuracy: 0.94\n",
      "Training - i: 1672 Loss: 0.14623848 Accuracy: 0.925\n",
      "Training - i: 1673 Loss: 0.10479757 Accuracy: 0.97\n",
      "Training - i: 1674 Loss: 0.08367174 Accuracy: 0.97\n",
      "Training - i: 1675 Loss: 0.1489444 Accuracy: 0.925\n",
      "Training - i: 1676 Loss: 0.08806825 Accuracy: 0.96\n",
      "Training - i: 1677 Loss: 0.10502682 Accuracy: 0.94\n",
      "Training - i: 1678 Loss: 0.11134484 Accuracy: 0.935\n",
      "Training - i: 1679 Loss: 0.08806518 Accuracy: 0.965\n",
      "Training - i: 1680 Loss: 0.13184355 Accuracy: 0.945\n",
      "Training - i: 1681 Loss: 0.09620573 Accuracy: 0.965\n",
      "Training - i: 1682 Loss: 0.10729082 Accuracy: 0.945\n",
      "Training - i: 1683 Loss: 0.10583155 Accuracy: 0.945\n",
      "Training - i: 1684 Loss: 0.09001717 Accuracy: 0.95\n",
      "Training - i: 1685 Loss: 0.08587296 Accuracy: 0.97\n",
      "Training - i: 1686 Loss: 0.08126266 Accuracy: 0.97\n",
      "Training - i: 1687 Loss: 0.09179715 Accuracy: 0.955\n",
      "Training - i: 1688 Loss: 0.09835745 Accuracy: 0.975\n",
      "Training - i: 1689 Loss: 0.08768673 Accuracy: 0.955\n",
      "Training - i: 1690 Loss: 0.09850469 Accuracy: 0.965\n",
      "Training - i: 1691 Loss: 0.11586983 Accuracy: 0.945\n",
      "Training - i: 1692 Loss: 0.14757185 Accuracy: 0.925\n",
      "Training - i: 1693 Loss: 0.087210216 Accuracy: 0.95\n",
      "Training - i: 1694 Loss: 0.11348688 Accuracy: 0.955\n",
      "Training - i: 1695 Loss: 0.097298816 Accuracy: 0.97\n",
      "Training - i: 1696 Loss: 0.15007086 Accuracy: 0.94\n",
      "Training - i: 1697 Loss: 0.08266527 Accuracy: 0.97\n",
      "Training - i: 1698 Loss: 0.1080061 Accuracy: 0.955\n",
      "Training - i: 1699 Loss: 0.16296795 Accuracy: 0.915\n",
      "Training - i: 1700 Loss: 0.098527394 Accuracy: 0.96\n",
      "Training - i: 1701 Loss: 0.08001654 Accuracy: 0.975\n",
      "Validation - i: 1701  Accuracy: [0.9]\n",
      "Training - i: 1702 Loss: 0.114125386 Accuracy: 0.96\n",
      "Training - i: 1703 Loss: 0.09420709 Accuracy: 0.98\n",
      "Training - i: 1704 Loss: 0.09864779 Accuracy: 0.96\n",
      "Training - i: 1705 Loss: 0.10285534 Accuracy: 0.95\n",
      "Training - i: 1706 Loss: 0.13406236 Accuracy: 0.94\n",
      "Training - i: 1707 Loss: 0.12923814 Accuracy: 0.935\n",
      "Training - i: 1708 Loss: 0.11053167 Accuracy: 0.95\n",
      "Training - i: 1709 Loss: 0.12637669 Accuracy: 0.93\n",
      "Training - i: 1710 Loss: 0.111121394 Accuracy: 0.94\n",
      "Training - i: 1711 Loss: 0.08717245 Accuracy: 0.965\n",
      "Training - i: 1712 Loss: 0.09240722 Accuracy: 0.98\n",
      "Training - i: 1713 Loss: 0.11423893 Accuracy: 0.95\n",
      "Training - i: 1714 Loss: 0.16424 Accuracy: 0.925\n",
      "Training - i: 1715 Loss: 0.103357784 Accuracy: 0.945\n",
      "Training - i: 1716 Loss: 0.10700972 Accuracy: 0.96\n",
      "Training - i: 1717 Loss: 0.11541966 Accuracy: 0.935\n",
      "Training - i: 1718 Loss: 0.09274603 Accuracy: 0.97\n",
      "Training - i: 1719 Loss: 0.07552493 Accuracy: 0.985\n",
      "Training - i: 1720 Loss: 0.1115484 Accuracy: 0.96\n",
      "Training - i: 1721 Loss: 0.07201504 Accuracy: 0.965\n",
      "Training - i: 1722 Loss: 0.1055465 Accuracy: 0.95\n",
      "Training - i: 1723 Loss: 0.098139055 Accuracy: 0.95\n",
      "Training - i: 1724 Loss: 0.071822226 Accuracy: 0.975\n",
      "Training - i: 1725 Loss: 0.10920908 Accuracy: 0.955\n",
      "Training - i: 1726 Loss: 0.098073184 Accuracy: 0.975\n",
      "Training - i: 1727 Loss: 0.0784825 Accuracy: 0.955\n",
      "Training - i: 1728 Loss: 0.08410552 Accuracy: 0.97\n",
      "Training - i: 1729 Loss: 0.08593392 Accuracy: 0.97\n",
      "Training - i: 1730 Loss: 0.08250852 Accuracy: 0.955\n",
      "Training - i: 1731 Loss: 0.082160495 Accuracy: 0.965\n",
      "Training - i: 1732 Loss: 0.050526276 Accuracy: 0.99\n",
      "Training - i: 1733 Loss: 0.09156643 Accuracy: 0.96\n",
      "Training - i: 1734 Loss: 0.13238344 Accuracy: 0.935\n",
      "Training - i: 1735 Loss: 0.075989574 Accuracy: 0.975\n",
      "Training - i: 1736 Loss: 0.062373552 Accuracy: 0.97\n",
      "Training - i: 1737 Loss: 0.08688708 Accuracy: 0.965\n",
      "Training - i: 1738 Loss: 0.10698646 Accuracy: 0.955\n",
      "Training - i: 1739 Loss: 0.10154726 Accuracy: 0.965\n",
      "Training - i: 1740 Loss: 0.11912103 Accuracy: 0.95\n",
      "Training - i: 1741 Loss: 0.09640191 Accuracy: 0.965\n",
      "Training - i: 1742 Loss: 0.120876886 Accuracy: 0.95\n",
      "Training - i: 1743 Loss: 0.101498365 Accuracy: 0.96\n",
      "Training - i: 1744 Loss: 0.12864013 Accuracy: 0.95\n",
      "Training - i: 1745 Loss: 0.06751302 Accuracy: 0.97\n",
      "Training - i: 1746 Loss: 0.09871521 Accuracy: 0.95\n",
      "Training - i: 1747 Loss: 0.1303713 Accuracy: 0.93\n",
      "Training - i: 1748 Loss: 0.10054804 Accuracy: 0.955\n",
      "Training - i: 1749 Loss: 0.08831522 Accuracy: 0.975\n",
      "Training - i: 1750 Loss: 0.11813713 Accuracy: 0.95\n",
      "Training - i: 1751 Loss: 0.07924005 Accuracy: 0.98\n",
      "Validation - i: 1751  Accuracy: [0.87]\n",
      "Training - i: 1752 Loss: 0.09948942 Accuracy: 0.945\n",
      "Training - i: 1753 Loss: 0.12937519 Accuracy: 0.93\n",
      "Training - i: 1754 Loss: 0.09835014 Accuracy: 0.955\n",
      "Training - i: 1755 Loss: 0.08818513 Accuracy: 0.975\n",
      "Training - i: 1756 Loss: 0.09334364 Accuracy: 0.98\n",
      "Training - i: 1757 Loss: 0.10074802 Accuracy: 0.98\n",
      "Training - i: 1758 Loss: 0.12655574 Accuracy: 0.96\n",
      "Training - i: 1759 Loss: 0.07884083 Accuracy: 0.96\n",
      "Training - i: 1760 Loss: 0.12309207 Accuracy: 0.95\n",
      "Training - i: 1761 Loss: 0.11075341 Accuracy: 0.945\n",
      "Training - i: 1762 Loss: 0.10834046 Accuracy: 0.945\n",
      "Training - i: 1763 Loss: 0.10796247 Accuracy: 0.96\n",
      "Training - i: 1764 Loss: 0.11570087 Accuracy: 0.96\n",
      "Training - i: 1765 Loss: 0.094254054 Accuracy: 0.97\n",
      "Training - i: 1766 Loss: 0.05350442 Accuracy: 0.98\n",
      "Training - i: 1767 Loss: 0.09181226 Accuracy: 0.965\n",
      "Training - i: 1768 Loss: 0.086582586 Accuracy: 0.96\n",
      "Training - i: 1769 Loss: 0.09574198 Accuracy: 0.96\n",
      "Training - i: 1770 Loss: 0.089203455 Accuracy: 0.965\n",
      "Training - i: 1771 Loss: 0.08381425 Accuracy: 0.97\n",
      "Training - i: 1772 Loss: 0.11646784 Accuracy: 0.955\n",
      "Training - i: 1773 Loss: 0.09302464 Accuracy: 0.96\n",
      "Training - i: 1774 Loss: 0.059458617 Accuracy: 0.985\n",
      "Training - i: 1775 Loss: 0.114181206 Accuracy: 0.935\n",
      "Training - i: 1776 Loss: 0.113867514 Accuracy: 0.935\n",
      "Training - i: 1777 Loss: 0.102294825 Accuracy: 0.96\n",
      "Training - i: 1778 Loss: 0.16304602 Accuracy: 0.925\n",
      "Training - i: 1779 Loss: 0.15814143 Accuracy: 0.92\n",
      "Training - i: 1780 Loss: 0.07518979 Accuracy: 0.98\n",
      "Training - i: 1781 Loss: 0.08258438 Accuracy: 0.97\n",
      "Training - i: 1782 Loss: 0.16477041 Accuracy: 0.915\n",
      "Training - i: 1783 Loss: 0.14920126 Accuracy: 0.93\n",
      "Training - i: 1784 Loss: 0.08170367 Accuracy: 0.98\n",
      "Training - i: 1785 Loss: 0.18722881 Accuracy: 0.935\n",
      "Training - i: 1786 Loss: 0.12772694 Accuracy: 0.95\n",
      "Training - i: 1787 Loss: 0.09548742 Accuracy: 0.95\n",
      "Training - i: 1788 Loss: 0.14654402 Accuracy: 0.93\n",
      "Training - i: 1789 Loss: 0.10424416 Accuracy: 0.95\n",
      "Training - i: 1790 Loss: 0.10618645 Accuracy: 0.955\n",
      "Training - i: 1791 Loss: 0.1662264 Accuracy: 0.92\n",
      "Training - i: 1792 Loss: 0.13455911 Accuracy: 0.965\n",
      "Training - i: 1793 Loss: 0.13079122 Accuracy: 0.95\n",
      "Training - i: 1794 Loss: 0.13291293 Accuracy: 0.945\n",
      "Training - i: 1795 Loss: 0.14637932 Accuracy: 0.945\n",
      "Training - i: 1796 Loss: 0.124976404 Accuracy: 0.95\n",
      "Training - i: 1797 Loss: 0.10342327 Accuracy: 0.965\n",
      "Training - i: 1798 Loss: 0.13528682 Accuracy: 0.945\n",
      "Training - i: 1799 Loss: 0.14767158 Accuracy: 0.945\n",
      "Training - i: 1800 Loss: 0.15906751 Accuracy: 0.92\n",
      "Training - i: 1801 Loss: 0.13153695 Accuracy: 0.945\n",
      "Validation - i: 1801  Accuracy: [0.895]\n",
      "Training - i: 1802 Loss: 0.1019936 Accuracy: 0.96\n",
      "Training - i: 1803 Loss: 0.1105972 Accuracy: 0.95\n",
      "Training - i: 1804 Loss: 0.13224012 Accuracy: 0.945\n",
      "Training - i: 1805 Loss: 0.12963222 Accuracy: 0.96\n",
      "Training - i: 1806 Loss: 0.12874165 Accuracy: 0.935\n",
      "Training - i: 1807 Loss: 0.13070092 Accuracy: 0.935\n",
      "Training - i: 1808 Loss: 0.12303934 Accuracy: 0.95\n",
      "Training - i: 1809 Loss: 0.11844639 Accuracy: 0.945\n",
      "Training - i: 1810 Loss: 0.09432698 Accuracy: 0.96\n",
      "Training - i: 1811 Loss: 0.122 Accuracy: 0.945\n",
      "Training - i: 1812 Loss: 0.18716964 Accuracy: 0.925\n",
      "Training - i: 1813 Loss: 0.09483038 Accuracy: 0.97\n",
      "Training - i: 1814 Loss: 0.0747347 Accuracy: 0.98\n",
      "Training - i: 1815 Loss: 0.07316183 Accuracy: 0.975\n",
      "Training - i: 1816 Loss: 0.15038103 Accuracy: 0.94\n",
      "Training - i: 1817 Loss: 0.16470794 Accuracy: 0.935\n",
      "Training - i: 1818 Loss: 0.12331805 Accuracy: 0.945\n",
      "Training - i: 1819 Loss: 0.124838136 Accuracy: 0.94\n",
      "Training - i: 1820 Loss: 0.107425034 Accuracy: 0.955\n",
      "Training - i: 1821 Loss: 0.064780235 Accuracy: 0.985\n",
      "Training - i: 1822 Loss: 0.103633024 Accuracy: 0.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - i: 1823 Loss: 0.10635779 Accuracy: 0.96\n",
      "Training - i: 1824 Loss: 0.09656748 Accuracy: 0.965\n",
      "Training - i: 1825 Loss: 0.11989103 Accuracy: 0.955\n",
      "Training - i: 1826 Loss: 0.10627584 Accuracy: 0.945\n",
      "Training - i: 1827 Loss: 0.13163759 Accuracy: 0.945\n",
      "Training - i: 1828 Loss: 0.086216755 Accuracy: 0.97\n",
      "Training - i: 1829 Loss: 0.099282496 Accuracy: 0.96\n",
      "Training - i: 1830 Loss: 0.080342084 Accuracy: 0.965\n",
      "Training - i: 1831 Loss: 0.10218209 Accuracy: 0.965\n",
      "Training - i: 1832 Loss: 0.15186915 Accuracy: 0.94\n",
      "Training - i: 1833 Loss: 0.122940235 Accuracy: 0.955\n",
      "Training - i: 1834 Loss: 0.08678123 Accuracy: 0.965\n",
      "Training - i: 1835 Loss: 0.07617626 Accuracy: 0.98\n",
      "Training - i: 1836 Loss: 0.059396554 Accuracy: 0.995\n",
      "Training - i: 1837 Loss: 0.11367882 Accuracy: 0.95\n",
      "Training - i: 1838 Loss: 0.06375016 Accuracy: 0.97\n",
      "Training - i: 1839 Loss: 0.10453091 Accuracy: 0.955\n",
      "Training - i: 1840 Loss: 0.13283493 Accuracy: 0.945\n",
      "Training - i: 1841 Loss: 0.12036755 Accuracy: 0.955\n",
      "Training - i: 1842 Loss: 0.10924287 Accuracy: 0.96\n",
      "Training - i: 1843 Loss: 0.113327615 Accuracy: 0.955\n",
      "Training - i: 1844 Loss: 0.07611237 Accuracy: 0.975\n",
      "Training - i: 1845 Loss: 0.073837824 Accuracy: 0.975\n",
      "Training - i: 1846 Loss: 0.10770716 Accuracy: 0.95\n",
      "Training - i: 1847 Loss: 0.121561185 Accuracy: 0.94\n",
      "Training - i: 1848 Loss: 0.10558023 Accuracy: 0.95\n",
      "Training - i: 1849 Loss: 0.113629855 Accuracy: 0.955\n",
      "Training - i: 1850 Loss: 0.11619602 Accuracy: 0.965\n",
      "Training - i: 1851 Loss: 0.07422044 Accuracy: 0.985\n",
      "Validation - i: 1851  Accuracy: [0.885]\n",
      "Training - i: 1852 Loss: 0.059699688 Accuracy: 0.98\n",
      "Training - i: 1853 Loss: 0.12103018 Accuracy: 0.94\n",
      "Training - i: 1854 Loss: 0.105310954 Accuracy: 0.95\n",
      "Training - i: 1855 Loss: 0.11667761 Accuracy: 0.955\n",
      "Training - i: 1856 Loss: 0.09587639 Accuracy: 0.955\n",
      "Training - i: 1857 Loss: 0.096762985 Accuracy: 0.97\n",
      "Training - i: 1858 Loss: 0.11403391 Accuracy: 0.96\n",
      "Training - i: 1859 Loss: 0.08866583 Accuracy: 0.955\n",
      "Training - i: 1860 Loss: 0.1339 Accuracy: 0.94\n",
      "Training - i: 1861 Loss: 0.1930119 Accuracy: 0.925\n",
      "Training - i: 1862 Loss: 0.09165587 Accuracy: 0.965\n",
      "Training - i: 1863 Loss: 0.11084312 Accuracy: 0.96\n",
      "Training - i: 1864 Loss: 0.18644482 Accuracy: 0.935\n",
      "Training - i: 1865 Loss: 0.09604979 Accuracy: 0.96\n",
      "Training - i: 1866 Loss: 0.10730989 Accuracy: 0.96\n",
      "Training - i: 1867 Loss: 0.14737755 Accuracy: 0.925\n",
      "Training - i: 1868 Loss: 0.09681658 Accuracy: 0.96\n",
      "Training - i: 1869 Loss: 0.09528696 Accuracy: 0.96\n",
      "Training - i: 1870 Loss: 0.15947549 Accuracy: 0.92\n",
      "Training - i: 1871 Loss: 0.07958297 Accuracy: 0.975\n",
      "Training - i: 1872 Loss: 0.10239423 Accuracy: 0.95\n",
      "Training - i: 1873 Loss: 0.1597823 Accuracy: 0.93\n",
      "Training - i: 1874 Loss: 0.111538894 Accuracy: 0.93\n",
      "Training - i: 1875 Loss: 0.07299377 Accuracy: 0.975\n",
      "Training - i: 1876 Loss: 0.13263375 Accuracy: 0.945\n",
      "Training - i: 1877 Loss: 0.095247954 Accuracy: 0.955\n",
      "Training - i: 1878 Loss: 0.07713552 Accuracy: 0.97\n",
      "Training - i: 1879 Loss: 0.109021276 Accuracy: 0.965\n",
      "Training - i: 1880 Loss: 0.12146133 Accuracy: 0.95\n",
      "Training - i: 1881 Loss: 0.059628982 Accuracy: 0.975\n",
      "Training - i: 1882 Loss: 0.06426417 Accuracy: 0.97\n",
      "Training - i: 1883 Loss: 0.070837796 Accuracy: 0.98\n",
      "Training - i: 1884 Loss: 0.095592625 Accuracy: 0.955\n",
      "Training - i: 1885 Loss: 0.07896284 Accuracy: 0.985\n",
      "Training - i: 1886 Loss: 0.08014533 Accuracy: 0.96\n",
      "Training - i: 1887 Loss: 0.09861065 Accuracy: 0.96\n",
      "Training - i: 1888 Loss: 0.073353864 Accuracy: 0.97\n",
      "Training - i: 1889 Loss: 0.060861513 Accuracy: 0.98\n",
      "Training - i: 1890 Loss: 0.10818319 Accuracy: 0.96\n",
      "Training - i: 1891 Loss: 0.11170719 Accuracy: 0.96\n",
      "Training - i: 1892 Loss: 0.07021053 Accuracy: 0.98\n",
      "Training - i: 1893 Loss: 0.12107127 Accuracy: 0.935\n",
      "Training - i: 1894 Loss: 0.08285 Accuracy: 0.96\n",
      "Training - i: 1895 Loss: 0.069727056 Accuracy: 0.97\n",
      "Training - i: 1896 Loss: 0.073409624 Accuracy: 0.97\n",
      "Training - i: 1897 Loss: 0.07219857 Accuracy: 0.97\n",
      "Training - i: 1898 Loss: 0.103346884 Accuracy: 0.94\n",
      "Training - i: 1899 Loss: 0.09275732 Accuracy: 0.96\n",
      "Training - i: 1900 Loss: 0.06290447 Accuracy: 0.975\n",
      "Training - i: 1901 Loss: 0.10827508 Accuracy: 0.945\n",
      "Validation - i: 1901  Accuracy: [0.895]\n",
      "Training - i: 1902 Loss: 0.11159109 Accuracy: 0.945\n",
      "Training - i: 1903 Loss: 0.10598175 Accuracy: 0.955\n",
      "Training - i: 1904 Loss: 0.06912175 Accuracy: 0.97\n",
      "Training - i: 1905 Loss: 0.06565514 Accuracy: 0.975\n",
      "Training - i: 1906 Loss: 0.060133725 Accuracy: 0.985\n",
      "Training - i: 1907 Loss: 0.07264586 Accuracy: 0.97\n",
      "Training - i: 1908 Loss: 0.08607589 Accuracy: 0.965\n",
      "Training - i: 1909 Loss: 0.092019245 Accuracy: 0.97\n",
      "Training - i: 1910 Loss: 0.071370326 Accuracy: 0.97\n",
      "Training - i: 1911 Loss: 0.08006563 Accuracy: 0.955\n",
      "Training - i: 1912 Loss: 0.08765274 Accuracy: 0.96\n",
      "Training - i: 1913 Loss: 0.112096235 Accuracy: 0.95\n",
      "Training - i: 1914 Loss: 0.062065843 Accuracy: 0.975\n",
      "Training - i: 1915 Loss: 0.058452595 Accuracy: 0.985\n",
      "Training - i: 1916 Loss: 0.080827504 Accuracy: 0.97\n",
      "Training - i: 1917 Loss: 0.10068209 Accuracy: 0.95\n",
      "Training - i: 1918 Loss: 0.07527484 Accuracy: 0.965\n",
      "Training - i: 1919 Loss: 0.1117143 Accuracy: 0.945\n",
      "Training - i: 1920 Loss: 0.08000918 Accuracy: 0.965\n",
      "Training - i: 1921 Loss: 0.08286581 Accuracy: 0.965\n",
      "Training - i: 1922 Loss: 0.11890581 Accuracy: 0.945\n",
      "Training - i: 1923 Loss: 0.089740105 Accuracy: 0.965\n",
      "Training - i: 1924 Loss: 0.07166423 Accuracy: 0.98\n",
      "Training - i: 1925 Loss: 0.060441636 Accuracy: 0.97\n",
      "Training - i: 1926 Loss: 0.07656925 Accuracy: 0.955\n",
      "Training - i: 1927 Loss: 0.06774864 Accuracy: 0.975\n",
      "Training - i: 1928 Loss: 0.076955155 Accuracy: 0.975\n",
      "Training - i: 1929 Loss: 0.08642309 Accuracy: 0.97\n",
      "Training - i: 1930 Loss: 0.0710356 Accuracy: 0.98\n",
      "Training - i: 1931 Loss: 0.09358761 Accuracy: 0.96\n",
      "Training - i: 1932 Loss: 0.049340047 Accuracy: 0.985\n",
      "Training - i: 1933 Loss: 0.07236936 Accuracy: 0.975\n",
      "Training - i: 1934 Loss: 0.099824406 Accuracy: 0.96\n",
      "Training - i: 1935 Loss: 0.1354084 Accuracy: 0.94\n",
      "Training - i: 1936 Loss: 0.08872364 Accuracy: 0.965\n",
      "Training - i: 1937 Loss: 0.0488202 Accuracy: 0.975\n",
      "Training - i: 1938 Loss: 0.08372764 Accuracy: 0.975\n",
      "Training - i: 1939 Loss: 0.07593485 Accuracy: 0.98\n",
      "Training - i: 1940 Loss: 0.11334555 Accuracy: 0.945\n",
      "Training - i: 1941 Loss: 0.063419916 Accuracy: 0.985\n",
      "Training - i: 1942 Loss: 0.12358179 Accuracy: 0.94\n",
      "Training - i: 1943 Loss: 0.06576207 Accuracy: 0.99\n",
      "Training - i: 1944 Loss: 0.075409405 Accuracy: 0.965\n",
      "Training - i: 1945 Loss: 0.08053491 Accuracy: 0.96\n",
      "Training - i: 1946 Loss: 0.0620273 Accuracy: 0.99\n",
      "Training - i: 1947 Loss: 0.11535154 Accuracy: 0.955\n",
      "Training - i: 1948 Loss: 0.07157577 Accuracy: 0.98\n",
      "Training - i: 1949 Loss: 0.10679076 Accuracy: 0.945\n",
      "Training - i: 1950 Loss: 0.106342375 Accuracy: 0.935\n",
      "Training - i: 1951 Loss: 0.08943234 Accuracy: 0.965\n",
      "Validation - i: 1951  Accuracy: [0.9]\n",
      "Training - i: 1952 Loss: 0.06549423 Accuracy: 0.97\n",
      "Training - i: 1953 Loss: 0.124338776 Accuracy: 0.94\n",
      "Training - i: 1954 Loss: 0.123259574 Accuracy: 0.96\n",
      "Training - i: 1955 Loss: 0.1012937 Accuracy: 0.965\n",
      "Training - i: 1956 Loss: 0.06178989 Accuracy: 0.985\n",
      "Training - i: 1957 Loss: 0.077604696 Accuracy: 0.97\n",
      "Training - i: 1958 Loss: 0.08728708 Accuracy: 0.97\n",
      "Training - i: 1959 Loss: 0.07350978 Accuracy: 0.97\n",
      "Training - i: 1960 Loss: 0.08605293 Accuracy: 0.96\n",
      "Training - i: 1961 Loss: 0.09441877 Accuracy: 0.95\n",
      "Training - i: 1962 Loss: 0.085540965 Accuracy: 0.97\n",
      "Training - i: 1963 Loss: 0.098391876 Accuracy: 0.96\n",
      "Training - i: 1964 Loss: 0.07165282 Accuracy: 0.98\n",
      "Training - i: 1965 Loss: 0.05613865 Accuracy: 0.98\n",
      "Training - i: 1966 Loss: 0.058952667 Accuracy: 0.975\n",
      "Training - i: 1967 Loss: 0.090951845 Accuracy: 0.96\n",
      "Training - i: 1968 Loss: 0.124588355 Accuracy: 0.945\n",
      "Training - i: 1969 Loss: 0.08830322 Accuracy: 0.98\n",
      "Training - i: 1970 Loss: 0.06817073 Accuracy: 0.985\n",
      "Training - i: 1971 Loss: 0.08137562 Accuracy: 0.965\n",
      "Training - i: 1972 Loss: 0.0642921 Accuracy: 0.97\n",
      "Training - i: 1973 Loss: 0.11855419 Accuracy: 0.935\n",
      "Training - i: 1974 Loss: 0.14967698 Accuracy: 0.935\n",
      "Training - i: 1975 Loss: 0.1862818 Accuracy: 0.905\n",
      "Training - i: 1976 Loss: 0.08267666 Accuracy: 0.975\n",
      "Training - i: 1977 Loss: 0.18678421 Accuracy: 0.92\n",
      "Training - i: 1978 Loss: 0.12652554 Accuracy: 0.96\n",
      "Training - i: 1979 Loss: 0.12794755 Accuracy: 0.935\n",
      "Training - i: 1980 Loss: 0.11337559 Accuracy: 0.965\n",
      "Training - i: 1981 Loss: 0.09587628 Accuracy: 0.96\n",
      "Training - i: 1982 Loss: 0.09098888 Accuracy: 0.97\n",
      "Training - i: 1983 Loss: 0.06657484 Accuracy: 0.97\n",
      "Training - i: 1984 Loss: 0.09711147 Accuracy: 0.955\n",
      "Training - i: 1985 Loss: 0.09576144 Accuracy: 0.96\n",
      "Training - i: 1986 Loss: 0.19256435 Accuracy: 0.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - i: 1987 Loss: 0.12161089 Accuracy: 0.95\n",
      "Training - i: 1988 Loss: 0.11666233 Accuracy: 0.955\n",
      "Training - i: 1989 Loss: 0.13208571 Accuracy: 0.94\n",
      "Training - i: 1990 Loss: 0.076571524 Accuracy: 0.98\n",
      "Training - i: 1991 Loss: 0.16182919 Accuracy: 0.945\n",
      "Training - i: 1992 Loss: 0.14569402 Accuracy: 0.92\n",
      "Training - i: 1993 Loss: 0.09509305 Accuracy: 0.95\n",
      "Training - i: 1994 Loss: 0.18567792 Accuracy: 0.9\n",
      "Training - i: 1995 Loss: 0.15010679 Accuracy: 0.92\n",
      "Training - i: 1996 Loss: 0.07000489 Accuracy: 0.98\n",
      "Training - i: 1997 Loss: 0.09051609 Accuracy: 0.97\n",
      "Training - i: 1998 Loss: 0.08733603 Accuracy: 0.96\n",
      "Training - i: 1999 Loss: 0.09398523 Accuracy: 0.95\n",
      "Training - i: 2000 Loss: 0.07329348 Accuracy: 0.955\n",
      "Training - i: 2001 Loss: 0.090168454 Accuracy: 0.955\n",
      "Validation - i: 2001  Accuracy: [0.855]\n",
      "Training - i: 2002 Loss: 0.13898635 Accuracy: 0.92\n",
      "Training - i: 2003 Loss: 0.08302562 Accuracy: 0.97\n",
      "Training - i: 2004 Loss: 0.057868708 Accuracy: 0.985\n",
      "Training - i: 2005 Loss: 0.104681395 Accuracy: 0.965\n",
      "Training - i: 2006 Loss: 0.111944936 Accuracy: 0.955\n",
      "Training - i: 2007 Loss: 0.084393956 Accuracy: 0.955\n",
      "Training - i: 2008 Loss: 0.09236303 Accuracy: 0.965\n",
      "Training - i: 2009 Loss: 0.104911014 Accuracy: 0.96\n",
      "Training - i: 2010 Loss: 0.08911293 Accuracy: 0.96\n",
      "Training - i: 2011 Loss: 0.066725 Accuracy: 0.965\n",
      "Training - i: 2012 Loss: 0.06950958 Accuracy: 0.975\n",
      "Training - i: 2013 Loss: 0.06278087 Accuracy: 0.98\n",
      "Training - i: 2014 Loss: 0.13311502 Accuracy: 0.95\n",
      "Training - i: 2015 Loss: 0.10069584 Accuracy: 0.95\n",
      "Training - i: 2016 Loss: 0.07042967 Accuracy: 0.965\n",
      "Training - i: 2017 Loss: 0.08466233 Accuracy: 0.97\n",
      "Training - i: 2018 Loss: 0.07224061 Accuracy: 0.97\n",
      "Training - i: 2019 Loss: 0.049718037 Accuracy: 0.99\n",
      "Training - i: 2020 Loss: 0.060804702 Accuracy: 0.985\n",
      "Training - i: 2021 Loss: 0.075293735 Accuracy: 0.98\n",
      "Training - i: 2022 Loss: 0.056645937 Accuracy: 0.985\n",
      "Training - i: 2023 Loss: 0.08205431 Accuracy: 0.965\n",
      "Training - i: 2024 Loss: 0.06650735 Accuracy: 0.985\n",
      "Training - i: 2025 Loss: 0.05311035 Accuracy: 0.98\n",
      "Training - i: 2026 Loss: 0.056609068 Accuracy: 0.98\n",
      "Training - i: 2027 Loss: 0.087570846 Accuracy: 0.975\n",
      "Training - i: 2028 Loss: 0.043869086 Accuracy: 0.99\n",
      "Training - i: 2029 Loss: 0.05993746 Accuracy: 0.97\n",
      "Training - i: 2030 Loss: 0.068460226 Accuracy: 0.975\n",
      "Training - i: 2031 Loss: 0.08597496 Accuracy: 0.975\n",
      "Training - i: 2032 Loss: 0.054751005 Accuracy: 0.98\n",
      "Training - i: 2033 Loss: 0.09432474 Accuracy: 0.96\n",
      "Training - i: 2034 Loss: 0.08168641 Accuracy: 0.965\n",
      "Training - i: 2035 Loss: 0.10101362 Accuracy: 0.95\n",
      "Training - i: 2036 Loss: 0.0926656 Accuracy: 0.96\n",
      "Training - i: 2037 Loss: 0.05238279 Accuracy: 0.99\n",
      "Training - i: 2038 Loss: 0.08678903 Accuracy: 0.965\n",
      "Training - i: 2039 Loss: 0.060325503 Accuracy: 0.975\n",
      "Training - i: 2040 Loss: 0.070289925 Accuracy: 0.975\n",
      "Training - i: 2041 Loss: 0.06364885 Accuracy: 0.98\n",
      "Training - i: 2042 Loss: 0.08317529 Accuracy: 0.96\n",
      "Training - i: 2043 Loss: 0.0694271 Accuracy: 0.97\n",
      "Training - i: 2044 Loss: 0.05969054 Accuracy: 0.98\n",
      "Training - i: 2045 Loss: 0.08591304 Accuracy: 0.965\n",
      "Training - i: 2046 Loss: 0.060824066 Accuracy: 0.98\n",
      "Training - i: 2047 Loss: 0.08264116 Accuracy: 0.96\n",
      "Training - i: 2048 Loss: 0.07994842 Accuracy: 0.96\n",
      "Training - i: 2049 Loss: 0.068082854 Accuracy: 0.98\n",
      "Training - i: 2050 Loss: 0.053987417 Accuracy: 0.985\n",
      "Training - i: 2051 Loss: 0.08330085 Accuracy: 0.97\n",
      "Validation - i: 2051  Accuracy: [0.905]\n",
      "Training - i: 2052 Loss: 0.06848641 Accuracy: 0.965\n",
      "Training - i: 2053 Loss: 0.109849125 Accuracy: 0.95\n",
      "Training - i: 2054 Loss: 0.106754035 Accuracy: 0.955\n",
      "Training - i: 2055 Loss: 0.06165634 Accuracy: 0.97\n",
      "Training - i: 2056 Loss: 0.08129603 Accuracy: 0.97\n",
      "Training - i: 2057 Loss: 0.110803835 Accuracy: 0.94\n",
      "Training - i: 2058 Loss: 0.07001116 Accuracy: 0.97\n",
      "Training - i: 2059 Loss: 0.1215724 Accuracy: 0.96\n",
      "Training - i: 2060 Loss: 0.047732078 Accuracy: 0.985\n",
      "Training - i: 2061 Loss: 0.07535148 Accuracy: 0.985\n",
      "Training - i: 2062 Loss: 0.04977853 Accuracy: 0.98\n",
      "Training - i: 2063 Loss: 0.06475953 Accuracy: 0.98\n",
      "Training - i: 2064 Loss: 0.10945467 Accuracy: 0.94\n",
      "Training - i: 2065 Loss: 0.09775754 Accuracy: 0.95\n",
      "Training - i: 2066 Loss: 0.0830811 Accuracy: 0.96\n",
      "Training - i: 2067 Loss: 0.09107644 Accuracy: 0.955\n",
      "Training - i: 2068 Loss: 0.09083352 Accuracy: 0.965\n",
      "Training - i: 2069 Loss: 0.06323812 Accuracy: 0.965\n",
      "Training - i: 2070 Loss: 0.061584264 Accuracy: 0.975\n",
      "Training - i: 2071 Loss: 0.10883464 Accuracy: 0.945\n",
      "Training - i: 2072 Loss: 0.10109863 Accuracy: 0.965\n",
      "Training - i: 2073 Loss: 0.061212763 Accuracy: 0.975\n",
      "Training - i: 2074 Loss: 0.0922916 Accuracy: 0.96\n",
      "Training - i: 2075 Loss: 0.07069768 Accuracy: 0.97\n",
      "Training - i: 2076 Loss: 0.06381075 Accuracy: 0.965\n",
      "Training - i: 2077 Loss: 0.06619051 Accuracy: 0.97\n",
      "Training - i: 2078 Loss: 0.07391322 Accuracy: 0.97\n",
      "Training - i: 2079 Loss: 0.038799237 Accuracy: 0.995\n",
      "Training - i: 2080 Loss: 0.057423398 Accuracy: 0.985\n",
      "Training - i: 2081 Loss: 0.08588078 Accuracy: 0.96\n",
      "Training - i: 2082 Loss: 0.080006644 Accuracy: 0.975\n",
      "Training - i: 2083 Loss: 0.06837491 Accuracy: 0.97\n",
      "Training - i: 2084 Loss: 0.05647569 Accuracy: 0.98\n",
      "Training - i: 2085 Loss: 0.11223404 Accuracy: 0.95\n",
      "Training - i: 2086 Loss: 0.085696675 Accuracy: 0.955\n",
      "Training - i: 2087 Loss: 0.09062805 Accuracy: 0.965\n",
      "Training - i: 2088 Loss: 0.08148308 Accuracy: 0.965\n",
      "Training - i: 2089 Loss: 0.14132442 Accuracy: 0.935\n",
      "Training - i: 2090 Loss: 0.09037155 Accuracy: 0.965\n",
      "Training - i: 2091 Loss: 0.090345785 Accuracy: 0.96\n",
      "Training - i: 2092 Loss: 0.109097995 Accuracy: 0.955\n",
      "Training - i: 2093 Loss: 0.09253775 Accuracy: 0.96\n",
      "Training - i: 2094 Loss: 0.06644151 Accuracy: 0.97\n",
      "Training - i: 2095 Loss: 0.0558004 Accuracy: 0.99\n",
      "Training - i: 2096 Loss: 0.09745335 Accuracy: 0.95\n",
      "Training - i: 2097 Loss: 0.06646554 Accuracy: 0.975\n",
      "Training - i: 2098 Loss: 0.08321426 Accuracy: 0.975\n",
      "Training - i: 2099 Loss: 0.089230254 Accuracy: 0.965\n",
      "Training - i: 2100 Loss: 0.08490934 Accuracy: 0.975\n",
      "Training - i: 2101 Loss: 0.10698893 Accuracy: 0.96\n",
      "Validation - i: 2101  Accuracy: [0.925]\n",
      "Training - i: 2102 Loss: 0.11495281 Accuracy: 0.94\n",
      "Training - i: 2103 Loss: 0.08337926 Accuracy: 0.98\n",
      "Training - i: 2104 Loss: 0.09565217 Accuracy: 0.96\n",
      "Training - i: 2105 Loss: 0.08094199 Accuracy: 0.975\n",
      "Training - i: 2106 Loss: 0.13371272 Accuracy: 0.935\n",
      "Training - i: 2107 Loss: 0.09446991 Accuracy: 0.96\n",
      "Training - i: 2108 Loss: 0.14717612 Accuracy: 0.925\n",
      "Training - i: 2109 Loss: 0.048565656 Accuracy: 0.995\n",
      "Training - i: 2110 Loss: 0.11593843 Accuracy: 0.965\n",
      "Training - i: 2111 Loss: 0.10198742 Accuracy: 0.965\n",
      "Training - i: 2112 Loss: 0.06506115 Accuracy: 0.975\n",
      "Training - i: 2113 Loss: 0.09963621 Accuracy: 0.97\n",
      "Training - i: 2114 Loss: 0.16681515 Accuracy: 0.9\n",
      "Training - i: 2115 Loss: 0.110597506 Accuracy: 0.955\n",
      "Training - i: 2116 Loss: 0.12129269 Accuracy: 0.96\n",
      "Training - i: 2117 Loss: 0.084857464 Accuracy: 0.965\n",
      "Training - i: 2118 Loss: 0.13117646 Accuracy: 0.935\n",
      "Training - i: 2119 Loss: 0.09512867 Accuracy: 0.96\n",
      "Training - i: 2120 Loss: 0.10975027 Accuracy: 0.94\n",
      "Training - i: 2121 Loss: 0.15934408 Accuracy: 0.925\n",
      "Training - i: 2122 Loss: 0.119652346 Accuracy: 0.95\n",
      "Training - i: 2123 Loss: 0.13883156 Accuracy: 0.94\n",
      "Training - i: 2124 Loss: 0.11556272 Accuracy: 0.945\n",
      "Training - i: 2125 Loss: 0.10636427 Accuracy: 0.955\n",
      "Training - i: 2126 Loss: 0.075539805 Accuracy: 0.975\n",
      "Training - i: 2127 Loss: 0.18111521 Accuracy: 0.925\n",
      "Training - i: 2128 Loss: 0.098056205 Accuracy: 0.975\n",
      "Training - i: 2129 Loss: 0.08764038 Accuracy: 0.965\n",
      "Training - i: 2130 Loss: 0.076333106 Accuracy: 0.975\n",
      "Training - i: 2131 Loss: 0.13002744 Accuracy: 0.94\n",
      "Training - i: 2132 Loss: 0.075314045 Accuracy: 0.96\n",
      "Training - i: 2133 Loss: 0.10552707 Accuracy: 0.955\n",
      "Training - i: 2134 Loss: 0.18056113 Accuracy: 0.945\n",
      "Training - i: 2135 Loss: 0.11395966 Accuracy: 0.97\n",
      "Training - i: 2136 Loss: 0.10454584 Accuracy: 0.955\n",
      "Training - i: 2137 Loss: 0.10267049 Accuracy: 0.96\n",
      "Training - i: 2138 Loss: 0.07339835 Accuracy: 0.97\n",
      "Training - i: 2139 Loss: 0.10774341 Accuracy: 0.955\n",
      "Training - i: 2140 Loss: 0.109589726 Accuracy: 0.945\n",
      "Training - i: 2141 Loss: 0.12036709 Accuracy: 0.945\n",
      "Training - i: 2142 Loss: 0.13711813 Accuracy: 0.95\n",
      "Training - i: 2143 Loss: 0.10187895 Accuracy: 0.95\n",
      "Training - i: 2144 Loss: 0.100825645 Accuracy: 0.95\n",
      "Training - i: 2145 Loss: 0.08119764 Accuracy: 0.96\n",
      "Training - i: 2146 Loss: 0.12531273 Accuracy: 0.955\n",
      "Training - i: 2147 Loss: 0.08510269 Accuracy: 0.965\n",
      "Training - i: 2148 Loss: 0.16325268 Accuracy: 0.94\n",
      "Training - i: 2149 Loss: 0.1313336 Accuracy: 0.935\n",
      "Training - i: 2150 Loss: 0.10494247 Accuracy: 0.95\n",
      "Training - i: 2151 Loss: 0.08799443 Accuracy: 0.965\n",
      "Validation - i: 2151  Accuracy: [0.89]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - i: 2152 Loss: 0.100637734 Accuracy: 0.96\n",
      "Training - i: 2153 Loss: 0.0703787 Accuracy: 0.98\n",
      "Training - i: 2154 Loss: 0.08409346 Accuracy: 0.97\n",
      "Training - i: 2155 Loss: 0.068246774 Accuracy: 0.98\n",
      "Training - i: 2156 Loss: 0.10694196 Accuracy: 0.955\n",
      "Training - i: 2157 Loss: 0.12626858 Accuracy: 0.94\n",
      "Training - i: 2158 Loss: 0.102437824 Accuracy: 0.965\n",
      "Training - i: 2159 Loss: 0.07978815 Accuracy: 0.965\n",
      "Training - i: 2160 Loss: 0.06531805 Accuracy: 0.975\n",
      "Training - i: 2161 Loss: 0.09164894 Accuracy: 0.97\n",
      "Training - i: 2162 Loss: 0.07405135 Accuracy: 0.975\n",
      "Training - i: 2163 Loss: 0.091424376 Accuracy: 0.965\n",
      "Training - i: 2164 Loss: 0.11639727 Accuracy: 0.935\n",
      "Training - i: 2165 Loss: 0.06880664 Accuracy: 0.975\n",
      "Training - i: 2166 Loss: 0.10735304 Accuracy: 0.96\n",
      "Training - i: 2167 Loss: 0.081702344 Accuracy: 0.975\n",
      "Training - i: 2168 Loss: 0.09351183 Accuracy: 0.96\n",
      "Training - i: 2169 Loss: 0.07593234 Accuracy: 0.97\n",
      "Training - i: 2170 Loss: 0.08547883 Accuracy: 0.955\n",
      "Training - i: 2171 Loss: 0.11458625 Accuracy: 0.95\n",
      "Training - i: 2172 Loss: 0.07307595 Accuracy: 0.975\n",
      "Training - i: 2173 Loss: 0.104524806 Accuracy: 0.975\n",
      "Training - i: 2174 Loss: 0.073395275 Accuracy: 0.98\n",
      "Training - i: 2175 Loss: 0.084264345 Accuracy: 0.96\n",
      "Training - i: 2176 Loss: 0.103038825 Accuracy: 0.95\n",
      "Training - i: 2177 Loss: 0.10936403 Accuracy: 0.955\n",
      "Training - i: 2178 Loss: 0.052077904 Accuracy: 0.97\n",
      "Training - i: 2179 Loss: 0.10430721 Accuracy: 0.97\n",
      "Training - i: 2180 Loss: 0.088453844 Accuracy: 0.96\n",
      "Training - i: 2181 Loss: 0.059177503 Accuracy: 0.985\n",
      "Training - i: 2182 Loss: 0.0792882 Accuracy: 0.965\n",
      "Training - i: 2183 Loss: 0.14637819 Accuracy: 0.93\n",
      "Training - i: 2184 Loss: 0.079009145 Accuracy: 0.97\n",
      "Training - i: 2185 Loss: 0.09624853 Accuracy: 0.955\n",
      "Training - i: 2186 Loss: 0.07457542 Accuracy: 0.965\n",
      "Training - i: 2187 Loss: 0.051437996 Accuracy: 0.97\n",
      "Training - i: 2188 Loss: 0.07545152 Accuracy: 0.975\n",
      "Training - i: 2189 Loss: 0.06922971 Accuracy: 0.96\n",
      "Training - i: 2190 Loss: 0.07373401 Accuracy: 0.97\n",
      "Training - i: 2191 Loss: 0.07053498 Accuracy: 0.97\n",
      "Training - i: 2192 Loss: 0.078454606 Accuracy: 0.97\n",
      "Training - i: 2193 Loss: 0.050258473 Accuracy: 0.98\n",
      "Training - i: 2194 Loss: 0.0557057 Accuracy: 0.98\n",
      "Training - i: 2195 Loss: 0.074009076 Accuracy: 0.96\n",
      "Training - i: 2196 Loss: 0.06270929 Accuracy: 0.965\n",
      "Training - i: 2197 Loss: 0.067751065 Accuracy: 0.97\n",
      "Training - i: 2198 Loss: 0.085611895 Accuracy: 0.965\n",
      "Training - i: 2199 Loss: 0.08650906 Accuracy: 0.97\n",
      "Training - i: 2200 Loss: 0.08144533 Accuracy: 0.965\n",
      "Training - i: 2201 Loss: 0.061548587 Accuracy: 0.98\n",
      "Validation - i: 2201  Accuracy: [0.89]\n",
      "Training - i: 2202 Loss: 0.08793932 Accuracy: 0.96\n",
      "Training - i: 2203 Loss: 0.07119677 Accuracy: 0.965\n",
      "Training - i: 2204 Loss: 0.08416587 Accuracy: 0.965\n",
      "Training - i: 2205 Loss: 0.05427392 Accuracy: 0.98\n",
      "Training - i: 2206 Loss: 0.100928 Accuracy: 0.955\n",
      "Training - i: 2207 Loss: 0.096341476 Accuracy: 0.97\n",
      "Training - i: 2208 Loss: 0.11610699 Accuracy: 0.94\n",
      "Training - i: 2209 Loss: 0.115935564 Accuracy: 0.94\n",
      "Training - i: 2210 Loss: 0.050698623 Accuracy: 0.98\n",
      "Training - i: 2211 Loss: 0.14200824 Accuracy: 0.945\n",
      "Training - i: 2212 Loss: 0.12275906 Accuracy: 0.94\n",
      "Training - i: 2213 Loss: 0.07878716 Accuracy: 0.97\n",
      "Training - i: 2214 Loss: 0.1314073 Accuracy: 0.955\n",
      "Training - i: 2215 Loss: 0.11242754 Accuracy: 0.955\n",
      "Training - i: 2216 Loss: 0.088185064 Accuracy: 0.955\n",
      "Training - i: 2217 Loss: 0.10297686 Accuracy: 0.96\n",
      "Training - i: 2218 Loss: 0.18965435 Accuracy: 0.92\n",
      "Training - i: 2219 Loss: 0.07313366 Accuracy: 0.965\n",
      "Training - i: 2220 Loss: 0.09437373 Accuracy: 0.965\n",
      "Training - i: 2221 Loss: 0.10032125 Accuracy: 0.96\n",
      "Training - i: 2222 Loss: 0.077507265 Accuracy: 0.975\n",
      "Training - i: 2223 Loss: 0.1191638 Accuracy: 0.945\n",
      "Training - i: 2224 Loss: 0.084849596 Accuracy: 0.965\n",
      "Training - i: 2225 Loss: 0.10336937 Accuracy: 0.96\n",
      "Training - i: 2226 Loss: 0.06734293 Accuracy: 0.975\n",
      "Training - i: 2227 Loss: 0.08193632 Accuracy: 0.95\n",
      "Training - i: 2228 Loss: 0.08534219 Accuracy: 0.97\n",
      "Training - i: 2229 Loss: 0.061942503 Accuracy: 0.985\n",
      "Training - i: 2230 Loss: 0.10227886 Accuracy: 0.965\n",
      "Training - i: 2231 Loss: 0.08064274 Accuracy: 0.975\n",
      "Training - i: 2232 Loss: 0.058040068 Accuracy: 0.98\n",
      "Training - i: 2233 Loss: 0.082723886 Accuracy: 0.985\n",
      "Training - i: 2234 Loss: 0.07419939 Accuracy: 0.96\n",
      "Training - i: 2235 Loss: 0.12552841 Accuracy: 0.95\n",
      "Training - i: 2236 Loss: 0.058720645 Accuracy: 0.99\n",
      "Training - i: 2237 Loss: 0.051404513 Accuracy: 0.98\n",
      "Training - i: 2238 Loss: 0.09932402 Accuracy: 0.945\n",
      "Training - i: 2239 Loss: 0.07457958 Accuracy: 0.975\n",
      "Training - i: 2240 Loss: 0.09037616 Accuracy: 0.95\n",
      "Training - i: 2241 Loss: 0.09104254 Accuracy: 0.965\n",
      "Training - i: 2242 Loss: 0.07751407 Accuracy: 0.96\n",
      "Training - i: 2243 Loss: 0.056885365 Accuracy: 0.98\n",
      "Training - i: 2244 Loss: 0.053974606 Accuracy: 0.98\n",
      "Training - i: 2245 Loss: 0.068220876 Accuracy: 0.975\n",
      "Training - i: 2246 Loss: 0.07665284 Accuracy: 0.965\n",
      "Training - i: 2247 Loss: 0.049715575 Accuracy: 0.985\n",
      "Training - i: 2248 Loss: 0.09423141 Accuracy: 0.965\n",
      "Training - i: 2249 Loss: 0.08319143 Accuracy: 0.965\n",
      "Training - i: 2250 Loss: 0.0903089 Accuracy: 0.975\n",
      "Training - i: 2251 Loss: 0.04561903 Accuracy: 0.99\n",
      "Validation - i: 2251  Accuracy: [0.895]\n",
      "Training - i: 2252 Loss: 0.095243655 Accuracy: 0.96\n",
      "Training - i: 2253 Loss: 0.061105765 Accuracy: 0.985\n",
      "Training - i: 2254 Loss: 0.06845399 Accuracy: 0.97\n",
      "Training - i: 2255 Loss: 0.044933032 Accuracy: 0.99\n",
      "Training - i: 2256 Loss: 0.11135422 Accuracy: 0.945\n",
      "Training - i: 2257 Loss: 0.13187233 Accuracy: 0.945\n",
      "Training - i: 2258 Loss: 0.077075526 Accuracy: 0.965\n",
      "Training - i: 2259 Loss: 0.09457819 Accuracy: 0.96\n",
      "Training - i: 2260 Loss: 0.07340554 Accuracy: 0.97\n",
      "Training - i: 2261 Loss: 0.12107561 Accuracy: 0.93\n",
      "Training - i: 2262 Loss: 0.096186645 Accuracy: 0.965\n",
      "Training - i: 2263 Loss: 0.05129359 Accuracy: 0.995\n",
      "Training - i: 2264 Loss: 0.048158683 Accuracy: 0.98\n",
      "Training - i: 2265 Loss: 0.05202896 Accuracy: 0.995\n",
      "Training - i: 2266 Loss: 0.08191626 Accuracy: 0.965\n",
      "Training - i: 2267 Loss: 0.08406459 Accuracy: 0.96\n",
      "Training - i: 2268 Loss: 0.11006042 Accuracy: 0.95\n",
      "Training - i: 2269 Loss: 0.07142258 Accuracy: 0.98\n",
      "Training - i: 2270 Loss: 0.08947631 Accuracy: 0.96\n",
      "Training - i: 2271 Loss: 0.090601556 Accuracy: 0.965\n",
      "Training - i: 2272 Loss: 0.046819497 Accuracy: 0.98\n",
      "Training - i: 2273 Loss: 0.058909073 Accuracy: 0.975\n",
      "Training - i: 2274 Loss: 0.092669375 Accuracy: 0.965\n",
      "Training - i: 2275 Loss: 0.06446362 Accuracy: 0.97\n",
      "Training - i: 2276 Loss: 0.068896316 Accuracy: 0.965\n",
      "Training - i: 2277 Loss: 0.11082388 Accuracy: 0.95\n",
      "Training - i: 2278 Loss: 0.055538505 Accuracy: 0.975\n",
      "Training - i: 2279 Loss: 0.060947828 Accuracy: 0.98\n",
      "Training - i: 2280 Loss: 0.1062897 Accuracy: 0.95\n",
      "Training - i: 2281 Loss: 0.066710256 Accuracy: 0.97\n",
      "Training - i: 2282 Loss: 0.067579955 Accuracy: 0.97\n",
      "Training - i: 2283 Loss: 0.07937255 Accuracy: 0.965\n",
      "Training - i: 2284 Loss: 0.08650225 Accuracy: 0.94\n",
      "Training - i: 2285 Loss: 0.04150448 Accuracy: 0.99\n",
      "Training - i: 2286 Loss: 0.046068024 Accuracy: 0.98\n",
      "Training - i: 2287 Loss: 0.066356204 Accuracy: 0.98\n",
      "Training - i: 2288 Loss: 0.07138303 Accuracy: 0.965\n",
      "Training - i: 2289 Loss: 0.11193037 Accuracy: 0.965\n",
      "Training - i: 2290 Loss: 0.052662157 Accuracy: 0.985\n",
      "Training - i: 2291 Loss: 0.053842302 Accuracy: 0.98\n",
      "Training - i: 2292 Loss: 0.06125504 Accuracy: 0.975\n",
      "Training - i: 2293 Loss: 0.043097388 Accuracy: 0.985\n",
      "Training - i: 2294 Loss: 0.078723766 Accuracy: 0.96\n",
      "Training - i: 2295 Loss: 0.0694516 Accuracy: 0.97\n",
      "Training - i: 2296 Loss: 0.04290824 Accuracy: 0.98\n",
      "Training - i: 2297 Loss: 0.042549692 Accuracy: 0.985\n",
      "Training - i: 2298 Loss: 0.0953497 Accuracy: 0.96\n",
      "Training - i: 2299 Loss: 0.0308773 Accuracy: 1.0\n",
      "Training - i: 2300 Loss: 0.06517219 Accuracy: 0.975\n",
      "Training - i: 2301 Loss: 0.06985126 Accuracy: 0.97\n",
      "Validation - i: 2301  Accuracy: [0.915]\n",
      "Training - i: 2302 Loss: 0.060690805 Accuracy: 0.97\n",
      "Training - i: 2303 Loss: 0.0795274 Accuracy: 0.945\n",
      "Training - i: 2304 Loss: 0.08448723 Accuracy: 0.955\n",
      "Training - i: 2305 Loss: 0.04455317 Accuracy: 0.985\n",
      "Training - i: 2306 Loss: 0.08360066 Accuracy: 0.955\n",
      "Training - i: 2307 Loss: 0.035003338 Accuracy: 0.985\n",
      "Training - i: 2308 Loss: 0.066157654 Accuracy: 0.97\n",
      "Training - i: 2309 Loss: 0.035856985 Accuracy: 0.98\n",
      "Training - i: 2310 Loss: 0.061345473 Accuracy: 0.965\n",
      "Training - i: 2311 Loss: 0.052387174 Accuracy: 0.975\n",
      "Training - i: 2312 Loss: 0.046502814 Accuracy: 0.98\n",
      "Training - i: 2313 Loss: 0.057366177 Accuracy: 0.98\n",
      "Training - i: 2314 Loss: 0.06398224 Accuracy: 0.975\n",
      "Training - i: 2315 Loss: 0.05604174 Accuracy: 0.965\n",
      "Training - i: 2316 Loss: 0.075062856 Accuracy: 0.975\n",
      "Training - i: 2317 Loss: 0.046407074 Accuracy: 0.98\n",
      "Training - i: 2318 Loss: 0.06844218 Accuracy: 0.97\n",
      "Training - i: 2319 Loss: 0.05229177 Accuracy: 0.985\n",
      "Training - i: 2320 Loss: 0.05946309 Accuracy: 0.97\n",
      "Training - i: 2321 Loss: 0.08124899 Accuracy: 0.965\n",
      "Training - i: 2322 Loss: 0.040464133 Accuracy: 0.985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - i: 2323 Loss: 0.057027988 Accuracy: 0.98\n",
      "Training - i: 2324 Loss: 0.054284297 Accuracy: 0.98\n",
      "Training - i: 2325 Loss: 0.0673471 Accuracy: 0.97\n",
      "Training - i: 2326 Loss: 0.07521184 Accuracy: 0.97\n",
      "Training - i: 2327 Loss: 0.04430933 Accuracy: 0.99\n",
      "Training - i: 2328 Loss: 0.067303605 Accuracy: 0.975\n",
      "Training - i: 2329 Loss: 0.0916249 Accuracy: 0.95\n",
      "Training - i: 2330 Loss: 0.033273216 Accuracy: 0.995\n",
      "Training - i: 2331 Loss: 0.04877286 Accuracy: 0.98\n",
      "Training - i: 2332 Loss: 0.047022544 Accuracy: 0.98\n",
      "Training - i: 2333 Loss: 0.061422262 Accuracy: 0.97\n",
      "Training - i: 2334 Loss: 0.06067094 Accuracy: 0.97\n",
      "Training - i: 2335 Loss: 0.12686974 Accuracy: 0.95\n",
      "Training - i: 2336 Loss: 0.046262633 Accuracy: 0.99\n",
      "Training - i: 2337 Loss: 0.06362248 Accuracy: 0.975\n",
      "Training - i: 2338 Loss: 0.038744878 Accuracy: 0.99\n",
      "Training - i: 2339 Loss: 0.07346422 Accuracy: 0.965\n",
      "Training - i: 2340 Loss: 0.078418024 Accuracy: 0.95\n",
      "Training - i: 2341 Loss: 0.05474741 Accuracy: 0.97\n",
      "Training - i: 2342 Loss: 0.07347066 Accuracy: 0.97\n",
      "Training - i: 2343 Loss: 0.07389624 Accuracy: 0.97\n",
      "Training - i: 2344 Loss: 0.039364845 Accuracy: 0.995\n",
      "Training - i: 2345 Loss: 0.08890615 Accuracy: 0.96\n",
      "Training - i: 2346 Loss: 0.07170377 Accuracy: 0.965\n",
      "Training - i: 2347 Loss: 0.07576704 Accuracy: 0.97\n",
      "Training - i: 2348 Loss: 0.04315576 Accuracy: 0.98\n",
      "Training - i: 2349 Loss: 0.08860104 Accuracy: 0.97\n",
      "Training - i: 2350 Loss: 0.03183119 Accuracy: 0.99\n",
      "Training - i: 2351 Loss: 0.05401405 Accuracy: 0.98\n",
      "Validation - i: 2351  Accuracy: [0.91]\n",
      "Training - i: 2352 Loss: 0.07143542 Accuracy: 0.97\n",
      "Training - i: 2353 Loss: 0.054835018 Accuracy: 0.985\n",
      "Training - i: 2354 Loss: 0.04719418 Accuracy: 0.98\n",
      "Training - i: 2355 Loss: 0.12506929 Accuracy: 0.955\n",
      "Training - i: 2356 Loss: 0.05323877 Accuracy: 0.975\n",
      "Training - i: 2357 Loss: 0.06993439 Accuracy: 0.975\n",
      "Training - i: 2358 Loss: 0.073956706 Accuracy: 0.965\n",
      "Training - i: 2359 Loss: 0.04227785 Accuracy: 0.99\n",
      "Training - i: 2360 Loss: 0.0690752 Accuracy: 0.965\n",
      "Training - i: 2361 Loss: 0.0471993 Accuracy: 0.985\n",
      "Training - i: 2362 Loss: 0.088338286 Accuracy: 0.95\n",
      "Training - i: 2363 Loss: 0.07054469 Accuracy: 0.97\n",
      "Training - i: 2364 Loss: 0.060732592 Accuracy: 0.97\n",
      "Training - i: 2365 Loss: 0.06803919 Accuracy: 0.97\n",
      "Training - i: 2366 Loss: 0.048820905 Accuracy: 0.99\n",
      "Training - i: 2367 Loss: 0.07139525 Accuracy: 0.965\n",
      "Training - i: 2368 Loss: 0.055712882 Accuracy: 0.985\n",
      "Training - i: 2369 Loss: 0.056270663 Accuracy: 0.975\n",
      "Training - i: 2370 Loss: 0.058973216 Accuracy: 0.975\n",
      "Training - i: 2371 Loss: 0.050976854 Accuracy: 0.985\n",
      "Training - i: 2372 Loss: 0.060663424 Accuracy: 0.97\n",
      "Training - i: 2373 Loss: 0.052255634 Accuracy: 0.985\n",
      "Training - i: 2374 Loss: 0.03374158 Accuracy: 0.99\n",
      "Training - i: 2375 Loss: 0.10701678 Accuracy: 0.96\n",
      "Training - i: 2376 Loss: 0.066318884 Accuracy: 0.975\n",
      "Training - i: 2377 Loss: 0.079677165 Accuracy: 0.945\n",
      "Training - i: 2378 Loss: 0.057059713 Accuracy: 0.98\n",
      "Training - i: 2379 Loss: 0.06288539 Accuracy: 0.965\n",
      "Training - i: 2380 Loss: 0.056266524 Accuracy: 0.975\n",
      "Training - i: 2381 Loss: 0.06397176 Accuracy: 0.975\n",
      "Training - i: 2382 Loss: 0.08998238 Accuracy: 0.97\n",
      "Training - i: 2383 Loss: 0.025864625 Accuracy: 0.995\n",
      "Training - i: 2384 Loss: 0.07620572 Accuracy: 0.955\n",
      "Training - i: 2385 Loss: 0.07693355 Accuracy: 0.965\n",
      "Training - i: 2386 Loss: 0.07884263 Accuracy: 0.97\n",
      "Training - i: 2387 Loss: 0.06288261 Accuracy: 0.97\n",
      "Training - i: 2388 Loss: 0.016182018 Accuracy: 1.0\n",
      "Training - i: 2389 Loss: 0.04682899 Accuracy: 0.98\n",
      "Training - i: 2390 Loss: 0.076465316 Accuracy: 0.965\n",
      "Training - i: 2391 Loss: 0.048863754 Accuracy: 0.98\n",
      "Training - i: 2392 Loss: 0.089370586 Accuracy: 0.97\n",
      "Training - i: 2393 Loss: 0.06763832 Accuracy: 0.965\n",
      "Training - i: 2394 Loss: 0.051504675 Accuracy: 0.97\n",
      "Training - i: 2395 Loss: 0.056572475 Accuracy: 0.98\n",
      "Training - i: 2396 Loss: 0.071927175 Accuracy: 0.97\n",
      "Training - i: 2397 Loss: 0.08117481 Accuracy: 0.955\n",
      "Training - i: 2398 Loss: 0.04960826 Accuracy: 0.985\n",
      "Training - i: 2399 Loss: 0.0597489 Accuracy: 0.99\n",
      "Training - i: 2400 Loss: 0.08060922 Accuracy: 0.975\n",
      "Training - i: 2401 Loss: 0.05761942 Accuracy: 0.98\n",
      "Validation - i: 2401  Accuracy: [0.88]\n",
      "Training - i: 2402 Loss: 0.07639896 Accuracy: 0.97\n",
      "Training - i: 2403 Loss: 0.05531596 Accuracy: 0.975\n",
      "Training - i: 2404 Loss: 0.10402604 Accuracy: 0.95\n",
      "Training - i: 2405 Loss: 0.069170244 Accuracy: 0.97\n",
      "Training - i: 2406 Loss: 0.08889991 Accuracy: 0.975\n",
      "Training - i: 2407 Loss: 0.08392745 Accuracy: 0.965\n",
      "Training - i: 2408 Loss: 0.051537815 Accuracy: 0.975\n",
      "Training - i: 2409 Loss: 0.07194627 Accuracy: 0.96\n",
      "Training - i: 2410 Loss: 0.0760289 Accuracy: 0.965\n",
      "Training - i: 2411 Loss: 0.06425344 Accuracy: 0.96\n",
      "Training - i: 2412 Loss: 0.034343485 Accuracy: 0.995\n",
      "Training - i: 2413 Loss: 0.055188064 Accuracy: 0.985\n",
      "Training - i: 2414 Loss: 0.05400777 Accuracy: 0.985\n",
      "Training - i: 2415 Loss: 0.06377262 Accuracy: 0.975\n",
      "Training - i: 2416 Loss: 0.06973036 Accuracy: 0.98\n",
      "Training - i: 2417 Loss: 0.058329254 Accuracy: 0.97\n",
      "Training - i: 2418 Loss: 0.053797517 Accuracy: 0.98\n",
      "Training - i: 2419 Loss: 0.07185498 Accuracy: 0.965\n",
      "Training - i: 2420 Loss: 0.058387004 Accuracy: 0.97\n",
      "Training - i: 2421 Loss: 0.050110508 Accuracy: 0.985\n",
      "Training - i: 2422 Loss: 0.07092925 Accuracy: 0.97\n",
      "Training - i: 2423 Loss: 0.07464232 Accuracy: 0.965\n",
      "Training - i: 2424 Loss: 0.08031476 Accuracy: 0.965\n",
      "Training - i: 2425 Loss: 0.07273012 Accuracy: 0.97\n",
      "Training - i: 2426 Loss: 0.075045735 Accuracy: 0.97\n",
      "Training - i: 2427 Loss: 0.060954448 Accuracy: 0.985\n",
      "Training - i: 2428 Loss: 0.05687313 Accuracy: 0.965\n",
      "Training - i: 2429 Loss: 0.07577909 Accuracy: 0.97\n",
      "Training - i: 2430 Loss: 0.07262771 Accuracy: 0.965\n",
      "Training - i: 2431 Loss: 0.06310214 Accuracy: 0.97\n",
      "Training - i: 2432 Loss: 0.08163293 Accuracy: 0.96\n",
      "Training - i: 2433 Loss: 0.080895945 Accuracy: 0.96\n",
      "Training - i: 2434 Loss: 0.09352619 Accuracy: 0.96\n",
      "Training - i: 2435 Loss: 0.066475414 Accuracy: 0.97\n",
      "Training - i: 2436 Loss: 0.07055242 Accuracy: 0.97\n",
      "Training - i: 2437 Loss: 0.06312403 Accuracy: 0.975\n",
      "Training - i: 2438 Loss: 0.043308757 Accuracy: 0.985\n",
      "Training - i: 2439 Loss: 0.091535665 Accuracy: 0.97\n",
      "Training - i: 2440 Loss: 0.0674384 Accuracy: 0.97\n",
      "Training - i: 2441 Loss: 0.07960233 Accuracy: 0.97\n",
      "Training - i: 2442 Loss: 0.11643369 Accuracy: 0.955\n",
      "Training - i: 2443 Loss: 0.04228686 Accuracy: 0.98\n",
      "Training - i: 2444 Loss: 0.042696822 Accuracy: 0.985\n",
      "Training - i: 2445 Loss: 0.06435538 Accuracy: 0.975\n",
      "Training - i: 2446 Loss: 0.08689556 Accuracy: 0.965\n",
      "Training - i: 2447 Loss: 0.050751716 Accuracy: 0.985\n",
      "Training - i: 2448 Loss: 0.08352106 Accuracy: 0.97\n",
      "Training - i: 2449 Loss: 0.073447496 Accuracy: 0.955\n",
      "Training - i: 2450 Loss: 0.07920985 Accuracy: 0.97\n",
      "Training - i: 2451 Loss: 0.08673767 Accuracy: 0.965\n",
      "Validation - i: 2451  Accuracy: [0.925]\n",
      "Training - i: 2452 Loss: 0.051523715 Accuracy: 0.98\n",
      "Training - i: 2453 Loss: 0.07417897 Accuracy: 0.97\n",
      "Training - i: 2454 Loss: 0.10016653 Accuracy: 0.955\n",
      "Training - i: 2455 Loss: 0.08251288 Accuracy: 0.97\n",
      "Training - i: 2456 Loss: 0.085681856 Accuracy: 0.97\n",
      "Training - i: 2457 Loss: 0.061404806 Accuracy: 0.97\n",
      "Training - i: 2458 Loss: 0.0503267 Accuracy: 0.99\n",
      "Training - i: 2459 Loss: 0.09877779 Accuracy: 0.95\n",
      "Training - i: 2460 Loss: 0.07260108 Accuracy: 0.975\n",
      "Training - i: 2461 Loss: 0.08833608 Accuracy: 0.96\n",
      "Training - i: 2462 Loss: 0.07087442 Accuracy: 0.975\n",
      "Training - i: 2463 Loss: 0.07581028 Accuracy: 0.985\n",
      "Training - i: 2464 Loss: 0.1191307 Accuracy: 0.95\n",
      "Training - i: 2465 Loss: 0.08677302 Accuracy: 0.97\n",
      "Training - i: 2466 Loss: 0.10355898 Accuracy: 0.965\n",
      "Training - i: 2467 Loss: 0.084796965 Accuracy: 0.96\n",
      "Training - i: 2468 Loss: 0.088417076 Accuracy: 0.97\n",
      "Training - i: 2469 Loss: 0.081096955 Accuracy: 0.965\n",
      "Training - i: 2470 Loss: 0.1076687 Accuracy: 0.96\n",
      "Training - i: 2471 Loss: 0.055369433 Accuracy: 0.99\n",
      "Training - i: 2472 Loss: 0.055123277 Accuracy: 0.985\n",
      "Training - i: 2473 Loss: 0.07500803 Accuracy: 0.965\n",
      "Training - i: 2474 Loss: 0.07560697 Accuracy: 0.965\n",
      "Training - i: 2475 Loss: 0.036571622 Accuracy: 0.995\n",
      "Training - i: 2476 Loss: 0.08096203 Accuracy: 0.965\n",
      "Training - i: 2477 Loss: 0.0692943 Accuracy: 0.965\n",
      "Training - i: 2478 Loss: 0.14073397 Accuracy: 0.94\n",
      "Training - i: 2479 Loss: 0.09302135 Accuracy: 0.96\n",
      "Training - i: 2480 Loss: 0.04444582 Accuracy: 0.975\n",
      "Training - i: 2481 Loss: 0.0666883 Accuracy: 0.965\n",
      "Training - i: 2482 Loss: 0.12102942 Accuracy: 0.945\n",
      "Training - i: 2483 Loss: 0.067890845 Accuracy: 0.965\n",
      "Training - i: 2484 Loss: 0.059941407 Accuracy: 0.98\n",
      "Training - i: 2485 Loss: 0.043090038 Accuracy: 0.99\n",
      "Training - i: 2486 Loss: 0.046127096 Accuracy: 0.98\n",
      "Training - i: 2487 Loss: 0.06530058 Accuracy: 0.97\n",
      "Training - i: 2488 Loss: 0.058651056 Accuracy: 0.975\n",
      "Training - i: 2489 Loss: 0.05729946 Accuracy: 0.975\n",
      "Training - i: 2490 Loss: 0.09976218 Accuracy: 0.96\n",
      "Training - i: 2491 Loss: 0.11062746 Accuracy: 0.94\n",
      "Training - i: 2492 Loss: 0.0568999 Accuracy: 0.985\n",
      "Training - i: 2493 Loss: 0.069442466 Accuracy: 0.965\n",
      "Training - i: 2494 Loss: 0.071123846 Accuracy: 0.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - i: 2495 Loss: 0.07205653 Accuracy: 0.97\n",
      "Training - i: 2496 Loss: 0.045909982 Accuracy: 0.985\n",
      "Training - i: 2497 Loss: 0.08301763 Accuracy: 0.965\n",
      "Training - i: 2498 Loss: 0.09424553 Accuracy: 0.95\n",
      "Training - i: 2499 Loss: 0.058667604 Accuracy: 0.975\n",
      "Training - i: 2500 Loss: 0.06771775 Accuracy: 0.96\n",
      "Training - i: 2501 Loss: 0.047848433 Accuracy: 0.985\n",
      "Validation - i: 2501  Accuracy: [0.905]\n",
      "Training - i: 2502 Loss: 0.14573498 Accuracy: 0.93\n",
      "Training - i: 2503 Loss: 0.050269447 Accuracy: 0.99\n",
      "Training - i: 2504 Loss: 0.10581978 Accuracy: 0.94\n",
      "Training - i: 2505 Loss: 0.022125497 Accuracy: 1.0\n",
      "Training - i: 2506 Loss: 0.0920718 Accuracy: 0.95\n",
      "Training - i: 2507 Loss: 0.09076008 Accuracy: 0.965\n",
      "Training - i: 2508 Loss: 0.08000234 Accuracy: 0.98\n",
      "Training - i: 2509 Loss: 0.09155284 Accuracy: 0.96\n",
      "Training - i: 2510 Loss: 0.063246205 Accuracy: 0.965\n",
      "Training - i: 2511 Loss: 0.07904118 Accuracy: 0.965\n",
      "Training - i: 2512 Loss: 0.065322585 Accuracy: 0.98\n",
      "Training - i: 2513 Loss: 0.087489255 Accuracy: 0.965\n",
      "Training - i: 2514 Loss: 0.050540596 Accuracy: 0.975\n",
      "Training - i: 2515 Loss: 0.102720775 Accuracy: 0.95\n",
      "Training - i: 2516 Loss: 0.07911504 Accuracy: 0.97\n",
      "Training - i: 2517 Loss: 0.07382852 Accuracy: 0.965\n",
      "Training - i: 2518 Loss: 0.11447175 Accuracy: 0.94\n",
      "Training - i: 2519 Loss: 0.10713432 Accuracy: 0.96\n",
      "Training - i: 2520 Loss: 0.068850584 Accuracy: 0.98\n",
      "Training - i: 2521 Loss: 0.03908258 Accuracy: 0.99\n",
      "Training - i: 2522 Loss: 0.07079228 Accuracy: 0.965\n",
      "Training - i: 2523 Loss: 0.07571866 Accuracy: 0.975\n",
      "Training - i: 2524 Loss: 0.08037798 Accuracy: 0.97\n",
      "Training - i: 2525 Loss: 0.092376 Accuracy: 0.955\n",
      "Training - i: 2526 Loss: 0.1032852 Accuracy: 0.95\n",
      "Training - i: 2527 Loss: 0.051748276 Accuracy: 0.975\n",
      "Training - i: 2528 Loss: 0.049825158 Accuracy: 0.985\n",
      "Training - i: 2529 Loss: 0.048139945 Accuracy: 0.98\n",
      "Training - i: 2530 Loss: 0.08301216 Accuracy: 0.955\n",
      "Training - i: 2531 Loss: 0.080305785 Accuracy: 0.945\n",
      "Training - i: 2532 Loss: 0.061993554 Accuracy: 0.975\n",
      "Training - i: 2533 Loss: 0.04098768 Accuracy: 0.98\n",
      "Training - i: 2534 Loss: 0.07831264 Accuracy: 0.965\n",
      "Training - i: 2535 Loss: 0.052985713 Accuracy: 0.975\n",
      "Training - i: 2536 Loss: 0.08279371 Accuracy: 0.965\n",
      "Training - i: 2537 Loss: 0.051376257 Accuracy: 0.98\n",
      "Training - i: 2538 Loss: 0.06421495 Accuracy: 0.98\n",
      "Training - i: 2539 Loss: 0.06121896 Accuracy: 0.97\n",
      "Training - i: 2540 Loss: 0.05479044 Accuracy: 0.975\n",
      "Training - i: 2541 Loss: 0.07363462 Accuracy: 0.98\n",
      "Training - i: 2542 Loss: 0.06677853 Accuracy: 0.98\n",
      "Training - i: 2543 Loss: 0.06264142 Accuracy: 0.98\n",
      "Training - i: 2544 Loss: 0.076576784 Accuracy: 0.97\n",
      "Training - i: 2545 Loss: 0.057095386 Accuracy: 0.985\n",
      "Training - i: 2546 Loss: 0.058289506 Accuracy: 0.98\n",
      "Training - i: 2547 Loss: 0.06022711 Accuracy: 0.99\n",
      "Training - i: 2548 Loss: 0.07780007 Accuracy: 0.965\n",
      "Training - i: 2549 Loss: 0.08779999 Accuracy: 0.965\n",
      "Training - i: 2550 Loss: 0.035629097 Accuracy: 0.99\n",
      "Training - i: 2551 Loss: 0.057734337 Accuracy: 0.975\n",
      "Validation - i: 2551  Accuracy: [0.9]\n",
      "Training - i: 2552 Loss: 0.04998463 Accuracy: 0.985\n",
      "Training - i: 2553 Loss: 0.05933913 Accuracy: 0.98\n",
      "Training - i: 2554 Loss: 0.059857167 Accuracy: 0.975\n",
      "Training - i: 2555 Loss: 0.07453245 Accuracy: 0.955\n",
      "Training - i: 2556 Loss: 0.068123855 Accuracy: 0.965\n",
      "Training - i: 2557 Loss: 0.06924608 Accuracy: 0.965\n",
      "Training - i: 2558 Loss: 0.06163539 Accuracy: 0.97\n",
      "Training - i: 2559 Loss: 0.04232254 Accuracy: 0.98\n",
      "Training - i: 2560 Loss: 0.048572388 Accuracy: 0.985\n",
      "Training - i: 2561 Loss: 0.097390264 Accuracy: 0.96\n",
      "Training - i: 2562 Loss: 0.0424683 Accuracy: 0.98\n",
      "Training - i: 2563 Loss: 0.08579415 Accuracy: 0.96\n",
      "Training - i: 2564 Loss: 0.059777588 Accuracy: 0.975\n",
      "Training - i: 2565 Loss: 0.0774308 Accuracy: 0.965\n",
      "Training - i: 2566 Loss: 0.06084898 Accuracy: 0.97\n",
      "Training - i: 2567 Loss: 0.0792562 Accuracy: 0.97\n",
      "Training - i: 2568 Loss: 0.059423447 Accuracy: 0.97\n",
      "Training - i: 2569 Loss: 0.057509728 Accuracy: 0.975\n",
      "Training - i: 2570 Loss: 0.06580582 Accuracy: 0.965\n",
      "Training - i: 2571 Loss: 0.08263764 Accuracy: 0.965\n",
      "Training - i: 2572 Loss: 0.0685617 Accuracy: 0.97\n",
      "Training - i: 2573 Loss: 0.06734839 Accuracy: 0.975\n",
      "Training - i: 2574 Loss: 0.050764266 Accuracy: 0.98\n",
      "Training - i: 2575 Loss: 0.07791039 Accuracy: 0.965\n",
      "Training - i: 2576 Loss: 0.028766299 Accuracy: 0.99\n",
      "Training - i: 2577 Loss: 0.058858603 Accuracy: 0.965\n",
      "Training - i: 2578 Loss: 0.048199806 Accuracy: 0.98\n",
      "Training - i: 2579 Loss: 0.054691676 Accuracy: 0.985\n",
      "Training - i: 2580 Loss: 0.033940066 Accuracy: 0.995\n",
      "Training - i: 2581 Loss: 0.05126766 Accuracy: 0.98\n",
      "Training - i: 2582 Loss: 0.09743775 Accuracy: 0.96\n",
      "Training - i: 2583 Loss: 0.063949 Accuracy: 0.975\n",
      "Training - i: 2584 Loss: 0.06390111 Accuracy: 0.965\n",
      "Training - i: 2585 Loss: 0.08525243 Accuracy: 0.955\n",
      "Training - i: 2586 Loss: 0.08252821 Accuracy: 0.945\n",
      "Training - i: 2587 Loss: 0.057091665 Accuracy: 0.98\n",
      "Training - i: 2588 Loss: 0.050093517 Accuracy: 0.985\n",
      "Training - i: 2589 Loss: 0.08749813 Accuracy: 0.945\n",
      "Training - i: 2590 Loss: 0.057928067 Accuracy: 0.97\n",
      "Training - i: 2591 Loss: 0.08917822 Accuracy: 0.965\n",
      "Training - i: 2592 Loss: 0.065280735 Accuracy: 0.97\n",
      "Training - i: 2593 Loss: 0.04725634 Accuracy: 0.99\n",
      "Training - i: 2594 Loss: 0.08973685 Accuracy: 0.96\n",
      "Training - i: 2595 Loss: 0.05482357 Accuracy: 0.985\n",
      "Training - i: 2596 Loss: 0.075867474 Accuracy: 0.965\n",
      "Training - i: 2597 Loss: 0.061365116 Accuracy: 0.975\n",
      "Training - i: 2598 Loss: 0.057814687 Accuracy: 0.97\n",
      "Training - i: 2599 Loss: 0.067900866 Accuracy: 0.97\n",
      "Training - i: 2600 Loss: 0.05831604 Accuracy: 0.98\n",
      "Training - i: 2601 Loss: 0.08174324 Accuracy: 0.965\n",
      "Validation - i: 2601  Accuracy: [0.9]\n",
      "Training - i: 2602 Loss: 0.07009826 Accuracy: 0.965\n",
      "Training - i: 2603 Loss: 0.08388539 Accuracy: 0.95\n",
      "Training - i: 2604 Loss: 0.06796184 Accuracy: 0.97\n",
      "Training - i: 2605 Loss: 0.0840202 Accuracy: 0.96\n",
      "Training - i: 2606 Loss: 0.0380366 Accuracy: 0.995\n",
      "Training - i: 2607 Loss: 0.07877041 Accuracy: 0.97\n",
      "Training - i: 2608 Loss: 0.042896844 Accuracy: 0.985\n",
      "Training - i: 2609 Loss: 0.045207176 Accuracy: 0.985\n",
      "Training - i: 2610 Loss: 0.06982901 Accuracy: 0.965\n",
      "Training - i: 2611 Loss: 0.061561298 Accuracy: 0.97\n",
      "Training - i: 2612 Loss: 0.047069624 Accuracy: 0.98\n",
      "Training - i: 2613 Loss: 0.052014712 Accuracy: 0.975\n",
      "Training - i: 2614 Loss: 0.04378273 Accuracy: 0.99\n",
      "Training - i: 2615 Loss: 0.03692301 Accuracy: 0.985\n",
      "Training - i: 2616 Loss: 0.047004223 Accuracy: 0.985\n",
      "Training - i: 2617 Loss: 0.044187564 Accuracy: 0.985\n",
      "Training - i: 2618 Loss: 0.042353585 Accuracy: 0.985\n",
      "Training - i: 2619 Loss: 0.051707696 Accuracy: 0.98\n",
      "Training - i: 2620 Loss: 0.03506325 Accuracy: 0.995\n",
      "Training - i: 2621 Loss: 0.07249218 Accuracy: 0.975\n",
      "Training - i: 2622 Loss: 0.047270432 Accuracy: 0.985\n",
      "Training - i: 2623 Loss: 0.06918131 Accuracy: 0.975\n",
      "Training - i: 2624 Loss: 0.05692405 Accuracy: 0.985\n",
      "Training - i: 2625 Loss: 0.09063359 Accuracy: 0.96\n",
      "Training - i: 2626 Loss: 0.064514376 Accuracy: 0.97\n",
      "Training - i: 2627 Loss: 0.025453944 Accuracy: 0.995\n",
      "Training - i: 2628 Loss: 0.051624224 Accuracy: 0.985\n",
      "Training - i: 2629 Loss: 0.062352486 Accuracy: 0.98\n",
      "Training - i: 2630 Loss: 0.08158026 Accuracy: 0.96\n",
      "Training - i: 2631 Loss: 0.057638817 Accuracy: 0.975\n",
      "Training - i: 2632 Loss: 0.046171024 Accuracy: 0.985\n",
      "Training - i: 2633 Loss: 0.083477534 Accuracy: 0.965\n",
      "Training - i: 2634 Loss: 0.06261845 Accuracy: 0.975\n",
      "Training - i: 2635 Loss: 0.060532674 Accuracy: 0.97\n",
      "Training - i: 2636 Loss: 0.057175063 Accuracy: 0.975\n",
      "Training - i: 2637 Loss: 0.06292274 Accuracy: 0.97\n",
      "Training - i: 2638 Loss: 0.034375995 Accuracy: 0.995\n",
      "Training - i: 2639 Loss: 0.04990088 Accuracy: 0.99\n",
      "Training - i: 2640 Loss: 0.048309155 Accuracy: 0.975\n",
      "Training - i: 2641 Loss: 0.08487664 Accuracy: 0.96\n",
      "Training - i: 2642 Loss: 0.06677202 Accuracy: 0.97\n",
      "Training - i: 2643 Loss: 0.04699109 Accuracy: 0.97\n",
      "Training - i: 2644 Loss: 0.085295945 Accuracy: 0.965\n",
      "Training - i: 2645 Loss: 0.119465865 Accuracy: 0.935\n",
      "Training - i: 2646 Loss: 0.05148055 Accuracy: 0.97\n",
      "Training - i: 2647 Loss: 0.043947324 Accuracy: 0.985\n",
      "Training - i: 2648 Loss: 0.03869346 Accuracy: 0.98\n",
      "Training - i: 2649 Loss: 0.049921494 Accuracy: 0.975\n",
      "Training - i: 2650 Loss: 0.036090888 Accuracy: 0.995\n",
      "Training - i: 2651 Loss: 0.06287182 Accuracy: 0.98\n",
      "Validation - i: 2651  Accuracy: [0.89]\n",
      "Training - i: 2652 Loss: 0.04297109 Accuracy: 0.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - i: 2653 Loss: 0.044915903 Accuracy: 0.975\n",
      "Training - i: 2654 Loss: 0.084543 Accuracy: 0.96\n",
      "Training - i: 2655 Loss: 0.07274019 Accuracy: 0.96\n",
      "Training - i: 2656 Loss: 0.04169903 Accuracy: 0.985\n",
      "Training - i: 2657 Loss: 0.07592577 Accuracy: 0.965\n",
      "Training - i: 2658 Loss: 0.0750506 Accuracy: 0.965\n",
      "Training - i: 2659 Loss: 0.046818294 Accuracy: 0.995\n",
      "Training - i: 2660 Loss: 0.08321939 Accuracy: 0.975\n",
      "Training - i: 2661 Loss: 0.055153526 Accuracy: 0.985\n",
      "Training - i: 2662 Loss: 0.03746498 Accuracy: 0.985\n",
      "Training - i: 2663 Loss: 0.048169736 Accuracy: 0.98\n",
      "Training - i: 2664 Loss: 0.07374696 Accuracy: 0.98\n",
      "Training - i: 2665 Loss: 0.058192376 Accuracy: 0.98\n",
      "Training - i: 2666 Loss: 0.057615146 Accuracy: 0.98\n",
      "Training - i: 2667 Loss: 0.054760683 Accuracy: 0.975\n",
      "Training - i: 2668 Loss: 0.065000355 Accuracy: 0.98\n",
      "Training - i: 2669 Loss: 0.083533905 Accuracy: 0.975\n",
      "Training - i: 2670 Loss: 0.06044894 Accuracy: 0.965\n",
      "Training - i: 2671 Loss: 0.0680184 Accuracy: 0.97\n",
      "Training - i: 2672 Loss: 0.07528056 Accuracy: 0.97\n",
      "Training - i: 2673 Loss: 0.08028782 Accuracy: 0.94\n",
      "Training - i: 2674 Loss: 0.05951972 Accuracy: 0.97\n",
      "Training - i: 2675 Loss: 0.064445116 Accuracy: 0.97\n",
      "Training - i: 2676 Loss: 0.06986393 Accuracy: 0.975\n",
      "Training - i: 2677 Loss: 0.10732904 Accuracy: 0.96\n",
      "Training - i: 2678 Loss: 0.06699583 Accuracy: 0.965\n",
      "Training - i: 2679 Loss: 0.099373795 Accuracy: 0.955\n",
      "Training - i: 2680 Loss: 0.11102993 Accuracy: 0.955\n",
      "Training - i: 2681 Loss: 0.061011437 Accuracy: 0.98\n",
      "Training - i: 2682 Loss: 0.046929874 Accuracy: 0.985\n",
      "Training - i: 2683 Loss: 0.06598155 Accuracy: 0.975\n",
      "Training - i: 2684 Loss: 0.100333735 Accuracy: 0.955\n",
      "Training - i: 2685 Loss: 0.062306393 Accuracy: 0.97\n",
      "Training - i: 2686 Loss: 0.042725965 Accuracy: 0.98\n",
      "Training - i: 2687 Loss: 0.11323456 Accuracy: 0.955\n",
      "Training - i: 2688 Loss: 0.054960947 Accuracy: 0.975\n",
      "Training - i: 2689 Loss: 0.0970325 Accuracy: 0.96\n",
      "Training - i: 2690 Loss: 0.040243063 Accuracy: 0.985\n",
      "Training - i: 2691 Loss: 0.050725907 Accuracy: 0.975\n",
      "Training - i: 2692 Loss: 0.04358736 Accuracy: 0.98\n",
      "Training - i: 2693 Loss: 0.04264501 Accuracy: 0.99\n",
      "Training - i: 2694 Loss: 0.06811344 Accuracy: 0.985\n",
      "Training - i: 2695 Loss: 0.059386045 Accuracy: 0.975\n",
      "Training - i: 2696 Loss: 0.05259986 Accuracy: 0.98\n",
      "Training - i: 2697 Loss: 0.04681207 Accuracy: 0.985\n",
      "Training - i: 2698 Loss: 0.061277628 Accuracy: 0.97\n",
      "Training - i: 2699 Loss: 0.07088355 Accuracy: 0.965\n",
      "Training - i: 2700 Loss: 0.08487743 Accuracy: 0.96\n",
      "Training - i: 2701 Loss: 0.051640626 Accuracy: 0.99\n",
      "Validation - i: 2701  Accuracy: [0.905]\n",
      "Training - i: 2702 Loss: 0.058759157 Accuracy: 0.975\n",
      "Training - i: 2703 Loss: 0.09350626 Accuracy: 0.96\n",
      "Training - i: 2704 Loss: 0.057310347 Accuracy: 0.98\n",
      "Training - i: 2705 Loss: 0.061182078 Accuracy: 0.98\n",
      "Training - i: 2706 Loss: 0.119542465 Accuracy: 0.95\n",
      "Training - i: 2707 Loss: 0.09045734 Accuracy: 0.97\n",
      "Training - i: 2708 Loss: 0.094417 Accuracy: 0.96\n",
      "Training - i: 2709 Loss: 0.032609485 Accuracy: 0.98\n",
      "Training - i: 2710 Loss: 0.113496274 Accuracy: 0.94\n",
      "Training - i: 2711 Loss: 0.076616354 Accuracy: 0.965\n",
      "Training - i: 2712 Loss: 0.086052 Accuracy: 0.96\n",
      "Training - i: 2713 Loss: 0.060161136 Accuracy: 0.965\n",
      "Training - i: 2714 Loss: 0.05615856 Accuracy: 0.97\n",
      "Training - i: 2715 Loss: 0.034650445 Accuracy: 0.99\n",
      "Training - i: 2716 Loss: 0.11888203 Accuracy: 0.965\n",
      "Training - i: 2717 Loss: 0.0720874 Accuracy: 0.97\n",
      "Training - i: 2718 Loss: 0.04780121 Accuracy: 0.975\n",
      "Training - i: 2719 Loss: 0.05329974 Accuracy: 0.975\n",
      "Training - i: 2720 Loss: 0.10342214 Accuracy: 0.955\n",
      "Training - i: 2721 Loss: 0.050622836 Accuracy: 0.985\n",
      "Training - i: 2722 Loss: 0.06231879 Accuracy: 0.975\n",
      "Training - i: 2723 Loss: 0.10129984 Accuracy: 0.97\n",
      "Training - i: 2724 Loss: 0.08683977 Accuracy: 0.97\n",
      "Training - i: 2725 Loss: 0.036882486 Accuracy: 0.985\n",
      "Training - i: 2726 Loss: 0.051406726 Accuracy: 0.985\n",
      "Training - i: 2727 Loss: 0.09126524 Accuracy: 0.97\n",
      "Training - i: 2728 Loss: 0.08127815 Accuracy: 0.97\n",
      "Training - i: 2729 Loss: 0.045951054 Accuracy: 0.985\n",
      "Training - i: 2730 Loss: 0.031760816 Accuracy: 0.995\n",
      "Training - i: 2731 Loss: 0.06437241 Accuracy: 0.98\n",
      "Training - i: 2732 Loss: 0.07073582 Accuracy: 0.97\n",
      "Training - i: 2733 Loss: 0.057412416 Accuracy: 0.98\n",
      "Training - i: 2734 Loss: 0.058161955 Accuracy: 0.98\n",
      "Training - i: 2735 Loss: 0.051691856 Accuracy: 0.975\n",
      "Training - i: 2736 Loss: 0.097000964 Accuracy: 0.965\n",
      "Training - i: 2737 Loss: 0.06655778 Accuracy: 0.97\n",
      "Training - i: 2738 Loss: 0.06805941 Accuracy: 0.965\n",
      "Training - i: 2739 Loss: 0.04175575 Accuracy: 0.985\n",
      "Training - i: 2740 Loss: 0.06329481 Accuracy: 0.97\n",
      "Training - i: 2741 Loss: 0.06800577 Accuracy: 0.98\n",
      "Training - i: 2742 Loss: 0.06551533 Accuracy: 0.97\n",
      "Training - i: 2743 Loss: 0.07764494 Accuracy: 0.965\n",
      "Training - i: 2744 Loss: 0.042281065 Accuracy: 0.98\n",
      "Training - i: 2745 Loss: 0.06095217 Accuracy: 0.985\n",
      "Training - i: 2746 Loss: 0.065939 Accuracy: 0.965\n",
      "Training - i: 2747 Loss: 0.044085156 Accuracy: 0.98\n",
      "Training - i: 2748 Loss: 0.06324319 Accuracy: 0.975\n",
      "Training - i: 2749 Loss: 0.0437257 Accuracy: 0.975\n",
      "Training - i: 2750 Loss: 0.059022047 Accuracy: 0.965\n",
      "Training - i: 2751 Loss: 0.0703077 Accuracy: 0.975\n",
      "Validation - i: 2751  Accuracy: [0.885]\n",
      "Training - i: 2752 Loss: 0.066233836 Accuracy: 0.96\n",
      "Training - i: 2753 Loss: 0.049083177 Accuracy: 0.99\n",
      "Training - i: 2754 Loss: 0.06727754 Accuracy: 0.975\n",
      "Training - i: 2755 Loss: 0.04663427 Accuracy: 0.975\n",
      "Training - i: 2756 Loss: 0.04272948 Accuracy: 0.99\n",
      "Training - i: 2757 Loss: 0.06110314 Accuracy: 0.975\n",
      "Training - i: 2758 Loss: 0.06390238 Accuracy: 0.97\n",
      "Training - i: 2759 Loss: 0.058745213 Accuracy: 0.98\n",
      "Training - i: 2760 Loss: 0.06233101 Accuracy: 0.965\n",
      "Training - i: 2761 Loss: 0.061952725 Accuracy: 0.97\n",
      "Training - i: 2762 Loss: 0.077908985 Accuracy: 0.97\n",
      "Training - i: 2763 Loss: 0.06501861 Accuracy: 0.965\n",
      "Training - i: 2764 Loss: 0.05459158 Accuracy: 0.975\n",
      "Training - i: 2765 Loss: 0.050193068 Accuracy: 0.985\n",
      "Training - i: 2766 Loss: 0.07793196 Accuracy: 0.96\n",
      "Training - i: 2767 Loss: 0.0658227 Accuracy: 0.96\n",
      "Training - i: 2768 Loss: 0.057737723 Accuracy: 0.975\n",
      "Training - i: 2769 Loss: 0.095671766 Accuracy: 0.945\n",
      "Training - i: 2770 Loss: 0.086880244 Accuracy: 0.955\n",
      "Training - i: 2771 Loss: 0.06261608 Accuracy: 0.98\n",
      "Training - i: 2772 Loss: 0.051322702 Accuracy: 0.985\n",
      "Training - i: 2773 Loss: 0.12997991 Accuracy: 0.955\n",
      "Training - i: 2774 Loss: 0.10710621 Accuracy: 0.95\n",
      "Training - i: 2775 Loss: 0.067628466 Accuracy: 0.965\n",
      "Training - i: 2776 Loss: 0.083968356 Accuracy: 0.955\n",
      "Training - i: 2777 Loss: 0.085262574 Accuracy: 0.955\n",
      "Training - i: 2778 Loss: 0.0892852 Accuracy: 0.965\n",
      "Training - i: 2779 Loss: 0.09149129 Accuracy: 0.96\n",
      "Training - i: 2780 Loss: 0.09496589 Accuracy: 0.97\n",
      "Training - i: 2781 Loss: 0.09161306 Accuracy: 0.95\n",
      "Training - i: 2782 Loss: 0.12603141 Accuracy: 0.93\n",
      "Training - i: 2783 Loss: 0.05002473 Accuracy: 0.975\n",
      "Training - i: 2784 Loss: 0.06505966 Accuracy: 0.975\n",
      "Training - i: 2785 Loss: 0.080647096 Accuracy: 0.97\n",
      "Training - i: 2786 Loss: 0.063700855 Accuracy: 0.975\n",
      "Training - i: 2787 Loss: 0.041227188 Accuracy: 0.995\n",
      "Training - i: 2788 Loss: 0.085415736 Accuracy: 0.97\n",
      "Training - i: 2789 Loss: 0.03202863 Accuracy: 0.99\n",
      "Training - i: 2790 Loss: 0.07630435 Accuracy: 0.97\n",
      "Training - i: 2791 Loss: 0.051533297 Accuracy: 0.98\n",
      "Training - i: 2792 Loss: 0.07844943 Accuracy: 0.97\n",
      "Training - i: 2793 Loss: 0.0596247 Accuracy: 0.975\n",
      "Training - i: 2794 Loss: 0.047655392 Accuracy: 0.98\n",
      "Training - i: 2795 Loss: 0.08985741 Accuracy: 0.96\n",
      "Training - i: 2796 Loss: 0.048739806 Accuracy: 0.985\n",
      "Training - i: 2797 Loss: 0.072217435 Accuracy: 0.965\n",
      "Training - i: 2798 Loss: 0.076346055 Accuracy: 0.96\n",
      "Training - i: 2799 Loss: 0.05276125 Accuracy: 0.975\n",
      "Training - i: 2800 Loss: 0.07457745 Accuracy: 0.965\n",
      "Training - i: 2801 Loss: 0.06349954 Accuracy: 0.98\n",
      "Validation - i: 2801  Accuracy: [0.935]\n",
      "Training - i: 2802 Loss: 0.052834067 Accuracy: 0.975\n",
      "Training - i: 2803 Loss: 0.04564473 Accuracy: 0.98\n",
      "Training - i: 2804 Loss: 0.10650559 Accuracy: 0.965\n",
      "Training - i: 2805 Loss: 0.032133594 Accuracy: 0.99\n",
      "Training - i: 2806 Loss: 0.071633436 Accuracy: 0.985\n",
      "Training - i: 2807 Loss: 0.11195169 Accuracy: 0.95\n",
      "Training - i: 2808 Loss: 0.058603324 Accuracy: 0.975\n",
      "Training - i: 2809 Loss: 0.069792874 Accuracy: 0.97\n",
      "Training - i: 2810 Loss: 0.07240204 Accuracy: 0.975\n",
      "Training - i: 2811 Loss: 0.10387992 Accuracy: 0.955\n",
      "Training - i: 2812 Loss: 0.06699507 Accuracy: 0.965\n",
      "Training - i: 2813 Loss: 0.053773966 Accuracy: 0.975\n",
      "Training - i: 2814 Loss: 0.04388883 Accuracy: 0.985\n",
      "Training - i: 2815 Loss: 0.049609303 Accuracy: 0.975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - i: 2816 Loss: 0.06566493 Accuracy: 0.965\n",
      "Training - i: 2817 Loss: 0.051261414 Accuracy: 0.98\n",
      "Training - i: 2818 Loss: 0.060157374 Accuracy: 0.975\n",
      "Training - i: 2819 Loss: 0.08445083 Accuracy: 0.97\n",
      "Training - i: 2820 Loss: 0.07076442 Accuracy: 0.97\n",
      "Training - i: 2821 Loss: 0.06714854 Accuracy: 0.97\n",
      "Training - i: 2822 Loss: 0.06149304 Accuracy: 0.97\n",
      "Training - i: 2823 Loss: 0.060419865 Accuracy: 0.96\n",
      "Training - i: 2824 Loss: 0.07938059 Accuracy: 0.975\n",
      "Training - i: 2825 Loss: 0.04935522 Accuracy: 0.985\n",
      "Training - i: 2826 Loss: 0.07562218 Accuracy: 0.975\n",
      "Training - i: 2827 Loss: 0.07353268 Accuracy: 0.97\n",
      "Training - i: 2828 Loss: 0.05614607 Accuracy: 0.97\n",
      "Training - i: 2829 Loss: 0.08401138 Accuracy: 0.96\n",
      "Training - i: 2830 Loss: 0.061782107 Accuracy: 0.985\n",
      "Training - i: 2831 Loss: 0.06323325 Accuracy: 0.965\n",
      "Training - i: 2832 Loss: 0.08944269 Accuracy: 0.95\n",
      "Training - i: 2833 Loss: 0.087014936 Accuracy: 0.955\n",
      "Training - i: 2834 Loss: 0.06050848 Accuracy: 0.975\n",
      "Training - i: 2835 Loss: 0.06512147 Accuracy: 0.96\n",
      "Training - i: 2836 Loss: 0.06911907 Accuracy: 0.97\n",
      "Training - i: 2837 Loss: 0.05536665 Accuracy: 0.98\n",
      "Training - i: 2838 Loss: 0.055292863 Accuracy: 0.975\n",
      "Training - i: 2839 Loss: 0.08203698 Accuracy: 0.975\n",
      "Training - i: 2840 Loss: 0.052036494 Accuracy: 0.975\n",
      "Training - i: 2841 Loss: 0.023629148 Accuracy: 0.99\n",
      "Training - i: 2842 Loss: 0.056791123 Accuracy: 0.98\n",
      "Training - i: 2843 Loss: 0.058530245 Accuracy: 0.985\n",
      "Training - i: 2844 Loss: 0.080018885 Accuracy: 0.985\n",
      "Training - i: 2845 Loss: 0.059669092 Accuracy: 0.985\n",
      "Training - i: 2846 Loss: 0.07476128 Accuracy: 0.97\n",
      "Training - i: 2847 Loss: 0.055867184 Accuracy: 0.985\n",
      "Training - i: 2848 Loss: 0.06667719 Accuracy: 0.965\n",
      "Training - i: 2849 Loss: 0.06143061 Accuracy: 0.975\n",
      "Training - i: 2850 Loss: 0.1011643 Accuracy: 0.945\n",
      "Training - i: 2851 Loss: 0.06925969 Accuracy: 0.975\n",
      "Validation - i: 2851  Accuracy: [0.94]\n",
      "Training - i: 2852 Loss: 0.070167564 Accuracy: 0.97\n",
      "Training - i: 2853 Loss: 0.06398382 Accuracy: 0.97\n",
      "Training - i: 2854 Loss: 0.08692952 Accuracy: 0.97\n",
      "Training - i: 2855 Loss: 0.052747585 Accuracy: 0.98\n",
      "Training - i: 2856 Loss: 0.083191186 Accuracy: 0.97\n",
      "Training - i: 2857 Loss: 0.07546006 Accuracy: 0.975\n",
      "Training - i: 2858 Loss: 0.042419787 Accuracy: 0.98\n",
      "Training - i: 2859 Loss: 0.064886235 Accuracy: 0.99\n",
      "Training - i: 2860 Loss: 0.053091396 Accuracy: 0.985\n",
      "Training - i: 2861 Loss: 0.08160614 Accuracy: 0.97\n",
      "Training - i: 2862 Loss: 0.06813721 Accuracy: 0.975\n",
      "Training - i: 2863 Loss: 0.04550515 Accuracy: 0.975\n",
      "Training - i: 2864 Loss: 0.078939 Accuracy: 0.975\n",
      "Training - i: 2865 Loss: 0.064064465 Accuracy: 0.97\n",
      "Training - i: 2866 Loss: 0.08396955 Accuracy: 0.97\n",
      "Training - i: 2867 Loss: 0.08869511 Accuracy: 0.955\n",
      "Training - i: 2868 Loss: 0.054131433 Accuracy: 0.975\n",
      "Training - i: 2869 Loss: 0.06784604 Accuracy: 0.97\n",
      "Training - i: 2870 Loss: 0.056010082 Accuracy: 0.985\n",
      "Training - i: 2871 Loss: 0.089270964 Accuracy: 0.96\n",
      "Training - i: 2872 Loss: 0.07592982 Accuracy: 0.96\n",
      "Training - i: 2873 Loss: 0.083257124 Accuracy: 0.975\n",
      "Training - i: 2874 Loss: 0.061693527 Accuracy: 0.965\n",
      "Training - i: 2875 Loss: 0.08325012 Accuracy: 0.955\n",
      "Training - i: 2876 Loss: 0.03757274 Accuracy: 0.985\n",
      "Training - i: 2877 Loss: 0.09702527 Accuracy: 0.95\n",
      "Training - i: 2878 Loss: 0.10199844 Accuracy: 0.955\n",
      "Training - i: 2879 Loss: 0.08589816 Accuracy: 0.955\n",
      "Training - i: 2880 Loss: 0.060868353 Accuracy: 0.98\n",
      "Training - i: 2881 Loss: 0.06663422 Accuracy: 0.975\n",
      "Training - i: 2882 Loss: 0.077580936 Accuracy: 0.965\n",
      "Training - i: 2883 Loss: 0.05357642 Accuracy: 0.97\n",
      "Training - i: 2884 Loss: 0.089062415 Accuracy: 0.98\n",
      "Training - i: 2885 Loss: 0.08231773 Accuracy: 0.95\n",
      "Training - i: 2886 Loss: 0.03952636 Accuracy: 0.98\n",
      "Training - i: 2887 Loss: 0.07950492 Accuracy: 0.955\n",
      "Training - i: 2888 Loss: 0.05106556 Accuracy: 0.985\n",
      "Training - i: 2889 Loss: 0.07430427 Accuracy: 0.97\n",
      "Training - i: 2890 Loss: 0.041704465 Accuracy: 0.995\n",
      "Training - i: 2891 Loss: 0.04508158 Accuracy: 1.0\n",
      "Training - i: 2892 Loss: 0.049735177 Accuracy: 0.985\n",
      "Training - i: 2893 Loss: 0.12275351 Accuracy: 0.94\n",
      "Training - i: 2894 Loss: 0.04155283 Accuracy: 0.98\n",
      "Training - i: 2895 Loss: 0.087254524 Accuracy: 0.95\n",
      "Training - i: 2896 Loss: 0.06516174 Accuracy: 0.95\n",
      "Training - i: 2897 Loss: 0.08315809 Accuracy: 0.955\n",
      "Training - i: 2898 Loss: 0.06168995 Accuracy: 0.97\n",
      "Training - i: 2899 Loss: 0.09617512 Accuracy: 0.96\n",
      "Training - i: 2900 Loss: 0.029563583 Accuracy: 0.995\n",
      "Training - i: 2901 Loss: 0.08437849 Accuracy: 0.96\n",
      "Validation - i: 2901  Accuracy: [0.9]\n",
      "Training - i: 2902 Loss: 0.06745997 Accuracy: 0.97\n",
      "Training - i: 2903 Loss: 0.06767073 Accuracy: 0.97\n",
      "Training - i: 2904 Loss: 0.0843412 Accuracy: 0.965\n",
      "Training - i: 2905 Loss: 0.06565844 Accuracy: 0.975\n",
      "Training - i: 2906 Loss: 0.036149994 Accuracy: 0.99\n",
      "Training - i: 2907 Loss: 0.055139467 Accuracy: 0.985\n",
      "Training - i: 2908 Loss: 0.035769448 Accuracy: 0.985\n",
      "Training - i: 2909 Loss: 0.062285796 Accuracy: 0.975\n",
      "Training - i: 2910 Loss: 0.05614345 Accuracy: 0.975\n",
      "Training - i: 2911 Loss: 0.061970383 Accuracy: 0.975\n",
      "Training - i: 2912 Loss: 0.03715907 Accuracy: 0.98\n",
      "Training - i: 2913 Loss: 0.04511225 Accuracy: 0.985\n",
      "Training - i: 2914 Loss: 0.06367716 Accuracy: 0.965\n",
      "Training - i: 2915 Loss: 0.073854 Accuracy: 0.975\n",
      "Training - i: 2916 Loss: 0.057727285 Accuracy: 0.98\n",
      "Training - i: 2917 Loss: 0.05589853 Accuracy: 0.97\n",
      "Training - i: 2918 Loss: 0.05191429 Accuracy: 0.98\n",
      "Training - i: 2919 Loss: 0.03223674 Accuracy: 0.995\n",
      "Training - i: 2920 Loss: 0.023118189 Accuracy: 0.995\n",
      "Training - i: 2921 Loss: 0.05069355 Accuracy: 0.98\n",
      "Training - i: 2922 Loss: 0.0450381 Accuracy: 0.985\n",
      "Training - i: 2923 Loss: 0.06252304 Accuracy: 0.975\n",
      "Training - i: 2924 Loss: 0.06825975 Accuracy: 0.975\n",
      "Training - i: 2925 Loss: 0.06255258 Accuracy: 0.98\n",
      "Training - i: 2926 Loss: 0.04403227 Accuracy: 0.99\n",
      "Training - i: 2927 Loss: 0.072744615 Accuracy: 0.965\n",
      "Training - i: 2928 Loss: 0.057585288 Accuracy: 0.97\n",
      "Training - i: 2929 Loss: 0.082273655 Accuracy: 0.96\n",
      "Training - i: 2930 Loss: 0.07376828 Accuracy: 0.96\n",
      "Training - i: 2931 Loss: 0.030751185 Accuracy: 1.0\n",
      "Training - i: 2932 Loss: 0.051030632 Accuracy: 0.98\n",
      "Training - i: 2933 Loss: 0.03938462 Accuracy: 0.99\n",
      "Training - i: 2934 Loss: 0.09554237 Accuracy: 0.965\n",
      "Training - i: 2935 Loss: 0.06570361 Accuracy: 0.98\n",
      "Training - i: 2936 Loss: 0.053242043 Accuracy: 0.97\n",
      "Training - i: 2937 Loss: 0.034463733 Accuracy: 0.98\n",
      "Training - i: 2938 Loss: 0.07596509 Accuracy: 0.95\n",
      "Training - i: 2939 Loss: 0.07204026 Accuracy: 0.975\n",
      "Training - i: 2940 Loss: 0.06380023 Accuracy: 0.97\n",
      "Training - i: 2941 Loss: 0.10785685 Accuracy: 0.96\n",
      "Training - i: 2942 Loss: 0.033025794 Accuracy: 0.99\n",
      "Training - i: 2943 Loss: 0.034664456 Accuracy: 0.99\n",
      "Training - i: 2944 Loss: 0.083720095 Accuracy: 0.96\n",
      "Training - i: 2945 Loss: 0.0744694 Accuracy: 0.97\n",
      "Training - i: 2946 Loss: 0.049246825 Accuracy: 0.975\n",
      "Training - i: 2947 Loss: 0.0632643 Accuracy: 0.96\n",
      "Training - i: 2948 Loss: 0.07466827 Accuracy: 0.965\n",
      "Training - i: 2949 Loss: 0.08813443 Accuracy: 0.97\n",
      "Training - i: 2950 Loss: 0.075044334 Accuracy: 0.965\n",
      "Training - i: 2951 Loss: 0.03520701 Accuracy: 0.99\n",
      "Validation - i: 2951  Accuracy: [0.875]\n",
      "Training - i: 2952 Loss: 0.060092073 Accuracy: 0.975\n",
      "Training - i: 2953 Loss: 0.046373557 Accuracy: 0.98\n",
      "Training - i: 2954 Loss: 0.043398995 Accuracy: 0.98\n",
      "Training - i: 2955 Loss: 0.039259903 Accuracy: 0.985\n",
      "Training - i: 2956 Loss: 0.06288442 Accuracy: 0.965\n",
      "Training - i: 2957 Loss: 0.05535982 Accuracy: 0.97\n",
      "Training - i: 2958 Loss: 0.043451596 Accuracy: 0.985\n",
      "Training - i: 2959 Loss: 0.08319119 Accuracy: 0.965\n",
      "Training - i: 2960 Loss: 0.08565955 Accuracy: 0.96\n",
      "Training - i: 2961 Loss: 0.041757468 Accuracy: 0.985\n",
      "Training - i: 2962 Loss: 0.06490803 Accuracy: 0.97\n",
      "Training - i: 2963 Loss: 0.07470745 Accuracy: 0.965\n",
      "Training - i: 2964 Loss: 0.030012857 Accuracy: 0.99\n",
      "Training - i: 2965 Loss: 0.068912536 Accuracy: 0.965\n",
      "Training - i: 2966 Loss: 0.06529174 Accuracy: 0.975\n",
      "Training - i: 2967 Loss: 0.035660565 Accuracy: 0.995\n",
      "Training - i: 2968 Loss: 0.061288785 Accuracy: 0.965\n",
      "Training - i: 2969 Loss: 0.055221878 Accuracy: 0.975\n",
      "Training - i: 2970 Loss: 0.046498183 Accuracy: 0.985\n",
      "Training - i: 2971 Loss: 0.03872551 Accuracy: 0.995\n",
      "Training - i: 2972 Loss: 0.051016852 Accuracy: 0.995\n",
      "Training - i: 2973 Loss: 0.0692243 Accuracy: 0.97\n",
      "Training - i: 2974 Loss: 0.08313944 Accuracy: 0.965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - i: 2975 Loss: 0.08380908 Accuracy: 0.95\n",
      "Training - i: 2976 Loss: 0.047682285 Accuracy: 0.985\n",
      "Training - i: 2977 Loss: 0.040169254 Accuracy: 0.995\n",
      "Training - i: 2978 Loss: 0.047728315 Accuracy: 0.985\n",
      "Training - i: 2979 Loss: 0.06360569 Accuracy: 0.99\n",
      "Training - i: 2980 Loss: 0.04706991 Accuracy: 0.975\n",
      "Training - i: 2981 Loss: 0.0693964 Accuracy: 0.965\n",
      "Training - i: 2982 Loss: 0.038676575 Accuracy: 0.995\n",
      "Training - i: 2983 Loss: 0.031966556 Accuracy: 0.985\n",
      "Training - i: 2984 Loss: 0.040958427 Accuracy: 0.985\n",
      "Training - i: 2985 Loss: 0.033823416 Accuracy: 0.99\n",
      "Training - i: 2986 Loss: 0.055094272 Accuracy: 0.98\n",
      "Training - i: 2987 Loss: 0.03183309 Accuracy: 0.99\n",
      "Training - i: 2988 Loss: 0.045563813 Accuracy: 0.98\n",
      "Training - i: 2989 Loss: 0.04978889 Accuracy: 0.985\n",
      "Training - i: 2990 Loss: 0.035836913 Accuracy: 0.99\n",
      "Training - i: 2991 Loss: 0.055332135 Accuracy: 0.98\n",
      "Training - i: 2992 Loss: 0.031535674 Accuracy: 0.99\n",
      "Training - i: 2993 Loss: 0.04679824 Accuracy: 0.975\n",
      "Training - i: 2994 Loss: 0.032401286 Accuracy: 0.985\n",
      "Training - i: 2995 Loss: 0.03892987 Accuracy: 0.985\n",
      "Training - i: 2996 Loss: 0.061178103 Accuracy: 0.975\n",
      "Training - i: 2997 Loss: 0.03681786 Accuracy: 0.985\n",
      "Training - i: 2998 Loss: 0.07499209 Accuracy: 0.97\n",
      "Training - i: 2999 Loss: 0.0527396 Accuracy: 0.97\n",
      "Training - i: 3000 Loss: 0.029988205 Accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "iters = 3000\n",
    "batch_size = 200\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "tf.summary.scalar('loss', loss_fn)\n",
    "tf.summary.scalar('acc', accuracy)\n",
    "summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(os.path.join(__TENSOR_LOG_DIR, model_fn.__name__), session.graph)\n",
    "\n",
    "for i in range(iters):\n",
    "    x_batch, y_batch = get_batch(X_train, Y_train, batch_size)\n",
    "    loss_val, _, acc_val, summary_res = session.run([loss_fn, optimizer, accuracy, summary], \n",
    "                                          feed_dict={x: x_batch, y: y_batch})\n",
    "\n",
    "    writer.add_summary(summary_res, global_step=i)    \n",
    "    print('Training - i:', i+1, 'Loss:', loss_val, 'Accuracy:', acc_val)\n",
    "\n",
    "    # Validate every 50 iterations\n",
    "    if i % 50 == 0:\n",
    "        x_batch, y_batch = get_batch(X_valid, Y_valid, batch_size)\n",
    "        acc_val = session.run([accuracy], feed_dict={x: x_batch, y: y_batch})\n",
    "        print('Validation - i:', i+1, ' Accuracy:', acc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We validate the model with the data it has not seen yet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.26099378 Accuracy: 0.9058219\n"
     ]
    }
   ],
   "source": [
    "# Validate the model with unseen data\n",
    "loss_val, _, acc_val = session.run([loss_fn, optimizer, accuracy], \n",
    "                                    feed_dict={x: X_valid, y: Y_valid})\n",
    "\n",
    "# Print test metrics\n",
    "print('Loss:', loss_val, 'Accuracy:', acc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the model that worked best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('{}.h5'.format(os.path.join(__MODEL_PATH, model_fn.__name__)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the model that worked best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model_file_name = 'model_7.h5'\n",
    "# best_model = tfk.models.load_model(os.path.join(__MODEL_PATH, best_model_file_name))\n",
    "\n",
    "# print('Model {} retrieved!'.format(best_model_file_name))\n",
    "\n",
    "# score = best_model.evaluate(X_valid, Y_valid)\n",
    "# print('Loss:', score[0], 'Accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
