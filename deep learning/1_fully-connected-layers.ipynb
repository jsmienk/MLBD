{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yahtzee\n",
    "\n",
    "__Fully Connected Networks__\n",
    "\n",
    "_By Marnick van der Arend & Jeroen Smienk_\n",
    "\n",
    "![Yahtzee](yahtzee.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "__MODEL_PATH = 'models'\n",
    "__TENSOR_LOG_DIR = 'logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Let's start with looking at the provided dataset:\n",
    "\n",
    "It consists of 5832 'dice throws' and the label that describes that type of throw.\n",
    "\n",
    "The existing Yahtzee labels are: `nothing`, `small-straight`, `three-of-a-kind`, `large-straight`,\n",
    " `full-house`, `four-of-a-kind`, and `yathzee`.\n",
    " \n",
    " Name | Description\n",
    "--- | ---\n",
    "3-of-a-kind | Three dice the same\n",
    "4-of-a-kind | Four dice the same\n",
    "Full-house | Three of one number and two of another\n",
    "Small-straight | Four sequential dice\n",
    "Large-straight | Five sequential dice\n",
    "Yahtzee | All five dice the same\n",
    "Nothing | None of the above combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('yahtzee-dataset.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to classify these categorical labels, we have to 'one-hot encode' them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_df = pd.get_dummies(df, prefix=['label'])\n",
    "one_hot_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can train any model, we have to split the data and the labels into X and Y after shuffling them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled = one_hot_df.sample(frac=1.)\n",
    "X = shuffled.iloc[:,:5].copy()\n",
    "Y = shuffled.iloc[:,5:].copy()\n",
    "\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also split the dataset into a 85:15 split for training and validating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(len(X.index) * .85)\n",
    "X_train = X.iloc[:split]\n",
    "X_valid = X.iloc[split:]\n",
    "Y_train = Y.iloc[:split]\n",
    "Y_valid = Y.iloc[split:]\n",
    "\n",
    "split = int(len(X_train.index) * .85)\n",
    "X_test = X_train.iloc[split:]\n",
    "X_train = X_train.iloc[:split]\n",
    "Y_test = Y_train.iloc[split:]\n",
    "Y_train = Y_train.iloc[:split]\n",
    "\n",
    "print('Split X (train, test, validation):', X_train.shape, X_test.shape, X_valid.shape)\n",
    "print('Split Y (train, test, validation):', Y_train.shape, Y_test.shape, Y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that returns a random batch of a certain size. This batch is used in training to let the model more easily adapt to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, labels, batch_size):\n",
    "    x_batch = data.sample(frac=batch_size / len(data.index))\n",
    "    return x_batch, labels.loc[x_batch.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "We designed several models:\n",
    "\n",
    "rank | name | layers | score\n",
    "--- | --- | --- | ---\n",
    "5 | model_8 | (64, Tanh, Drop=.2) (128, Tanh, Drop=.3) (256, Tanh, Drop=.4) (512, Tanh, Drop=.5) (64, Tanh) | 0.\n",
    "4 | model_7 | (64, ReLU) (128, ReLU) (256, ReLU) (512, ReLU, Drop=.3) (64, ReLU) | 0.\n",
    "3 | model_6 | (200, Tanh) (300, Tanh) (600, Tanh) | 0.\n",
    "2 | model_5 | (600, Tanh) (300, Tanh) (200, Tanh) | 0.\n",
    "6 | model_4 | (128, Tanh) (64, Tanh) (32, Tanh) | 0.\n",
    "7 | model_3 | (12, ReLU) (24, ReLU) (48, ReLU, Drop=.1) (96, ReLU) | 0.\n",
    "8 | model_2 | (128, ReLU) (64, ReLU) (32, ReLU) | 0.\n",
    "9 | model_1 | (128, Sigmoid) | 0.\n",
    "\n",
    "Dropout has a positive effect on the score as can be seen in the table. We also found that the tanh activation function performed well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model 1: BLUE\n",
    "- Model 2: RED\n",
    "- Model 3: LIGHT BLUE\n",
    "- Model 4: PINK\n",
    "- Model 5: GREEN\n",
    "- Model 6: GRAY\n",
    "- Model 7: ORANGE\n",
    "- Model 8: ORANGE\n",
    "\n",
    "### Batch Accuracy\n",
    "\n",
    "![Batch Accuracy](yahtzee-acc.png)\n",
    "\n",
    "### Batch Loss\n",
    "\n",
    "![Batch Loss](yahtzee-loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_1(x, output_shape):\n",
    "    \"\"\"\n",
    "    Single hidden layer with 128 neurons and Sigmoid activation function.\n",
    "    \"\"\"\n",
    "    l_1 = tf.layers.dense(x, units=128, activation=tf.nn.sigmoid)\n",
    "    return tf.layers.dense(l_1, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2(x, output_shape):\n",
    "    \"\"\"\n",
    "    Three hidden layers with different amounts of neurons and relu activation functions.\n",
    "    \"\"\"\n",
    "    l_1 = tf.layers.dense(x, units=128, activation=tf.nn.relu)\n",
    "    l_2 = tf.layers.dense(l_1, units=64, activation=tf.nn.relu)\n",
    "    l_3 = tf.layers.dense(l_2, units=32, activation=tf.nn.relu)\n",
    "    return tf.layers.dense(l_3, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_3(x, output_shape):\n",
    "    \"\"\"\n",
    "    Six hidden layers with different amounts of neurons and \n",
    "    relu activation functions and 2 dropout layers.\n",
    "    \"\"\"\n",
    "    l_1 = tf.layers.dense(x, units=12, activation=tf.nn.relu)\n",
    "    l_2 = tf.layers.dense(l_1, units=24, activation=tf.nn.relu)\n",
    "    l_3 = tf.layers.dense(l_2, units=48, activation=tf.nn.relu)\n",
    "    d_3 = tf.layers.dropout(l_3, rate=.1)\n",
    "    l_4 = tf.layers.dense(d_3, units=96, activation=tf.nn.relu)\n",
    "    return tf.layers.dense(l_4, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_4(x, output_shape):\n",
    "    \"\"\"\n",
    "    Three hidden layers with different amounts of neurons and relu activation functions.\n",
    "    \"\"\"\n",
    "    l_1 = tf.layers.dense(x, units=128, activation=tf.nn.tanh)\n",
    "    l_2 = tf.layers.dense(l_1, units=64, activation=tf.nn.tanh)\n",
    "    l_3 = tf.layers.dense(l_2, units=32, activation=tf.nn.tanh)\n",
    "    return tf.layers.dense(l_3, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_5(x, output_shape):\n",
    "    \"\"\"\n",
    "    High number of neurons in layers, decreasing per layer\n",
    "    \"\"\"\n",
    "    l_1 = tf.layers.dense(x, units=600, activation=tf.nn.tanh)\n",
    "    l_2 = tf.layers.dense(l_1, units=300, activation=tf.nn.tanh)\n",
    "    l_3 = tf.layers.dense(l_2, units=200, activation=tf.nn.tanh)\n",
    "    return tf.layers.dense(l_3, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_5_2(x, output_shape):\n",
    "    \"\"\"\n",
    "    High number of neurons in layers, decreasing per layer\n",
    "    \"\"\"\n",
    "    l_1 = tf.layers.dense(x, units=600, activation=tf.nn.tanh)\n",
    "    d_1 = tf.layers.dropout(l_1, rate=.3)\n",
    "    l_2 = tf.layers.dense(d_1, units=300, activation=tf.nn.tanh)\n",
    "    d_2 = tf.layers.dropout(l_2, rate=.3)\n",
    "    l_3 = tf.layers.dense(d_2, units=200, activation=tf.nn.tanh)\n",
    "    d_3 = tf.layers.dropout(l_3, rate=.3)\n",
    "    return tf.layers.dense(d_3, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_6(x, output_shape):\n",
    "    \"\"\"\n",
    "    High number of neurons in layers, increasing per layer\n",
    "    \"\"\"\n",
    "    l_1 = tf.layers.dense(x, units=200, activation=tf.nn.tanh)\n",
    "    l_2 = tf.layers.dense(l_1, units=300, activation=tf.nn.tanh)\n",
    "    l_3 = tf.layers.dense(l_2, units=600, activation=tf.nn.tanh)\n",
    "    return tf.layers.dense(l_3, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_7(x, output_shape):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    l_1 = tf.layers.dense(x, units=64, activation=tf.nn.relu)\n",
    "    l_2 = tf.layers.dense(l_1, units=128, activation=tf.nn.relu)\n",
    "    l_3 = tf.layers.dense(l_2, units=256, activation=tf.nn.relu)\n",
    "    l_4 = tf.layers.dense(l_3, units=512, activation=tf.nn.relu)\n",
    "    d_4 = tf.layers.dropout(l_4, rate=.3)\n",
    "    l_5 = tf.layers.dense(d_4, units=64, activation=tf.nn.relu)\n",
    "    return tf.layers.dense(l_5, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_8(x, output_shape):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    l_1 = tf.layers.dense(x, units=64, activation=tf.nn.tanh)\n",
    "    d_1 = tf.layers.dropout(l_1, rate=.2)\n",
    "    l_2 = tf.layers.dense(d_1, units=128, activation=tf.nn.tanh)\n",
    "    d_2 = tf.layers.dropout(l_2, rate=.3)\n",
    "    l_3 = tf.layers.dense(d_2, units=256, activation=tf.nn.tanh)\n",
    "    d_3 = tf.layers.dropout(l_3, rate=.4)\n",
    "    l_4 = tf.layers.dense(d_3, units=512, activation=tf.nn.tanh)\n",
    "    d_4 = tf.layers.dropout(l_4, rate=.5)\n",
    "    l_5 = tf.layers.dense(d_4, units=64, activation=tf.nn.tanh)\n",
    "    return tf.layers.dense(l_5, units=output_shape, activation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the placeholder for our 5-dice input and 7-class output and choose a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, X.shape[1]], name='x')\n",
    "y = tf.placeholder(tf.float32, shape=[None, Y.shape[1]], name='y')\n",
    "\n",
    "model_fn = model_1\n",
    "y_pred = model_fn(x, Y.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Test, Validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose an optimizer, a loss functon and metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=y_pred)\n",
    "loss_fn = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# Optimizer minimizes the loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=.001).minimize(loss_fn)\n",
    "\n",
    "# Accuracy metric\n",
    "#   checks if the indices of the highest values in the real \n",
    "#   and predicted arrays are equal\n",
    "prediction = tf.equal(tf.argmax(y, axis=1), tf.argmax(y_pred, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model using a certain batch size and for a number of iterations while posting scalars to TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iters = 3000\n",
    "train_batch_size = 200\n",
    "test_batch_size = 50\n",
    "\n",
    "session = tf.Session()\n",
    "with session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Defining the metrics we want to log in TensorBoard\n",
    "    sum_loss_train = tf.summary.scalar('loss_train', loss_fn)\n",
    "    sum_loss_test = tf.summary.scalar('loss_test', loss_fn)\n",
    "    sum_acc_train = tf.summary.scalar('acc_train', accuracy)\n",
    "    sum_acc_test = tf.summary.scalar('acc_test', accuracy)\n",
    "    tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(os.path.join(__TENSOR_LOG_DIR, model_fn.__name__), session.graph)\n",
    "\n",
    "    # Start training for a certain number of iterations\n",
    "    for i in range(iters):\n",
    "        # Every iteration we get a random batch of the training data\n",
    "        x_batch, y_batch = get_batch(X_train, Y_train, train_batch_size)\n",
    "        # We train the model by providing the 'optimizer' variable to the run function.\n",
    "        # We also want to calculate the accuracy and loss TensorBoard metrics\n",
    "        loss_val, _, acc_val, sum_1, sum_2 = session.run([loss_fn, optimizer, accuracy, \n",
    "                                                          sum_loss_train, sum_acc_train], \n",
    "                                                         feed_dict={x: x_batch, y: y_batch})\n",
    "        # Write the metrics to TensorBoard\n",
    "        writer.add_summary(sum_1, global_step=i)\n",
    "        writer.add_summary(sum_2, global_step=i)\n",
    "\n",
    "        # Validate every 50 iterations\n",
    "        if i % 50 == 0:\n",
    "            # DO NOT PROVIDE THE 'optimzer' VARIABLE HERE\n",
    "            # ELSE THE MODEL WILL TRAIN ON THE TEST DATA\n",
    "            acc_val, sum_1, sum_2 = session.run([accuracy, sum_loss_test, sum_acc_test], \n",
    "                                                feed_dict={x: X_test, y: Y_test})\n",
    "            # Write the metrics to TensorBoard\n",
    "            writer.add_summary(sum_1, global_step=i)\n",
    "            writer.add_summary(sum_2, global_step=i)\n",
    "            print('Testing - i:', i+1, ' Accuracy:', acc_val)\n",
    "    \n",
    "\n",
    "    # Validate the model with unseen data\n",
    "    acc_val = session.run([accuracy], feed_dict={x: X_valid, y: Y_valid})\n",
    "    print('Validation accuracy:', acc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting & Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '{}.ckpt'.format(os.path.join(__MODEL_PATH, model_fn.__name__, model_fn.__name__))\n",
    "\n",
    "model_to_load = model_5_2\n",
    "\n",
    "load_path = '{}.ckpt'.format(os.path.join(__MODEL_PATH, model_to_load.__name__, model_to_load.__name__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the model that worked best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with session:\n",
    "#     tf.train.Saver().save(session, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the model that worked best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as saved_session:\n",
    "    tf.train.Saver().restore(saved_session, load_path)\n",
    "\n",
    "    # Validate the model with unseen data\n",
    "    acc_val = saved_session.run([accuracy], feed_dict={x: X_valid, y: Y_valid})\n",
    "\n",
    "    # Print test metrics\n",
    "    print('Accuracy:', acc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
